[
  {
    "objectID": "ws4.html",
    "href": "ws4.html",
    "title": "Workshop 4: Community traits",
    "section": "",
    "text": "This vignette uses hypervolumes to understand the spatial variation in trait diversity of coral reef communities. Hypervolumes are generated using species trait data and weighted based on random percent cover of species from mean and sd across all sites within a region. This process is repeated 100 times. Hypervolume size is used to quantify the trait diversity of each community.\nR script: github\nR script of workshop 3"
  },
  {
    "objectID": "ws4.html#trait-diversity-of-coral-communities-in-puerto-rico",
    "href": "ws4.html#trait-diversity-of-coral-communities-in-puerto-rico",
    "title": "Workshop 4: Community traits",
    "section": "",
    "text": "This vignette uses hypervolumes to understand the spatial variation in trait diversity of coral reef communities. Hypervolumes are generated using species trait data and weighted based on random percent cover of species from mean and sd across all sites within a region. This process is repeated 100 times. Hypervolume size is used to quantify the trait diversity of each community.\nR script: github\nR script of workshop 3"
  },
  {
    "objectID": "ws4.html#data",
    "href": "ws4.html#data",
    "title": "Workshop 4: Community traits",
    "section": "data",
    "text": "data\nThe data used for this example comes from the NOAA which monitors coral benthic communities throughout Puerto Rico. The benthic cover data consists of percent cover (pc = mean percent cover) from coral species collected across each site in the 8 regions in 2021, as well as information about each site. Data is averaged across transects at each site. The coral trait data is based on average values of each trait for each species from published literature values. This dataset is z-scored already across all species for each trait.\n\n\n\nMap of sampling regions\n\n\n\n# load libraries\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nlibrary(hypervolume)\n## Loading required package: Rcpp\n\n# load trait data\ndf_tr = read_csv('data/coral_traits.csv') \n## Rows: 26 Columns: 6\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): species\n## dbl (5): corallite diameter, growth rate, skeletal density, symbiodinium den...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndf_tr\n## # A tibble: 26 × 6\n##    species                 `corallite diameter` `growth rate` `skeletal density`\n##    &lt;chr&gt;                                  &lt;dbl&gt;         &lt;dbl&gt;              &lt;dbl&gt;\n##  1 Acropora cervicornis                 -1.14          2.33              -0.0978\n##  2 Acropora palmata                     -1.87          1.94               0.587 \n##  3 Agaricia agaricites                  -0.401        -1.46               0.935 \n##  4 Colpophyllia natans                   2.36         -0.274             -2.47  \n##  5 Diploria labyrinthifor…               0.965        -0.562             -0.172 \n##  6 Montastraea cavernosa                 0.792        -0.456              0.321 \n##  7 Orbicella annularis                  -0.289        -0.0359             0.373 \n##  8 Orbicella faveolata                  -0.326         0.103             -0.958 \n##  9 Orbicella franksi                    -0.0173       -0.329              1.08  \n## 10 Porites astreoides                   -0.962        -0.641             -0.0403\n## # ℹ 16 more rows\n## # ℹ 2 more variables: `symbiodinium density` &lt;dbl&gt;,\n## #   `colony maximum diameter` &lt;dbl&gt;\n\n# load benthic community percent cover data across regions\n# pc = percent cover 0-100\n# pc_sd = standard deviation in percent cover\ndf_ben = read_csv('data/PR_coral_2021.csv')\n## Rows: 332 Columns: 8\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): site, region, reef_zone, species\n## dbl (4): depth, dist_river, dist_mpa, pc\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndf_ben\n## # A tibble: 332 × 8\n##    site  region    reef_zone  depth dist_river dist_mpa species               pc\n##    &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n##  1 beril southwest shelf edge    20       12.2        0 Agaricia agaricit… 0.842\n##  2 beril southwest shelf edge    20       12.2        0 Madracis decactis  0.038\n##  3 beril southwest shelf edge    20       12.2        0 Meandrina meandri… 0.268\n##  4 beril southwest shelf edge    20       12.2        0 Montastraea caver… 1.74 \n##  5 beril southwest shelf edge    20       12.2        0 Orbicella annular… 2.60 \n##  6 beril southwest shelf edge    20       12.2        0 Orbicella faveola… 4.31 \n##  7 beril southwest shelf edge    20       12.2        0 Orbicella franksi  0.9  \n##  8 beril southwest shelf edge    20       12.2        0 Porites astreoides 1.24 \n##  9 beril southwest shelf edge    20       12.2        0 Porites porites    1.18 \n## 10 beril southwest shelf edge    20       12.2        0 Pseudodiploria st… 0.318\n## # ℹ 322 more rows"
  },
  {
    "objectID": "ws4.html#prep-data",
    "href": "ws4.html#prep-data",
    "title": "Workshop 4: Community traits",
    "section": "Prep data",
    "text": "Prep data\nTo generate hypervolumes we need to combine the trait data with all of the benthic site information.\n\ndf = df_ben |&gt; \n  # join trait data to benthic data\n  inner_join(df_tr, by = 'species')  |&gt; \n  # drop species since don't need for hypervolume\n  select(-species)\n\ndf \n## # A tibble: 332 × 12\n##    site  region   reef_zone depth dist_river dist_mpa    pc `corallite diameter`\n##    &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;                &lt;dbl&gt;\n##  1 beril southwe… shelf ed…    20       12.2        0 0.842              -0.401 \n##  2 beril southwe… shelf ed…    20       12.2        0 0.038              -0.104 \n##  3 beril southwe… shelf ed…    20       12.2        0 0.268              -0.104 \n##  4 beril southwe… shelf ed…    20       12.2        0 1.74                0.792 \n##  5 beril southwe… shelf ed…    20       12.2        0 2.60               -0.289 \n##  6 beril southwe… shelf ed…    20       12.2        0 4.31               -0.326 \n##  7 beril southwe… shelf ed…    20       12.2        0 0.9                -0.0173\n##  8 beril southwe… shelf ed…    20       12.2        0 1.24               -0.962 \n##  9 beril southwe… shelf ed…    20       12.2        0 1.18               -0.104 \n## 10 beril southwe… shelf ed…    20       12.2        0 0.318               1.02  \n## # ℹ 322 more rows\n## # ℹ 4 more variables: `growth rate` &lt;dbl&gt;, `skeletal density` &lt;dbl&gt;,\n## #   `symbiodinium density` &lt;dbl&gt;, `colony maximum diameter` &lt;dbl&gt;"
  },
  {
    "objectID": "ws4.html#hypervolumes",
    "href": "ws4.html#hypervolumes",
    "title": "Workshop 4: Community traits",
    "section": "Hypervolumes",
    "text": "Hypervolumes\nHere we nest two columns one with all of the data for the hypervolume (data) and one with a vector of the percent cover that will be used to weight the hypervolume (weight). Hypervolume size is extracted from each hypervolume using map_dbl() and get_size().\n\n\ndf_hv = df |&gt; \n  group_by(site, region, reef_zone, depth, dist_river, dist_mpa) |&gt; \n  # create a column for the percent cover to weight hypervolume as well as input data\n  nest(weight = pc, data = `corallite diameter`:`colony maximum diameter`) |&gt; \n  # create community weighted hypervolumes \n  mutate(hv = map2(data,weight, \\(data,weight) hypervolume_gaussian(data, \n                                                                    name = site,\n                                                                    weight = weight$pc,\n                                                                    samples.per.point = 1000,\n                                                                    kde.bandwidth = estimate_bandwidth(data), \n                                                                    sd.count = 3, \n                                                                    quantile.requested = 0.95, \n                                                                    quantile.requested.type = \"probability\", \n                                                                    chunk.size = 1000, \n                                                                    verbose = F)),\n  # extrace size for each hypervolume \n          hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n\n** Do not try to view df in rstudio it will freeze your r since it is too big\n\nhead(df_hv)\n## # A tibble: 6 × 10\n## # Groups:   site, region, reef_zone, depth, dist_river, dist_mpa [6]\n##   site   region reef_zone depth dist_river dist_mpa weight   data     hv        \n##   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;list&gt;   &lt;list&gt;   &lt;list&gt;    \n## 1 beril  south… shelf ed…    20       12.2        0 &lt;tibble&gt; &lt;tibble&gt; &lt;Hypervlm&gt;\n## 2 boya … south… shelf ed…    20       16.3        0 &lt;tibble&gt; &lt;tibble&gt; &lt;Hypervlm&gt;\n## 3 cabez… north… fore reef    10        8.8        0 &lt;tibble&gt; &lt;tibble&gt; &lt;Hypervlm&gt;\n## 4 canal… viequ… fore reef     5       32          0 &lt;tibble&gt; &lt;tibble&gt; &lt;Hypervlm&gt;\n## 5 canji… viequ… bank/she…    15       20.5       10 &lt;tibble&gt; &lt;tibble&gt; &lt;Hypervlm&gt;\n## 6 carlo… viequ… fore reef    10       32          0 &lt;tibble&gt; &lt;tibble&gt; &lt;Hypervlm&gt;\n## # ℹ 1 more variable: hv_size &lt;dbl&gt;"
  },
  {
    "objectID": "ws4.html#hypervolume-size",
    "href": "ws4.html#hypervolume-size",
    "title": "Workshop 4: Community traits",
    "section": "Hypervolume size",
    "text": "Hypervolume size\nWe can look at how trait diversity varies across different gradients and variables.\n\n# plot hypervolume size\nd = df_hv |&gt; \n  ungroup() |&gt; \n  mutate(region = factor(region, \n                         levels = c('north/northeast', 'vieques/culebra',\n                                    'southeast', 'south', 'southwest',\n                                     'west', 'mona/desecheo'),\n                         labels = c('North/\\nNortheast', 'Vieques/\\nCulebra',\n                                    'Southeast', 'South', 'Southwest',\n                                    'West', 'Mona/\\nDesecheo')),\n         reef_zone = factor(reef_zone,\n                            levels = c(\"back reef\", \"reef crest\", \n                                       \"fore reef\", \"bank/shelf\", \"shelf edge\"),\n                            labels = c(\"Back reef\", \"Reef crest\", \n                                       \"Fore reef\", \"Bank/shelf\", \"Shelf edge\")))\n\n\n# plot by region\nggplot(d, aes(region, hv_size, fill = region))+\n  geom_point(aes(color = region), size = 1, \n             position=position_jitterdodge(dodge.width = 0.75, jitter.width = 1))+\n  geom_boxplot(alpha = 0.6, outliers = F)+\n  labs(x = 'Region', y = 'Trait diversity')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n# plot by reef zone \nggplot(d, aes(reef_zone, hv_size, fill = reef_zone))+\n  geom_point(aes(color = reef_zone), size = 1, \n             position=position_jitterdodge(dodge.width = 0.75, jitter.width = 1))+\n  geom_boxplot(alpha = 0.6, outliers = F)+\n  labs(x = 'Reef zone', y = 'Trait diversity')+\n  scale_fill_viridis_d(option = 'virdis')+\n  scale_color_viridis_d(option = 'viridis')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n## Warning in viridisLite::viridis(n, alpha, begin, end, direction, option):\n## Option 'virdis' does not exist. Defaulting to 'viridis'.\n\n\n\n\n\n\n\n\n# plot depth \nggplot(d, aes(depth, hv_size))+\n  geom_point(size = 2)+\n  geom_smooth(method = 'lm', linewidth = 1, color = '#9E1B32')+\n  labs(x = 'Depth (m)', y = 'Trait diversity')+\n  scale_fill_viridis_d(option = 'virdis')+\n  scale_color_viridis_d(option = 'viridis')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# plot distance to closest river mouth\nggplot(d, aes(dist_river, hv_size))+\n  geom_point(size = 2)+\n  geom_smooth(method = 'lm', linewidth = 1, color = '#9E1B32')+\n  labs(x = 'Distance to nearest river (km)', y = 'Trait diversity')+\n  scale_fill_viridis_d(option = 'virdis')+\n  scale_color_viridis_d(option = 'viridis')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# plot distance to nearest MPA\nggplot(d, aes(dist_mpa, hv_size))+\n  geom_point(size = 2)+\n  geom_smooth(method = 'lm', linewidth = 1, color = '#9E1B32')+\n  labs(x = 'Distance to nearest MPA (km)', y = 'Trait diversity')+\n  scale_fill_viridis_d(option = 'virdis')+\n  scale_color_viridis_d(option = 'viridis')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n## `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "ws4.html#occupancy",
    "href": "ws4.html#occupancy",
    "title": "Workshop 4: Community traits",
    "section": "Occupancy",
    "text": "Occupancy\nWe can use hypervolume occupancy to understand where in the trait space is shared by multiple reefs. HVs are joined together and points are generated to understand how much of the trait space is shared by multiple hypervolumes. Each point is scored between 1/n and 1 hypervolume with n being the total hypervolumes.\n\n\nd = df_hv |&gt;\n  ungroup() |&gt; \n  filter(region %in% c('west','north/northeast')) |&gt; \n  select(site, region, hv) \n\nhv_join = reduce(d$hv, hypervolume_join)\n\nhv_occ = hypervolume_n_occupancy(hv_join,\n  method = \"box\",\n  classification = d$region,\n  box_density = 500,\n  FUN = mean,\n  verbose = F)\n\n\nwrite_rds(df_occ, 'data/hvOCC.rds')\ndf_occ = read_rds('data/hvOCC.rds')\n\nWe can extract metrics from the hv_occ like combine size and weighted centroid\n\n# overall size\nget_volume(hv_occ)\n\n# weighted centroid\nget_centroid_weighted(hv_occ)\n\nAnd turn it into a data frame to plot\n\n# convert to data frame\ndf_occ = hypervolume_to_data_frame(hv_occ) |&gt; \n  rename(site = Name, occupancy = ValueAtRandomPoints, \n         `Corallite diameter` = \"corallite.diameter\",\n         `Growth rate` = \"growth.rate\",\n         `Skeletal density` = \"skeletal.density\",\n         `Symbiodinium density` = \"symbiodinium.density\",\n         `Colony maximum diameter` = \"colony.maximum.diameter\")\n\nhead(df_occ)\n##              site Corallite diameter Growth rate Skeletal density\n## 1 north/northeast         -1.1773937  0.41855593      -1.99951167\n## 2 north/northeast          1.4890029  0.58081945       0.08841968\n## 3 north/northeast         -0.6859578 -0.23403288       1.50973666\n## 4 north/northeast         -1.0340224  3.41759254      -0.45431489\n## 5 north/northeast         -1.4554137 -1.77127275       0.43179770\n## 6 north/northeast          0.7183359 -0.05524995      -1.18899211\n##   Symbiodinium density Colony maximum diameter occupancy\n## 1           2.52365947              2.49640314 0.1666667\n## 2           1.20475354             -2.00242503 0.1666667\n## 3           2.41420858             -1.81243283 0.1666667\n## 4           0.03163161             -0.34401979 0.1666667\n## 5          -0.53213239             -0.03346148 0.3333333\n## 6          -0.27366023             -2.11342085 0.1666667\n\nWe can also plot, but need to modify ggplot into gtable and manually configure in order to create the biplots\n\n\n# get points for plotting\ndo = df_occ |&gt; \n  group_by(site) |&gt;\n  mutate(num = row_number()) |&gt; \n  pivot_longer(`Corallite diameter`:`Colony maximum diameter`, names_to = 'axis', values_to = 'value')\n\n#make dataframe for first axis\ndo1 = do |&gt; \n  rename(axis1 = axis, val1 = value)\n\n# make dataframe for second axis\ndo2 = do |&gt; \n  rename(axis2 = axis, val2 = value)\n\n#make all unique axis combinations\nspc = tibble(axis1 = unique(do$axis),\n             axis2 = unique(do$axis))\n\ndf_ax = spc |&gt;  expand(axis1,axis2)\n\n# remove duplicates \ndf_ax = df_ax[!duplicated(t(apply(df_ax,1,sort))),] |&gt;\n  filter(!(axis1 == axis2))\n\n# combine data from north/northeast\ndf_axn = df_ax |&gt;\n  mutate(site = 'north/northeast') |&gt; \n  slice(rep(1:n(), each=max(do$num[do$site == 'north/northeast']))) |&gt; \n  group_by(site, axis1, axis2) |&gt; \n  mutate(num = row_number())\n  \n# combine data from west\ndf_axw = df_ax |&gt;\n  mutate(site = 'west') |&gt; \n  slice(rep(1:n(), each=max(do$num[do$site == 'west']))) |&gt; \n  group_by(site, axis1, axis2) |&gt; \n  mutate(num = row_number())\n\n# bind all data together\ndf_axx = bind_rows(df_axw, df_axn) |&gt;\n  left_join(do1) |&gt; \n  left_join(do2) |&gt; \n  ungroup()\n## Joining with `by = join_by(axis1, site, num)`\n## Joining with `by = join_by(axis2, site, num, occupancy)`\n\n# subset enormous dataframe for plotting\ndf_sub = df_axx |&gt; \n  distinct(site, num) |&gt;                  # get unique site and point number\n  slice_sample(n = 3000) |&gt;                  # Randomly select 10,000 points\n  left_join(df_axx, by = c('num','site'))\n\nPlot\n\n# make ggplot object\nlibrary(gtable)\nlibrary(grid)\n\n# west  \npw = ggplot()+\n  geom_point(data = df_axx |&gt;  filter(site == 'west'),\n             aes(val2, val1, color = occupancy), size = 0.5, alpha = 0.5) +\n  scale_x_continuous(limits = c(-7,7), breaks = c(-6,-4,-2,0,2,4,6))+\n  scale_y_continuous(limits = c(-7,7), breaks = c(-6,-4,-2,0,2,4,6))+\n  labs(x = NULL, y = NULL, fill = 'Species', color = 'Occupancy')+\n  scale_color_gradient(low = \"#add8e6\", high = \"#132B43\")+\n  theme_bw()+\n  facet_grid(cols = vars(axis2), rows = vars(axis1), switch = 'both')+\n  theme(\n        # axis.title = element_text(size = 14),\n        # axis.text.y = element_text(size = 13, colour = \"black\"),\n        # axis.text.x = element_text(size = 13, colour = \"black\"),\n        # plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'right',\n        # legend.title = element_text(size = 17),\n        # strip.text = element_text(size = 17),\n        # legend.text = element_text(size = 16),\n        strip.text.y.left = element_text(angle = 0),\n        strip.background = element_blank(),\n        strip.placement = \"outside\")\n\npw\n## Warning: Removed 440 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n# fix all labels\ngt = ggplotGrob(pw)\n## Warning: Removed 440 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n# Panels and horizontal strip labels to delete (first column and top row)\ndel = which(gt$layout$name %in% c(\n  'panel-2-1', 'panel-3-1', 'panel-4-1',\n  'panel-3-2', 'panel-4-2',\n  'panel-4-3',\n  'strip-b-1', 'strip-b-2', 'strip-b-3'\n))\ngt$grobs[del] = NULL\ngt$layout = gt$layout[-del, ]\n\n# y axis row 2\ngt$layout$l[gt$layout$name == 'axis-l-2'] = gt$layout$l[gt$layout$name == 'axis-l-2'] + 2\ngt$layout$r[gt$layout$name == 'axis-l-2'] = gt$layout$r[gt$layout$name == 'axis-l-2'] + 2\n\n# y axis row 3\ngt$layout$l[gt$layout$name == 'axis-l-3'] = gt$layout$l[gt$layout$name == 'axis-l-3'] + 4\ngt$layout$r[gt$layout$name == 'axis-l-3'] = gt$layout$r[gt$layout$name == 'axis-l-3'] + 4\n\n# y axis row 4\ngt$layout$l[gt$layout$name == 'axis-l-4'] = gt$layout$l[gt$layout$name == 'axis-l-4'] + 6\ngt$layout$r[gt$layout$name == 'axis-l-4'] = gt$layout$r[gt$layout$name == 'axis-l-4'] + 6\n\n# x axis column 1\ngt$layout$t[gt$layout$name == 'axis-b-1'] = gt$layout$t[gt$layout$name == 'axis-b-1'] - 6\ngt$layout$b[gt$layout$name == 'axis-b-1'] = gt$layout$b[gt$layout$name == 'axis-b-1'] - 6\n\n# x axis column 2\ngt$layout$t[gt$layout$name == 'axis-b-2'] = gt$layout$t[gt$layout$name == 'axis-b-2'] - 4\ngt$layout$b[gt$layout$name == 'axis-b-2'] = gt$layout$b[gt$layout$name == 'axis-b-2'] - 4\n\n# x axis column 3\ngt$layout$t[gt$layout$name == 'axis-b-3'] = gt$layout$t[gt$layout$name == 'axis-b-3'] - 2\ngt$layout$b[gt$layout$name == 'axis-b-3'] = gt$layout$b[gt$layout$name == 'axis-b-3'] - 2\n\n# y strip row 2\ngt$layout$l[gt$layout$name == 'strip-l-2'] = gt$layout$l[gt$layout$name == 'strip-l-2'] + 3\ngt$layout$r[gt$layout$name == 'strip-l-2'] = gt$layout$r[gt$layout$name == 'strip-l-2'] + 3\n\n# y strip row 3\ngt$layout$l[gt$layout$name == 'strip-l-3'] = gt$layout$l[gt$layout$name == 'strip-l-3'] + 5\ngt$layout$r[gt$layout$name == 'strip-l-3'] = gt$layout$r[gt$layout$name == 'strip-l-3'] + 5\n\n# y strip row 4\ngt$layout$l[gt$layout$name == 'strip-l-4'] = gt$layout$l[gt$layout$name == 'strip-l-4'] + 7\ngt$layout$r[gt$layout$name == 'strip-l-4'] = gt$layout$r[gt$layout$name == 'strip-l-4'] + 7\n\n\n\ngrid.draw(gt)\n\n\n\n\n\n\n\n\n\n# west  \npn = ggplot()+\n  geom_point(data = df_axx |&gt;  filter(site == 'north/northeast'),\n             aes(val2, val1, color = occupancy), size = 0.5, alpha = 0.5) +\n  scale_x_continuous(limits = c(-7,7), breaks = c(-6,-4,-2,0,2,4,6))+\n  scale_y_continuous(limits = c(-7,7), breaks = c(-6,-4,-2,0,2,4,6))+\n  labs(x = NULL, y = NULL, fill = 'Species', color = 'Occupancy')+\n  scale_color_gradient(low = \"#add8e6\", high = \"#132B43\")+\n  theme_bw()+\n  facet_grid(cols = vars(axis2), rows = vars(axis1), switch = 'both')+\n  theme(\n        # axis.title = element_text(size = 14),\n        # axis.text.y = element_text(size = 13, colour = \"black\"),\n        # axis.text.x = element_text(size = 13, colour = \"black\"),\n        # plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'right',\n        # legend.title = element_text(size = 17),\n        # strip.text = element_text(size = 17),\n        # legend.text = element_text(size = 16),\n        strip.text.y.left = element_text(angle = 0),\n        strip.background = element_blank(),\n        strip.placement = \"outside\")\n\npn\n\n# fix all labels\ngt = ggplotGrob(pn)\n\n# Panels and horizontal strip labels to delete (first column and top row)\ndel = which(gt$layout$name %in% c(\n  'panel-2-1', 'panel-3-1', 'panel-4-1',\n  'panel-3-2', 'panel-4-2',\n  'panel-4-3',\n  'strip-b-1', 'strip-b-2', 'strip-b-3'\n))\ngt$grobs[del] = NULL\ngt$layout = gt$layout[-del, ]\n\n# y axis row 2\ngt$layout$l[gt$layout$name == 'axis-l-2'] = gt$layout$l[gt$layout$name == 'axis-l-2'] + 2\ngt$layout$r[gt$layout$name == 'axis-l-2'] = gt$layout$r[gt$layout$name == 'axis-l-2'] + 2\n\n# y axis row 3\ngt$layout$l[gt$layout$name == 'axis-l-3'] = gt$layout$l[gt$layout$name == 'axis-l-3'] + 4\ngt$layout$r[gt$layout$name == 'axis-l-3'] = gt$layout$r[gt$layout$name == 'axis-l-3'] + 4\n\n# y axis row 4\ngt$layout$l[gt$layout$name == 'axis-l-4'] = gt$layout$l[gt$layout$name == 'axis-l-4'] + 6\ngt$layout$r[gt$layout$name == 'axis-l-4'] = gt$layout$r[gt$layout$name == 'axis-l-4'] + 6\n\n# x axis column 1\ngt$layout$t[gt$layout$name == 'axis-b-1'] = gt$layout$t[gt$layout$name == 'axis-b-1'] - 6\ngt$layout$b[gt$layout$name == 'axis-b-1'] = gt$layout$b[gt$layout$name == 'axis-b-1'] - 6\n\n# x axis column 2\ngt$layout$t[gt$layout$name == 'axis-b-2'] = gt$layout$t[gt$layout$name == 'axis-b-2'] - 4\ngt$layout$b[gt$layout$name == 'axis-b-2'] = gt$layout$b[gt$layout$name == 'axis-b-2'] - 4\n\n# x axis column 3\ngt$layout$t[gt$layout$name == 'axis-b-3'] = gt$layout$t[gt$layout$name == 'axis-b-3'] - 2\ngt$layout$b[gt$layout$name == 'axis-b-3'] = gt$layout$b[gt$layout$name == 'axis-b-3'] - 2\n\n# y strip row 2\ngt$layout$l[gt$layout$name == 'strip-l-2'] = gt$layout$l[gt$layout$name == 'strip-l-2'] + 3\ngt$layout$r[gt$layout$name == 'strip-l-2'] = gt$layout$r[gt$layout$name == 'strip-l-2'] + 3\n\n# y strip row 3\ngt$layout$l[gt$layout$name == 'strip-l-3'] = gt$layout$l[gt$layout$name == 'strip-l-3'] + 5\ngt$layout$r[gt$layout$name == 'strip-l-3'] = gt$layout$r[gt$layout$name == 'strip-l-3'] + 5\n\n# y strip row 4\ngt$layout$l[gt$layout$name == 'strip-l-4'] = gt$layout$l[gt$layout$name == 'strip-l-4'] + 7\ngt$layout$r[gt$layout$name == 'strip-l-4'] = gt$layout$r[gt$layout$name == 'strip-l-4'] + 7\n\n\n\ngrid.draw(gt)\n\n\n\n\n\n\n\n\nYou can use hypervolume_occupancy_test() where in the trait space one group is occupying significantly more than another.\n\n# first step is to permute the occupancy\nhv_op = hypervolume_n_occupancy_permute('data/w_v_ne/',\n                                hv_occ, hv_join,\n                                verbose = F, n = 100,\n                                cores = 4)\n\n\n# then we can use t\nhv_ocp = hypervolume_n_occupancy_test(hv_occ, 'Objects/data/w_v_ne/north', alternative = \"two_sided\")\n\n\ndf_ocp = hypervolume_to_data_frame(hv_ocp)\n\n\np = df_ocp |&gt; \n  mutate(region = if_else(ValueAtRandomPoints &lt; 0, 'north/northeast', 'west')) |&gt; \n  rename(occupancy = ValueAtRandomPoints) |&gt; \n  pivot_longer(corallite.diameter:colony.maximum.diameter, names_to = 'Trait', values_to = 'value') |&gt; \n  group_by(Trait,region) |&gt; \n  mutate(mean = sum(occupancy*value)/sum(occupancy))\n\n\n\nggplot(p, aes(value, fill = region))+\n  geom_density(aes(weight = occupancy),alpha = 0.75)+\n  geom_vline(aes(xintercept = mean, color = region), linewidth = 1)+\n  labs(x = 'Trait value', y = 'Density', fill = 'Region')+\n  facet_grid(~Trait)+\n  theme_bw()+\n  scale_color_manual(values = c('dodgerblue3', 'firebrick'))+\n  scale_fill_manual(values = c('dodgerblue3', 'firebrick'),\n                    labels = c('North/Northeast', 'West'))+\n  guides(color = 'none')+\n  theme(axis.text = element_text(size = 12, color = \"black\"),\n        axis.title = element_text(size = 14, color = \"black\"),\n        plot.title = element_text(size = 14, color = \"black\"),\n        panel.grid.major = element_blank(),\n        axis.text.x = element_text(size = 10,  color = \"black\"),\n        # axis.ticks.x = element_blank(),\n        axis.line = element_line(colour = \"black\"),\n        panel.grid.minor = element_blank(),\n        #panel.border = element_blank(),\n        panel.background = element_blank(),\n        legend.title = element_text(size = 12),\n        strip.text = element_text(size = 10),\n        legend.text = element_text(size = 10))"
  },
  {
    "objectID": "ws2.html",
    "href": "ws2.html",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "",
    "text": "This workshop uses hypervolumes to quantify the trophic niche of seagrass consumers. Hypervolumes are generated using mean and standard deviation resource use data from stable isotope mixing models. This process is repeated 50 times. Hypervolume overlap metrics, as well as size and centroid distance, are used to understand how consumers niches change between seasons.\nR script: github\nR script of workshop 2"
  },
  {
    "objectID": "ws2.html#seasonal-comparison-of-trophic-niche-of-seagrass-consumers",
    "href": "ws2.html#seasonal-comparison-of-trophic-niche-of-seagrass-consumers",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "",
    "text": "This workshop uses hypervolumes to quantify the trophic niche of seagrass consumers. Hypervolumes are generated using mean and standard deviation resource use data from stable isotope mixing models. This process is repeated 50 times. Hypervolume overlap metrics, as well as size and centroid distance, are used to understand how consumers niches change between seasons.\nR script: github\nR script of workshop 2"
  },
  {
    "objectID": "ws2.html#data",
    "href": "ws2.html#data",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "data",
    "text": "data\nThe data used for this vignette comes from James et al. 2022. The resource use data consists of mean and standard deviation of four resources for each species in each season based on mixing model outputs. Data comes from species collected across Florida Bay, USA.\n\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nlibrary(hypervolume)\n## Loading required package: Rcpp\nlibrary(truncnorm)\n\n# load all data\nd = read_csv('data/CESImixResults.csv')\n## Rows: 56 Columns: 7\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): species, season, source\n## dbl (4): mean, sd, lowend, highend\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nd\n## # A tibble: 56 × 7\n##    species             season source     mean    sd lowend highend\n##    &lt;chr&gt;               &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1 Bay anchovy         Wet    Algae     0.02  0.016  0       0.061\n##  2 Mojarra             Wet    Algae     0.004 0.004  0       0.013\n##  3 Pigfish             Wet    Algae     0.008 0.008  0       0.028\n##  4 Pinfish             Wet    Algae     0.028 0.018  0       0.062\n##  5 Pink shrimp         Wet    Algae     0.009 0.008  0       0.031\n##  6 Rainwater killifish Wet    Algae     0.016 0.017  0       0.063\n##  7 Silver perch        Wet    Algae     0.02  0.024  0       0.091\n##  8 Bay anchovy         Wet    Epiphytes 0.3   0.067  0.153   0.418\n##  9 Mojarra             Wet    Epiphytes 0.04  0.02   0.009   0.084\n## 10 Pigfish             Wet    Epiphytes 0.076 0.034  0.019   0.149\n## # ℹ 46 more rows"
  },
  {
    "objectID": "ws2.html#prepare-data",
    "href": "ws2.html#prepare-data",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "Prepare data",
    "text": "Prepare data\nHere a custom function HVvalues() is used to generate random points from mean and standard deviation data between end points (lower and upper bounds) using the truncnorm package. Values can then be z-scored.\n*** Note chose either column names or column numbers for ID_rows and names either work but must be the same - df = dataframe or tibble with each row containing - unique entry for making random points - ID_rows = vector of column names or numbers with id information - names = column name or number of name of measure variables - mean = column name or column number of df with mean - sd = column name or column number of df with sd - n = number of points to randomly generate - z_score = T or F, if T z-scores values - end_points = T or F for if random points need to be generated between - a high and low end point (e.g. 5% and 95% interval) - low and high required if end_points = T - low = column name or column number of df with lower bound to sample in - high = column name or column number of df with upper bound to sample in\n\nHVvalues = function(df, ID_rows, names, mean, sd, n, z_score = F,\n                    end_points = F, low = NULL, high = NULL){\n  require(tidyverse)\n  require(truncnorm)\n  \n  # check to see if information is needed to restrict where points are\n  if (end_points){\n    if (is_empty(df[,low]) | is_empty(df[,high])){\n      return(cat('Warning: low and/or high columns not specified \\n\n                  Specific and run again or end_points = F \\n'))\n    }\n  }\n  \n  # check to see if there are more \n  if(T %in% duplicated(df[,c(ID_rows,names)])){\n    return(cat('Warning: some of the rows contain duplicated information \\n\n                make sure data is correct \\n'))\n  }\n  \n  # rename variables to make code work\n  if (is.numeric(mean)){\n    names(df)[mean] = 'Mean'\n  }else {\n    df = df |&gt;  rename(Mean = mean)\n  }\n  \n  if (is.numeric(sd)){\n    names(df)[sd] = 'SD'\n  }else {\n    df = df |&gt;  rename(SD = sd)\n  }\n  \n  if (end_points){\n    if (is.numeric(low)){\n      names(df)[low] = 'lower'\n    }else {\n      df = df |&gt;  rename(lower = low)\n    }\n    \n    if (is.numeric(high)){\n      names(df)[high] = 'upper'\n    }else {\n      df = df |&gt;  rename(upper = high)\n    }\n  }\n  \n  # make sure the names is not numeric \n  if (is.numeric(names)){\n    names = names(df)[names]\n  }\n  \n  labs = unique(as.vector(df[,names])[[1]])\n  # generate random points within bounds\n  if(end_points){\n    \n    df_tot = df |&gt; slice(rep(1:n(), each=n))|&gt; \n      mutate(point = \n               truncnorm::rtruncnorm(1, a = lower, b = upper,\n                                     mean = Mean, sd = SD)) |&gt; \n      ungroup() |&gt; \n      mutate(num = rep(1:n, times=nrow(df))) |&gt;\n      dplyr::select(-Mean, -SD, -lower, -upper)|&gt;\n      pivot_wider(names_from = names, values_from = point)|&gt; \n      dplyr::select(-num)\n  }else {\n    # generate random points outside of bounds\n    df_tot = df |&gt; slice(rep(1:n(), each=n))|&gt;\n      mutate(point = \n               truncnorm::rtruncnorm(1, mean = Mean, sd = SD)) |&gt; \n      ungroup() |&gt; \n      mutate(num = rep(1:n, times=nrow(df))) |&gt;\n      dplyr::select(-Mean, -SD)|&gt;\n      pivot_wider(names_from = names, values_from = point)|&gt; \n      dplyr::select(-num)\n  }\n  if (z_score){\n    df_tot = df_tot  |&gt;  \n      mutate(across(all_of(labs), scale))\n  }\n  \n  return(df_tot)\n  \n}\n\n30 points are generated for each species in each season using map(), and this process is repeated 50 times.\n\n# number or iterations\nreps = 50\n\n# generate points and z-score across iterations\nset.seed(14)\ndf = d |&gt; \n  # duplicate for number of reps\n  slice(rep(1:n(), each=reps))|&gt; \n  mutate(i = rep(1:reps, times=nrow(d))) |&gt; \n  group_by(i) |&gt; \n  nest() |&gt; \n  # apply function to generate random points\n  mutate(points = map(data, \\(data) HVvalues(df = data, ID_rows = c('species', 'season'),\n                                             names = c('source'), \n                                             mean = 'mean', sd = 'sd', n = 30,\n                                             end_points = T, low = 'lowend', high = 'highend',\n                                             z_score = T))) |&gt; \n  select(i, points) |&gt; \n  unnest(points)\n## Warning: There were 3 warnings in `mutate()`.\n## The first warning was:\n## ℹ In argument: `points = map(...)`.\n## ℹ In group 1: `i = 1`.\n## Caused by warning:\n## ! Using an external vector in selections was deprecated in tidyselect 1.1.0.\n## ℹ Please use `all_of()` or `any_of()` instead.\n##   # Was:\n##   data %&gt;% select(low)\n## \n##   # Now:\n##   data %&gt;% select(all_of(low))\n## \n## See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n## ℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\ndf\n## # A tibble: 21,000 × 7\n## # Groups:   i [50]\n##        i species     season Algae[,1] Epiphytes[,1] Mangrove[,1] Seagrass[,1]\n##    &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n##  1     1 Bay anchovy Wet    -0.748           0.260          1.99      -0.330 \n##  2     1 Bay anchovy Wet     0.282           0.332          2.15      -0.466 \n##  3     1 Bay anchovy Wet     0.456          -0.0134         2.10       0.0613\n##  4     1 Bay anchovy Wet     0.186          -0.648          2.39      -0.260 \n##  5     1 Bay anchovy Wet    -0.478          -0.265          1.82      -0.232 \n##  6     1 Bay anchovy Wet     0.0711         -0.369          1.76       0.0775\n##  7     1 Bay anchovy Wet    -0.490          -0.675          1.66      -0.481 \n##  8     1 Bay anchovy Wet     0.000560       -0.465          2.24      -0.322 \n##  9     1 Bay anchovy Wet    -0.625          -0.158          2.34      -0.619 \n## 10     1 Bay anchovy Wet    -0.0106         -0.425          2.07      -0.670 \n## # ℹ 20,990 more rows"
  },
  {
    "objectID": "ws2.html#hypervolumes",
    "href": "ws2.html#hypervolumes",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "Hypervolumes",
    "text": "Hypervolumes\nHypervolumes are generated for each species and season and the size of each hypervolume is calculated.\n\ndf = df |&gt; \n  group_by(species, season, i) |&gt; \n  nest() |&gt; \n  mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = paste(species, season, i,sep = '_'),\n                                                     samples.per.point = 500,\n                                                     kde.bandwidth = estimate_bandwidth(data), \n                                                     sd.count = 3, \n                                                     quantile.requested = 0.95, \n                                                     quantile.requested.type = \"probability\", \n                                                     chunk.size = 1000, \n                                                     verbose = F)),\n         hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n\n** Do not try to view df in rstudio it will freeze your r since it is too big\n\nhead(df)\n## # A tibble: 6 × 7\n## # Groups:   species, season, i [6]\n##       i species             season data              hv         hv_size centroid\n##   &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;       &lt;dbl&gt; &lt;list&gt;  \n## 1     1 Bay anchovy         Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  3.27   &lt;dbl&gt;   \n## 2     1 Mojarra             Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  0.0909 &lt;dbl&gt;   \n## 3     1 Pigfish             Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  1.22   &lt;dbl&gt;   \n## 4     1 Pinfish             Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  3.09   &lt;dbl&gt;   \n## 5     1 Pink shrimp         Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  0.962  &lt;dbl&gt;   \n## 6     1 Rainwater killifish Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  6.32   &lt;dbl&gt;"
  },
  {
    "objectID": "ws2.html#hypervolume-metrics",
    "href": "ws2.html#hypervolume-metrics",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "Hypervolume metrics",
    "text": "Hypervolume metrics\nCombine each species and season to calculate the overlap and centroid distance of each species\n\nov_sn = df |&gt; \n  select(species, season, hv, hv_size) |&gt; \n  pivot_wider(names_from = season, values_from = c(hv,hv_size)) |&gt; \n  mutate(size_rat = hv_size_Dry/hv_size_Wet,\n         set = map2(hv_Wet,hv_Dry, \\(hv1, hv2) hypervolume_set(hv1, hv2, check.memory = F, verbose = F)),\n         ov = map(set, \\(set) hypervolume_overlap_statistics(set)),\n         dist_cent = map2_dbl(hv_Wet,hv_Dry, \\(hv1,hv2) hypervolume_distance(hv1, hv2, type = 'centroid', check.memory=F))) |&gt; \n  unnest_wider(ov) |&gt; \n  select(species, i, hv_size_Wet, hv_size_Dry, \n         size_rat, jaccard, sorensen,\n         uniq_Wet = frac_unique_1, uniq_Dry = frac_unique_2, \n         dist_cent)\n\n\n## Rows: 350 Columns: 10\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): species\n## dbl (9): i, hv_size_Wet, hv_size_Dry, size_rat, jaccard, sorensen, uniq_Wet,...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOverlap\n\ncols = c(\"Pinfish\" = 'yellow2',\n         \"Mojarra\" = 'slategray4',\n         \"Silver perch\" = 'snow3',\n         \"Bay anchovy\" = 'deepskyblue1',\n         \"Pigfish\" = 'orange', \n         \"Pink shrimp\" = 'pink',\n         \"Rainwater killifish\" = 'firebrick',\n         'All' = 'black')\n\ndf = ov_sn|&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(sorensen),\n            median = median(sorensen),\n            low = quantile(sorensen, 0.025),\n            up = quantile(sorensen, 0.975))\n\nggplot(df, aes(x = species, y = mean, color = species))+\n  geom_pointrange(aes(ymin = low, ymax = up),\n                  size = 1.5, linewidth = 1.5, fatten = 2, \n                  position=position_dodge(width = 0.5))+\n  labs(x = NULL, y = 'Niche overlap') +\n  scale_x_discrete(labels = c(\"Bay \\nanchovy\",\n                              \"Mojarra\",\n                              \"Pigfish\",\n                              \"Pinfish\",\n                              \"Pink \\nshrimp\",\n                              \"Rainwater \\nkillifish\",\n                              \"Silver \\nperch\" ))+\n  scale_color_manual(values = cols)+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"), \n        axis.text.x = element_text(size = 12, colour = \"black\"),\n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\n\nPercent unique\n\ndf = ov_sn|&gt;  \n  pivot_longer(uniq_Wet:uniq_Dry,names_to = 'season',\n               values_to = 'vol') |&gt; \n  group_by(species, season) |&gt; \n  summarize(mean = mean(vol),\n            median = median(vol),\n            low = quantile(vol, 0.025),\n            up = quantile(vol, 0.975)) |&gt; \n  mutate(season = factor(season, levels = c('uniq_Wet', \n                                            'uniq_Dry')))\n## `summarise()` has grouped output by 'species'. You can override using the\n## `.groups` argument.\n\nggplot(df, aes(x = species, y = mean, color = season))+\n  geom_pointrange(aes(ymin = low, ymax = up),\n                  size = 1.5, linewidth = 1.5, fatten = 2, \n                  position=position_dodge(width = 0.5))+\n  labs(x = 'Species', y = 'Niche volume unique',\n       color = 'Season') +\n  scale_fill_manual(values = c('uniq_Wet' = 'skyblue3', \n                               'uniq_Dry' = 'indianred3'),\n                    labels = c('uniq_Wet' = 'Wet', \n                               'uniq_Dry' = 'Dry')) +\n  scale_color_manual(values = c('uniq_Wet' = 'skyblue3', \n                                'uniq_Dry' = 'indianred3'),\n                     labels = c('uniq_Wet' = 'Wet',\n                                'uniq_Dry' = 'Dry')) +\n  scale_x_discrete(labels = c(\"Bay \\nanchovy\",\n                              \"Mojarra\",\n                              \"Pigfish\",\n                              \"Pinfish\",\n                              \"Pink \\nshrimp\",\n                              \"Rainwater \\nkillifish\",\n                              \"Silver \\nperch\" ))+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"), \n        axis.text.x = element_text(size = 12, colour = \"black\"),\n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'right',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n## Warning: No shared levels found between `names(values)` of the manual scale and the\n## data's fill values.\n\n\n\n\n\n\n\n\n\n\nHypervolume size\n\ndf = ov_sn |&gt; \n  pivot_longer(hv_size_Wet:hv_size_Dry,names_to = 'season',\n               values_to = 'vol') |&gt; \n  group_by(species, season) |&gt; \n  summarize(mean = mean(vol),\n            median = median(vol),\n            low = quantile(vol, 0.025),\n            up = quantile(vol, 0.975)) |&gt; \n  mutate(season = factor(season, levels = c('hv_size_Wet', \n                                            'hv_size_Dry')))\n## `summarise()` has grouped output by 'species'. You can override using the\n## `.groups` argument.\n\nggplot(df, aes(x = species, y = mean, color = season))+\n  geom_pointrange(aes(ymin = low, ymax = up),\n                  size = 1.5, linewidth = 1.5, fatten = 2, \n                  position=position_dodge(width = 0.5))+\n  labs(x = 'Species', y = 'Trophic niche width',\n       color = 'Season') +\n  scale_fill_manual(values = c('hv_size_Wet' = 'skyblue3', \n                               'hv_size_Dry' = 'indianred3'),\n                    labels = c('hv_size_Wet' = 'Wet', \n                               'hv_size_Dry' = 'Dry')) +\n  scale_color_manual(values = c('hv_size_Wet' = 'skyblue3', \n                                'hv_size_Dry' = 'indianred3'),\n                     labels = c('hv_size_Wet' = 'Wet',\n                                'hv_size_Dry' = 'Dry')) +\n  scale_x_discrete(labels = c(\"Bay \\nanchovy\",\n                              \"Mojarra\",\n                              \"Pigfish\",\n                              \"Pinfish\",\n                              \"Pink \\nshrimp\",\n                              \"Rainwater \\nkillifish\",\n                              \"Silver \\nperch\" ))+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"), \n        axis.text.x = element_text(size = 12, colour = \"black\"),\n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'right',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n## Warning: No shared levels found between `names(values)` of the manual scale and the\n## data's fill values.\n\n\n\n\n\n\n\n\n\n\nCentroid distance\n\ndf = ov_sn|&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(dist_cent),\n            median = median(dist_cent),\n            low = quantile(dist_cent, 0.025),\n            up = quantile(dist_cent, 0.975))\n\nggplot(df, aes(x = species, y = mean, color = species))+\n  geom_hline(aes(yintercept = 1), linewidth = 1, linetype = 'dashed')+\n  geom_pointrange(aes(ymin = low, ymax = up),\n                  size = 1.5, linewidth = 1.5, fatten = 2, \n                  position=position_dodge(width = 0.5))+\n  labs(x = NULL, y = 'Centroid distance') +\n  scale_x_discrete(labels = c(\"Bay \\nanchovy\",\n                              \"Mojarra\",\n                              \"Pigfish\",\n                              \"Pinfish\",\n                              \"Pink \\nshrimp\",\n                              \"Rainwater \\nkillifish\",\n                              \"Silver \\nperch\" ))+\n  scale_color_manual(values = cols)+\n  scale_y_continuous(limits = c(0,4.1))+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"), \n        axis.text.x = element_text(size = 12, colour = \"black\"),\n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))"
  },
  {
    "objectID": "usefulfx.html",
    "href": "usefulfx.html",
    "title": "Useful functions for hypervolumes",
    "section": "",
    "text": "This script contains information about the functions used in the hypervolume scripts.\nR script of useful functions"
  },
  {
    "objectID": "usefulfx.html#mergejoin",
    "href": "usefulfx.html#mergejoin",
    "title": "Useful functions for hypervolumes",
    "section": "Merge/Join",
    "text": "Merge/Join\nIf two data frames contain different columns of data, then they can be merged together with the family of join functions.\n+left_join() = uses left df as template and joins all matching columns from right df +right_join() = uses right df as template and joins all matching columns from left df +inner_join() = only matches columns contained in both dfs +full_join() = combines all rows in both dfs\n\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nleft = tibble(name = c('a', 'b', 'c'),\n              n = c(1, 6, 7), \n              bio = c(100, 43, 57))\n\nright = tibble(name = c('a', 'b', 'd', 'e'),\n               cals = c(500, 450, 570, 600))\n\nleft_join(left, right, by = 'name')\n## # A tibble: 3 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n## 3 c         7    57    NA\n\nright_join(left, right, by = 'name')\n## # A tibble: 4 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n## 3 d        NA    NA   570\n## 4 e        NA    NA   600\n\ninner_join(left, right, by = 'name')\n## # A tibble: 2 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n\nfull_join(left, right, by = 'name')\n## # A tibble: 5 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n## 3 c         7    57    NA\n## 4 d        NA    NA   570\n## 5 e        NA    NA   600\n\n# multiple matches\nfish = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n              year = rep(c(1999,2005,2020), each = 2),\n              catch = c(50, 60, 40, 50, 60, 100))\n\ncol = tibble(species = c('Salmon', 'Cod'),\n             coast = c('West', 'East'))\n\nleft_join(fish, col, by = 'species')\n## # A tibble: 6 × 4\n##   species  year catch coast\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n## 1 Salmon   1999    50 West \n## 2 Cod      1999    60 East \n## 3 Salmon   2005    40 West \n## 4 Cod      2005    50 East \n## 5 Salmon   2020    60 West \n## 6 Cod      2020   100 East"
  },
  {
    "objectID": "usefulfx.html#scaling-data",
    "href": "usefulfx.html#scaling-data",
    "title": "Useful functions for hypervolumes",
    "section": "scaling data",
    "text": "scaling data\nBecause hypervolumes can be generated with any continuous data as an axes, many of the times the units are not comparable. Blonder et al. 2014 & 2018 to convert all of the axes into the same units. This can be done by taking the z-score of the values to convert units into standard deviations. Z-scoring data can be done with the formula: \\[ z = \\frac{x_{i}-\\overline{x}}{sd} \\] Where \\(x_{i}\\) is a value, \\(\\overline{x}\\) is the mean, and \\(sd\\) is the standard deviation. By z-scoring each axis, 0 is the mean of that axis, a value of 1 means that the value is 1 standard deviation above the global mean of that axis, and a value of -1 is 1 standard deviation below the global mean of the axis. In R this can be done manually or with the scale() function.\n\nfish = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n              year = rep(c(1999,2005,2020), each = 2),\n              catch = c(50, 60, 40, 50, 60, 100))\n\n#\nfish = fish |&gt; \n  mutate(zcatch1 = (catch - mean(catch))/sd(catch), # manual\n         zcatch2 = scale(catch)) # with scale\n\nfish \n## # A tibble: 6 × 5\n##   species  year catch zcatch1 zcatch2[,1]\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Salmon   1999    50  -0.477      -0.477\n## 2 Cod      1999    60   0           0    \n## 3 Salmon   2005    40  -0.953      -0.953\n## 4 Cod      2005    50  -0.477      -0.477\n## 5 Salmon   2020    60   0           0    \n## 6 Cod      2020   100   1.91        1.91\n\n# center = mean, scale = sd\nfish$zcatch2\n##            [,1]\n## [1,] -0.4767313\n## [2,]  0.0000000\n## [3,] -0.9534626\n## [4,] -0.4767313\n## [5,]  0.0000000\n## [6,]  1.9069252\n## attr(,\"scaled:center\")\n## [1] 60\n## attr(,\"scaled:scale\")\n## [1] 20.97618"
  },
  {
    "objectID": "usefulfx.html#nesting-data",
    "href": "usefulfx.html#nesting-data",
    "title": "Useful functions for hypervolumes",
    "section": "nesting data",
    "text": "nesting data\nOne benefit of tibbles is that they can contain list columns. This means that we can make columns of tibbles that are nested within a dataset. Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns. Nesting is a implicitly summarising operation: you get one row for each group defined by the non-nested columns. This is useful in conjunction with other summaries that work with whole datasets, most notably models. This can be done with the nest() and then flattened with unnest()\n\nfish = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n              year = rep(c(1999,2005,2020), each = 2),\n              catch = c(50, 60, 40, 50, 60, 100))\n\n# using group_by\nfish_nest = fish |&gt; \n  group_by(species) |&gt; \n  nest()\n\nfish_nest\n## # A tibble: 2 × 2\n## # Groups:   species [2]\n##   species data            \n##   &lt;chr&gt;   &lt;list&gt;          \n## 1 Salmon  &lt;tibble [3 × 2]&gt;\n## 2 Cod     &lt;tibble [3 × 2]&gt;\nfish_nest$data\n## [[1]]\n## # A tibble: 3 × 2\n##    year catch\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1  1999    50\n## 2  2005    40\n## 3  2020    60\n## \n## [[2]]\n## # A tibble: 3 × 2\n##    year catch\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1  1999    60\n## 2  2005    50\n## 3  2020   100\n\n# using .by in nest\n# column name becomes data unless you change .key\nfish_nest2 = fish |&gt; \n  nest(.by = year, .key = 'df')\n\nfish_nest2\n## # A tibble: 3 × 2\n##    year df              \n##   &lt;dbl&gt; &lt;list&gt;          \n## 1  1999 &lt;tibble [2 × 2]&gt;\n## 2  2005 &lt;tibble [2 × 2]&gt;\n## 3  2020 &lt;tibble [2 × 2]&gt;\nfish_nest2$df\n## [[1]]\n## # A tibble: 2 × 2\n##   species catch\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 Salmon     50\n## 2 Cod        60\n## \n## [[2]]\n## # A tibble: 2 × 2\n##   species catch\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 Salmon     40\n## 2 Cod        50\n## \n## [[3]]\n## # A tibble: 2 × 2\n##   species catch\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 Salmon     60\n## 2 Cod       100"
  },
  {
    "objectID": "usefulfx.html#map",
    "href": "usefulfx.html#map",
    "title": "Useful functions for hypervolumes",
    "section": "map",
    "text": "map\n\npurr\nThe newest and new standard package with tidyverse is purr with its set of map() functions. Some similarity to plyr (and base) and dplyr functions but with more consistent names and arguments. Notice that map function can have some specification for the type of output. + map() makes a list. + map_lgl() makes a logical vector. + map_int() makes an integer vector. + map_dbl() makes a double vector. + map_chr() makes a character vector.\n\ndf = iris  |&gt; \n  select(-Species)\n#summary statistics\nmap_dbl(df, mean)\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\n# using map with mutate and nest\nd = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n           year = rep(c(1999,2005,2020), each = 2),\n           catch = c(50, 60, 40, 50, 60, 100)) |&gt; \n  nest(.by = species) |&gt; \n  mutate(correlation = map(data, \\(data) cor.test(data$year, data$catch)))\n\nd\n## # A tibble: 2 × 3\n##   species data             correlation\n##   &lt;chr&gt;   &lt;list&gt;           &lt;list&gt;     \n## 1 Salmon  &lt;tibble [3 × 2]&gt; &lt;htest&gt;    \n## 2 Cod     &lt;tibble [3 × 2]&gt; &lt;htest&gt;\nd$correlation\n## [[1]]\n## \n##  Pearson's product-moment correlation\n## \n## data:  data$year and data$catch\n## t = 0.96225, df = 1, p-value = 0.5122\n## alternative hypothesis: true correlation is not equal to 0\n## sample estimates:\n##       cor \n## 0.6933752 \n## \n## \n## [[2]]\n## \n##  Pearson's product-moment correlation\n## \n## data:  data$year and data$catch\n## t = 1.963, df = 1, p-value = 0.3\n## alternative hypothesis: true correlation is not equal to 0\n## sample estimates:\n##       cor \n## 0.8910421\n\n# extract information from list\nd = d |&gt; \n  mutate(r = map_dbl(correlation, \\(x) x$estimate),\n         p = map_dbl(correlation, \\(x) x$p.value))\n\nd\n## # A tibble: 2 × 5\n##   species data             correlation     r     p\n##   &lt;chr&gt;   &lt;list&gt;           &lt;list&gt;      &lt;dbl&gt; &lt;dbl&gt;\n## 1 Salmon  &lt;tibble [3 × 2]&gt; &lt;htest&gt;     0.693 0.512\n## 2 Cod     &lt;tibble [3 × 2]&gt; &lt;htest&gt;     0.891 0.300\n\nSometimes, there are multiple arguments required for the function you are mapping. You can provide two lists (or columns) using map2(). If you have more than two lists needed, you can use pmap(). These functions work the same as map() based on the desired output.\n\nlibrary(performance)\n# using map2 and pmap\ndf = iris  |&gt; \n  nest(.by = Species) |&gt; \n  mutate(sw = map(data, \\(data) lm(Sepal.Length ~ Sepal.Width, data = data)),\n         pl = map(data, \\(data) lm(Sepal.Length ~ Petal.Length, data = data)),\n         mod_c = map2(sw,pl, \\(sw,pl) compare_performance(sw,pl)),\n         top = map_chr(mod_c, \\(x) x$Name[which.min(x$AICc)]),\n         sum_top = pmap(list(sw,pl,top), \\(x,y,z) \n                        if (z == 'sw'){\n                          summary(x)\n                        }else{\n                          summary(y)}),\n         p = map_dbl(sum_top, \\(s) s$coefficients[2, 4]))\n\n\ndf\n## # A tibble: 3 × 8\n##   Species    data              sw     pl    mod_c      top   sum_top           p\n##   &lt;fct&gt;      &lt;list&gt;            &lt;list&gt; &lt;lis&gt; &lt;list&gt;     &lt;chr&gt; &lt;list&gt;        &lt;dbl&gt;\n## 1 setosa     &lt;tibble [50 × 4]&gt; &lt;lm&gt;   &lt;lm&gt;  &lt;cmpr_prf&gt; sw    &lt;smmry.lm&gt; 6.71e-10\n## 2 versicolor &lt;tibble [50 × 4]&gt; &lt;lm&gt;   &lt;lm&gt;  &lt;cmpr_prf&gt; pl    &lt;smmry.lm&gt; 2.59e-10\n## 3 virginica  &lt;tibble [50 × 4]&gt; &lt;lm&gt;   &lt;lm&gt;  &lt;cmpr_prf&gt; pl    &lt;smmry.lm&gt; 6.30e-16"
  },
  {
    "objectID": "readings.html#workshop-week-1",
    "href": "readings.html#workshop-week-1",
    "title": "BSC6926-B53B Spring 2025",
    "section": "Workshop week 1",
    "text": "Workshop week 1\nBlonder 2017. Hypervolume concepts in niche- and trait-based ecology. Ecography"
  },
  {
    "objectID": "readings.html#workshop-week-2",
    "href": "readings.html#workshop-week-2",
    "title": "BSC6926-B53B Spring 2025",
    "section": "Workshop week 2",
    "text": "Workshop week 2\nLesser et al. 2020. Trophic niche size and overlap decreases with increasing ecosystem productivity. Oikos"
  },
  {
    "objectID": "readings.html#workshop-week-3",
    "href": "readings.html#workshop-week-3",
    "title": "BSC6926-B53B Spring 2025",
    "section": "Workshop week 3",
    "text": "Workshop week 3\nJones et al. 2021. Stress gradients interact with disturbance to reveal alternative states in salt marsh: Multivariate resilience at the landscape scale. Journal of Ecology"
  },
  {
    "objectID": "readings.html#workshop-week-4",
    "href": "readings.html#workshop-week-4",
    "title": "BSC6926-B53B Spring 2025",
    "section": "Workshop week 4",
    "text": "Workshop week 4\nHelsen et al. 2020. Inter- and intraspecific trait variation shape multidimensional trait overlap between two plant invaders and the invaded communities"
  },
  {
    "objectID": "comp.html",
    "href": "comp.html",
    "title": "BSC6926-B53B Spring 2025",
    "section": "",
    "text": "R and RStudio\nR and RStudio are separate downloads and installations. R is the underlying statistical computing environment, but using R alone is no fun. RStudio is a graphical integrated development environment (IDE) that makes using R much easier and more interactive. You need to install R before you install RStudio. In the sections below are the instructions for installing R and R Studio on your operating system.\n\nWindows\n\nIf you already have R and RStudio installed\n\nOpen RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio.\nTo check which version of R you are using, start RStudio and the first thing that appears in the console indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it. You can check here for more information on how to remove old versions from your system if you wish to do so.\n\n\n\nIf you don’t have R and RStudio installed\n\nDownload R from the CRAN website.\nRun the .exe file that was just downloaded\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Windows 10/11 (where x, y, and z represent version numbers)\nDouble click the file to install it\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages.\n\n\n\n\nmacOS\n\nIf you already have R and RStudio installed\n\nOpen RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio.\nTo check the version of R you are using, start RStudio and the first thing that appears on the terminal indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it.\n\n\n\nIf you don’t have R and RStudio installed\n\nDownload R from the CRAN website.\nSelect the .pkg file for the latest R version\nDouble click on the downloaded file to install R\nIt is also a good idea to install XQuartz (needed by some packages)\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Mac OS X 10.15+ (64-bit) (where x, y, and z represent version numbers)\nDouble click the file to install RStudio\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages.\n\n\n\n\nLinux\n\nFollow the instructions for your distribution from CRAN, they provide information to get the most recent version of R for common distributions. For most distributions, you could use your package manager (e.g., for Debian/Ubuntu run sudo apt-get install r-base, and for Fedora sudo yum install R), but we don’t recommend this approach as the versions provided by this are usually out of date. In any case, make sure you have at least R 3.3.1.\nGo to the RStudio download page\nUnder Installers select the version that matches your distribution, and install it with your preferred method (e.g., with Debian/Ubuntu sudo dpkg -i   rstudio-x.yy.zzz-amd64.deb at the terminal).\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSC6926-B53B Spring 2025",
    "section": "",
    "text": "BSC6926-B53B: Using a multivariate tool to assess population and community dynamics\nThis is the course website for the R workshop Using a multivariate tool to assess population and community dynamics. This website will have the Quarto markdown lessons for each workshop. Find the course schedule and Syllabus here. This course will be based in R and information about downloading R and Rstudio can be found here.\n\n\nClass Resources\n\nZoom link\nGithub repository\n\n\n\nR Resources\n\nR for Data Science by Hadley Wickham and Garret Grolemund – An introduction to programming with R: https://r4ds.hadley.nz/\n\nQuick-R by datacamp: Quick overview on R programming and statistical approaches. There are more tutorials, but you will be required to register\n\nRStudio Cloud Training Exercises: https://rstudio.cloud/learn/primers\n\nRstudio: learn R https://education.rstudio.com/learn/beginner/"
  },
  {
    "objectID": "sylb.html",
    "href": "sylb.html",
    "title": "Syllabus",
    "section": "",
    "text": "BSC6926-B53B: Using a multivariate tool to assess population and community dynamics \nSpring 2025\nFridays 10:00 am - 1:00 pm\nFormat:\n- In-person + Face-to-Face: MSB 113\n- Remote: Zoom link\n\nInstructors:\nW. Ryan James\nwjames@fiu.edu\nJustin Lesser\njlesser@fiu.edu\nRolando Santos\nrsantosc@fiu.edu\n\n\nCourse description and learning outcome:\nEcosystem structure and function are complex due to ecological and physicochemical processes occurring across multiple spatiotemporal scales. Due to advances in computing and statistical programming, new multivariate tools incorporating multi-scalar ecosystem responses are becoming increasingly popular to characterize ecosystem conditions and stability. One such technique is hypervolume modelling, which is based on Hutchinson’s n-dimensional hypervolume concept that describes the ecological requirements for an individual, species, or population to persist in a location. Due to the geometrical nature of hypervolumes and the flexibility of defining variables as axes, the hypervolume concept has been adapted to other ecological questions and processes above the species level (e.g., communities, habitats). The diverse use of hypervolumes includes quantifying species and community niche space, quantifying the state, stability, and resilience of ecosystems following disturbance and restoration, and how functional diversity, life-history strategies, and habitat filtering of community assemblages vary across environmental and disturbance gradients. During the workshop, we will discuss papers and cases related to these topics, and demonstrate how to conduct these assessments in R. These case studies will be followed by a walkthrough of the data and R code used to conduct the hypervolume analyses. R scripts and markdown/quarto files will be provided to encourage students to conduct hypervolume analysis on their own data and promote comparative cross-system studies.\nHypervolume modelling is a multivariate tool that can be used to characterize ecosystem condition and stability. Multiple case studies using hypervolumes in coastal ecosystems with the data and R code used to conduct the analyses will be presented.\n\n\nWhen and where:\nIn-Person Sessions: Lectures and hands-on programming/modeling exercises will be an integral part of the workshop’s learning experience; thus, most sessions will be based on face-to-face meetings to facilitate learning and assistance during the workshop sessions. In-person sessions will be offered only at BBC (MSB 113).\nCANVAS: We will upload workshop materials on CANVAS and communicate via CANVAS and email. We will announce any changes to the schedule one week or more in advance.\nOffice hours: By appointment - We are constantly in/out of the office; thus, please send an email or talk to us after the workshop lectures to set up an office meeting.\n\n\nTeaching schedule:\nEvery Friday from 03/14/2025-04/18/2025 (3hrs, 10 am to 1:00 pm) at BBC MSB-113. Changes to this schedule and other announcements regarding the course will be posted in CANVAS (sent to FIU email addresses only).\n\n\n\nLearning materials:\nRequired R scripts/files and readings for the workshop exercises will be posted to CANVAS and the workshop website https://seascapeecologylab-workshops.github.io/BSC6926-B53B_Spring2025.\nPlease bring a laptop to class so you can follow the R scripts during class and perform workshop exercises. Please, contact us if you do not have access to a laptop\nR and R studio are free software environment for statistical computing and graphics required for the workshop. Please download it to your laptops:\n   Download R: https://cran.r-project.org/bin/windows/base/\n   Download Rstudio: https://www.rstudio.com/products/rstudio/download/\nMaterials and links on R programming and statistical analyses helpful in learning R and the workshop exercises/homework/project\n\nR for Data Science by Hadley Wickham and Garret Grolemund – An introduction to programming with R: https://r4ds.had.co.nz/\n\nQuick-R by datacamp: Quick overview on R programming and statistical approaches.There are more tutorials, but you will be required to register\n\nRStudio Cloud Training Exercises: https://rstudio.cloud/learn/primers\n\nVirtual Ecology Portal/EcoVirtual R Package: Website that provides various examples of population and community models that will be discussed in class and the workshop. There is also an R package (EcoVirtual) you can use to run various models included on this website: http://ecovirtual.ib.usp.br/doku.php?id=start\n\nModernDive: Introductory book on R and statistical inference: https://moderndive.com/index.html\n\n\n\nTeaching:\nLectures and R programming exercises will be part of each workshop session to introduce several topics on multidimensional and hypervolume-based niche analysis. The grade will be based on class participation (50%), manuscript contributions (25%), and final presentation (25%).\n\n\nGrading:\nClass participation (Total 50 pts) will count for 50% of your final grade. Class attendance, participation in paper/topic discussions, including asking and answering questions, insightful comments and suggestions, and helping peers.\nManuscript Contribution (Total 25 pts) will count for 25% of your grade. As part of the workshop, we will work together to prepare a manuscript on a research topic developed during the workshop. The grade will be based on your contribution to the manuscript, which will entail providing two or three paragraphs.\nFinal Presentation (Total 25 pts) will count for 25% of your grade. Each student will receive a dataset that will contribute to the manuscript that we will prepare as a team. Thus, as part of this grade, you must present the results of a hypervolume analysis of the dataset assigned to you.\n\n\nRubric: \nGrade scale is A (pass): 100-80; C (fail): &lt;79. This is a graduate-level workshop; thus, if you are taking this course, it is because you have a genuine interest in ecological learning and developing your academic career. For this reason, final grades will be pass (A) or no pass (C). Students who don’t commit to the work will be given a C or less. Students will be graded on their performance in the above areas ONLY. Future career plans will have ZERO influence on the grade you receive in this class. Incomplete grades will be considered only under extraordinary circumstances.\n\n\nMake-up policy: \nI will provide make-up opportunities only when students present valid excuses (e.g., medical/family emergencies, COVID-19-related emergencies and precautions, major fieldwork trips, or conferences). Regardless of the reason, please contact me to identify alternative methods for completing course requirements, depending on the type of assignment missed and the severity of missed assignments. Below is a guideline of potential make-up scenarios/methods (Subject to change): - Participation – Due to the limited number of sessions, I will award a participation points for only one missed session with a validated and accepted excuse. - Manuscript/Final Presentation – Make-up alternatives only for medical (including COVID-19) emergencies. The make-up will consist of submitting the homework materials at a later date (e.g., 48-72 hours after the submission deadline), depending on the gravity of the emergency.\n\n\nFIU Discrimination, Harassment and Sexual Misconduct Policy Statement (Title IX):\nFlorida International University (the University) is committed to encouraging and sustaining a learning and living environment that is free from discrimination based on sex, including gender, gender expression, gender identity, and sexual orientation. Discrimination based on sex encompasses Sexual Misconduct, Sexual Harassment, Gender-Based Harassment, Domestic Violence, Dating Violence, and/or Stalking https://dei.fiu.edu/civil-rights-and-accessibility/index.html.\n\nFIU is committed to eliminating sexual harassment. In accordance with the FIU Faculty Senate guidelines, this syllabus includes a warning that any misconduct will be reported. FIU’s sexual harassment policy is available at: https://dei.fiu.edu/civil-rights-and-accessibility/sexual-misconduct/index.html\n\n\nProfessional and academic integrity:\nStudents are encouraged to employ critical thinking and rely on data and verifiable sources to interrogate all assigned readings and subject matter in this course as a way of determining whether they agree with their classmates and/or their instructor. No lesson is intended to espouse, promote, advance, inculcate, or compel a particular feeling, perception, viewpoint, or belief.\nFIU is a community dedicated to generating and imparting knowledge through excellent teaching and research, the rigorous and respectful exchange of ideas, and community service. All students should respect others’ right to have an equitable opportunity to learn and honestly demonstrate the quality of their learning. Therefore, all students are expected to adhere to a standard of academic conduct, which demonstrates respect for themselves, their fellow students, and the University’s educational mission. The University deems all students to understand that if they are found responsible for academic misconduct (e.g., cheating, plagiarism, academic dishonesty), they will be subject to the Academic Misconduct procedures and sanctions, as outlined in the Student Handbook.\n\n\nHonesty Code Statement:\nFIU defines academic misconduct in the Student Conduct and Honor Code (Code) as “any act or omission by a Student, which violates the concept of academic integrity and undermines the academic mission of the University in violation of the Code.” Code violations include, but are not limited to: academic dishonesty, bribery, cheating, commercial use, complicity, falsification, and plagiarism. The Code is available here: https://regulations.fiu.edu/regulation=FIU-2501"
  },
  {
    "objectID": "ws1.html",
    "href": "ws1.html",
    "title": "Workshop 1: Introduction to hypervolumes",
    "section": "",
    "text": "This workshop discusses working with community trait data by introducing the hypervolume package by going over how to prepare the data and how to build hypervolumes in multiple ways.\n\nR script: github\nR script of workshop 1\nabundance data\ntrait data"
  },
  {
    "objectID": "ws1.html#getting-to-know-the-basics",
    "href": "ws1.html#getting-to-know-the-basics",
    "title": "Workshop 1: Introduction to hypervolumes",
    "section": "",
    "text": "This workshop discusses working with community trait data by introducing the hypervolume package by going over how to prepare the data and how to build hypervolumes in multiple ways.\n\nR script: github\nR script of workshop 1\nabundance data\ntrait data"
  },
  {
    "objectID": "ws1.html#hypervolumes",
    "href": "ws1.html#hypervolumes",
    "title": "Workshop 1: Introduction to hypervolumes",
    "section": "Hypervolumes",
    "text": "Hypervolumes\nHypervolumes are a multidimensional tool that is based on Hutchinson’s n-dimensional niche concept and we can build them with the hypervolume package.\n\nPreparing the data\nTypically we have a dataset that has the abundance of the community and another that has trait data. Therefore we need to combine them. Also, we can’t have NAs, so we have to filter any missing data one we combine the datasets.\n\nlibrary(tidyverse)\n# abundance data\nab = read_csv('data/abundHermine.csv')\n## Rows: 68 Columns: 5\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): site, species, hur, period\n## dbl (1): abund\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# trait data\ntr = read_csv('data/fishTraits.csv')\n## Rows: 46 Columns: 7\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): species, project, Class, Order\n## dbl (3): trophic_level, temp_preference, generation_time\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# combine\ndf = left_join(ab, tr, by = 'species') |&gt; \n  drop_na()\n\nNow we can make the hypervolume by selecting the data to be included and z-scoring\n\ndf_before = df |&gt; \n  mutate(across(trophic_level:generation_time, scale)) |&gt; \n  filter(period == 'Before') |&gt; \n  select(trophic_level, temp_preference, generation_time) \n  \n\ndf_before\n## # A tibble: 23 × 3\n##    trophic_level[,1] temp_preference[,1] generation_time[,1]\n##                &lt;dbl&gt;               &lt;dbl&gt;               &lt;dbl&gt;\n##  1            -0.633              -0.597              -1.02 \n##  2            -0.913              -0.147              -1.16 \n##  3            -0.493              -0.957              -0.728\n##  4             0.974               1.20                1.44 \n##  5            -0.144              -0.147               1.44 \n##  6            -0.563               1.92               -0.393\n##  7            -0.563               0.393               1.03 \n##  8             0.345              -0.597               0.524\n##  9            -0.633              -0.597              -1.02 \n## 10            -0.913              -0.147              -1.16 \n## # ℹ 13 more rows\n\n\n\nBuilding hypervolumes\nWith a nested dataset of our columns that we want to build hypervolumes for we can use mutate() and map() to generate the hypervolume.\n\nlibrary(hypervolume)\n## Loading required package: Rcpp\n\nhv_before = hypervolume_gaussian(df_before, name = 'Before',\n                                              samples.per.point = 1000,\n                                              kde.bandwidth = estimate_bandwidth(df_before), \n                                              sd.count = 3, \n                                              quantile.requested = 0.95, \n                                              quantile.requested.type = \"probability\", \n                                              chunk.size = 1000, \n                                              verbose = F)\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n\nhv_before\n## ***** Object of class Hypervolume *****\n## Name: Before\n## Method: Gaussian kernel density estimate\n## Number of data points (after weighting): 23\n## Dimensionality: 3\n## Volume: 78.068034\n## Random point density: 171.606729\n## Number of random points: 13397\n## Random point values:\n##  min: 0.000\n##  mean: 0.001\n##  median: 0.000\n##  max:0.003\n## Parameters:\n##  kde.bandwidth: 0.6189038 0.6189038 0.6189038\n##  kde.method: silverman\n##  samples.per.point: 1000\n##  sd.count: 3\n##  quantile.requested: 0.95\n##  quantile.requested.type: probability\n\nThis can also be done to multiple at the same time with nest().\n\ndf = df |&gt; \n  select(period, trophic_level, temp_preference, generation_time) |&gt; \n  mutate(across(trophic_level:generation_time, scale)) |&gt; \n  group_by(period) |&gt; \n  nest()\n\ndf\n## # A tibble: 2 × 2\n## # Groups:   period [2]\n##   period data             \n##   &lt;chr&gt;  &lt;list&gt;           \n## 1 Before &lt;tibble [23 × 3]&gt;\n## 2 After  &lt;tibble [25 × 3]&gt;\n\ndf = df |&gt; \n  mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = period,\n                                              samples.per.point = 1000,\n                                              kde.bandwidth = estimate_bandwidth(data), \n                                              sd.count = 3, \n                                              quantile.requested = 0.95, \n                                              quantile.requested.type = \"probability\", \n                                              chunk.size = 1000, \n                                              verbose = F)))\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n\nhead(df)\n## # A tibble: 2 × 3\n## # Groups:   period [2]\n##   period data              hv        \n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;    \n## 1 Before &lt;tibble [23 × 3]&gt; &lt;Hypervlm&gt;\n## 2 After  &lt;tibble [25 × 3]&gt; &lt;Hypervlm&gt;\ndf$hv\n## [[1]]\n## ***** Object of class Hypervolume *****\n## Name: Before\n## Method: Gaussian kernel density estimate\n## Number of data points (after weighting): 23\n## Dimensionality: 3\n## Volume: 70.900748\n## Random point density: 188.883762\n## Number of random points: 13392\n## Random point values:\n##  min: 0.000\n##  mean: 0.001\n##  median: 0.000\n##  max:0.004\n## Parameters:\n##  kde.bandwidth: 0.5996196 0.5570341 0.6430513\n##  kde.method: silverman\n##  samples.per.point: 1000\n##  sd.count: 3\n##  quantile.requested: 0.95\n##  quantile.requested.type: probability\n## \n## [[2]]\n## ***** Object of class Hypervolume *****\n## Name: After\n## Method: Gaussian kernel density estimate\n## Number of data points (after weighting): 25\n## Dimensionality: 3\n## Volume: 90.256157\n## Random point density: 160.354712\n## Number of random points: 14473\n## Random point values:\n##  min: 0.000\n##  mean: 0.000\n##  median: 0.000\n##  max:0.002\n## Parameters:\n##  kde.bandwidth: 0.6391039 0.6706801 0.5990887\n##  kde.method: silverman\n##  samples.per.point: 1000\n##  sd.count: 3\n##  quantile.requested: 0.95\n##  quantile.requested.type: probability\n\n\n\nplotting hypervolumes\nWe can plot multiple hypervolumes by joining them together\n\nhvj = hypervolume_join(df$hv[[1]], df$hv[[2]])\n\nplot(hvj, pairplot = T, colors=c('dodgerblue3','cyan2'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)\n\n\n\n\n\n\n\n\n\n\nhypervolume metrics\nThe geometry of hypervolumes are useful when characterizing and comparing hypervolumes. Hypervolume size represents the variation of the data, centroid distance compares the euclidian distance between two hypervolume centroids (mean conditions), and overlap measures the simularity of hypervolumes.\n\n# size \ndf = df |&gt; \n  mutate(hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n\nhead(df)\n## # A tibble: 2 × 4\n## # Groups:   period [2]\n##   period data              hv         hv_size\n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;       &lt;dbl&gt;\n## 1 Before &lt;tibble [23 × 3]&gt; &lt;Hypervlm&gt;    70.9\n## 2 After  &lt;tibble [25 × 3]&gt; &lt;Hypervlm&gt;    90.3\n\n# centroid distance \nhypervolume_distance(df$hv[[1]], df$hv[[2]], type = 'centroid', check.memory=F)\n## [1] 0.2435712\n\n# overlap \nhvset = hypervolume_set(df$hv[[1]], df$hv[[2]], check.memory = F)\n## Choosing num.points.max=53958 (use a larger value for more accuracy.)\n## Using minimum density of 160.354712\n## Retaining 11369 points in hv1 and 14473 points in hv2.\n## Beginning ball queries... \n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## Finished ball queries.\n\nhypervolume_overlap_statistics(hvset)\n##       jaccard      sorensen frac_unique_1 frac_unique_2 \n##    0.75154245    0.85814929    0.02471633    0.23386566\n\n\n\nWeight hypervolume input\nThe above hypervolume is just based on the traits using presence of species, but we can weight the points to shape the hypervolume based on abundance\n\n\n#prep data\ndf_w = left_join(ab, tr, by = 'species') |&gt; \n  drop_na() |&gt; \n  select(period, abund, trophic_level, temp_preference, generation_time) |&gt; \n  mutate(across(trophic_level:generation_time, scale)) |&gt; \n  group_by(period) |&gt; \n  nest(weight = abund, data = trophic_level:generation_time) \ndf_w\n## # A tibble: 2 × 3\n## # Groups:   period [2]\n##   period weight            data             \n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           \n## 1 Before &lt;tibble [23 × 1]&gt; &lt;tibble [23 × 3]&gt;\n## 2 After  &lt;tibble [25 × 1]&gt; &lt;tibble [25 × 3]&gt;\n\n# make hypervolumes\ndf_w = df_w |&gt; \n    mutate(hv = map2(data,weight, \\(data,weight) hypervolume_gaussian(data, name = paste(period,'weighted',sep = '_'),\n                                                      weight = weight$abund,\n                                                      samples.per.point = 1000,\n                                                      kde.bandwidth = estimate_bandwidth(data), \n                                                      sd.count = 3, \n                                                      quantile.requested = 0.95, \n                                                      quantile.requested.type = \"probability\", \n                                                      chunk.size = 1000, \n                                                      verbose = F)),\n           hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Warning: There were 2 warnings in `mutate()`.\n## The first warning was:\n## ℹ In argument: `hv = map2(...)`.\n## ℹ In group 1: `period = \"After\"`.\n## Caused by warning in `hypervolume_gaussian()`:\n## ! The sum of the weights must be equal to 1. Normalizing the weights.\n## ℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\ndf_w\n## # A tibble: 2 × 5\n## # Groups:   period [2]\n##   period weight            data              hv         hv_size\n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;            &lt;list&gt;       &lt;dbl&gt;\n## 1 Before &lt;tibble [23 × 1]&gt; &lt;tibble [23 × 3]&gt; &lt;Hypervlm&gt;    19.0\n## 2 After  &lt;tibble [25 × 1]&gt; &lt;tibble [25 × 3]&gt; &lt;Hypervlm&gt;    24.4\n\n\nhvj_w = hypervolume_join(df_w$hv[[1]], df_w$hv[[2]])\n\nplot(hvj_w, pairplot = T, colors=c('dodgerblue3','cyan2'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)\n\n\n\n\n\n\n\n\n# centroid distance \nhypervolume_distance(df_w$hv[[1]], df_w$hv[[2]], type = 'centroid', check.memory=F)\n## [1] 0.01744452\n\n# overlap \nhvset_w = hypervolume_set(df_w$hv[[1]], df_w$hv[[2]], check.memory = F)\n## Choosing num.points.max=53958 (use a larger value for more accuracy.)\n## Using minimum density of 160.631671\n## Retaining 3047 points in hv1 and 3921 points in hv2.\n## Beginning ball queries... \n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## Finished ball queries.\n\nhypervolume_overlap_statistics(hvset_w)\n##       jaccard      sorensen frac_unique_1 frac_unique_2 \n##    0.74999133    0.85713720    0.02001969    0.23833902\n\n\n\nFrom mean and sd\nSometimes, we do not have enough points to meet assumptions to make a hypervolume. Therefore, we can simulate random points based on mean and sd of our axes. We can then simulate the information needed and make our hypervolumes.\n\n# mean and sd\ndf_m = left_join(ab, tr, by = 'species') |&gt; \n  drop_na() |&gt; \n  pivot_longer(trophic_level:generation_time, names_to = 'trait', values_to = 'value') |&gt; \n  group_by(period,trait) |&gt; \n  summarize(mean = mean(value),\n            sd = sd(value))\n## `summarise()` has grouped output by 'period'. You can override using the\n## `.groups` argument.\n\ndf_m\n## # A tibble: 6 × 4\n## # Groups:   period [2]\n##   period trait            mean    sd\n##   &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n## 1 After  generation_time  3.10 1.79 \n## 2 After  temp_preference 25.1  1.22 \n## 3 After  trophic_level    3.52 0.150\n## 4 Before generation_time  3.35 1.90 \n## 5 Before temp_preference 25.0  1.00 \n## 6 Before trophic_level    3.54 0.139\n\n#generate points \n# number of points \nn = 50 \n\ndf_tot = df_m |&gt; slice(rep(1:n(), each=n))|&gt;\n      mutate(point = map2_dbl(mean,sd, \\(mean,sd) rnorm(1,mean =mean,sd =sd))) |&gt; \n      group_by(period, trait) |&gt; \n      mutate(num = row_number()) |&gt;\n      ungroup() |&gt; \n      select(-mean, -sd)|&gt;\n      pivot_wider(names_from = trait, values_from = point)|&gt; \n      select(-num) |&gt; \n      mutate(across(generation_time:trophic_level,scale)) |&gt; \n      group_by(period) |&gt; \n      nest()\n\n\n# generate hypervolumes\ndf_tot = df_tot |&gt; \n  mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = period,\n                                              samples.per.point = 1000,\n                                              kde.bandwidth = estimate_bandwidth(data), \n                                              sd.count = 3, \n                                              quantile.requested = 0.95, \n                                              quantile.requested.type = \"probability\", \n                                              chunk.size = 1000, \n                                              verbose = F)),\n         hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n\n\n#plot\nhvj_tot = hypervolume_join(df_tot$hv[[1]], df_tot$hv[[2]])\n\nplot(hvj_tot, pairplot = T, colors=c('dodgerblue3','cyan2'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)\n\n\n\n\n\n\n\n\n# centroid distance \nhypervolume_distance(df_tot$hv[[1]], df_tot$hv[[2]], type = 'centroid', check.memory=F)\n## [1] 0.173522\n\n# overlap \nhvset_tot = hypervolume_set(df_tot$hv[[1]], df_tot$hv[[2]], check.memory = F)\n## Choosing num.points.max=53958 (use a larger value for more accuracy.)\n## Using minimum density of 251.407651\n## Retaining 25684 points in hv1 and 24089 points in hv2.\n## Beginning ball queries... \n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## Finished ball queries.\n\nhypervolume_overlap_statistics(hvset_tot)\n##       jaccard      sorensen frac_unique_1 frac_unique_2 \n##     0.5654774     0.7224345     0.2999922     0.2536543"
  },
  {
    "objectID": "ws3.html",
    "href": "ws3.html",
    "title": "Workshop 3: Ecosystem stability",
    "section": "",
    "text": "This vignette uses hypervolumes to understand the temporal stability of seagrass ecosystems. Hypervolumes are generated using common seagrass monitoring metrics yearly from 2007-2023 across four basins in Florida Bay, USA. Centroid distance is used to compare mean conditions across all years to determine temporal stability and variable importance is calculated to determine the metric driving stability within each basin.\nR script: github\nR script of workshop 3"
  },
  {
    "objectID": "ws3.html#stability-of-seagrass-ecosystems-in-florida-bay",
    "href": "ws3.html#stability-of-seagrass-ecosystems-in-florida-bay",
    "title": "Workshop 3: Ecosystem stability",
    "section": "",
    "text": "This vignette uses hypervolumes to understand the temporal stability of seagrass ecosystems. Hypervolumes are generated using common seagrass monitoring metrics yearly from 2007-2023 across four basins in Florida Bay, USA. Centroid distance is used to compare mean conditions across all years to determine temporal stability and variable importance is calculated to determine the metric driving stability within each basin.\nR script: github\nR script of workshop 3"
  },
  {
    "objectID": "ws3.html#data",
    "href": "ws3.html#data",
    "title": "Workshop 3: Ecosystem stability",
    "section": "data",
    "text": "data\nThe data used for this example comes from the South Florida Fisheries Habitat Assessment Program which monitors seagrass habitats annually using quadrat samples. The benthic cover data consists of data from 4 basins and measures 6 metrics of the SAV community. Data is averaged across stations for each metric.\n\nBASIN = Basin sampled\nYEAR = year of monitoring\nSTATION = monitoring station\nTT = Thalassia testudium percent cover\nHW = Halodule wrightii percent cover\nSF = Syringodium filiforme percent cover\nTMA = total macroalgae percent cover\nTDR = total drift algae percent cover\nsg_rich = seagrass species richness\n\n\n\n\n\nMap of sampling basins\n\n\n\n# load libraries\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nlibrary(hypervolume)\n## Loading required package: Rcpp\n\n# load sav monitoring data \ndf = read_csv('data/FLbay_SAV.csv') \n## Rows: 2892 Columns: 9\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): BASIN\n## dbl (8): YEAR, STATION, TT, HW, SF, TMA, TDR, sg_rich\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(df)\n## # A tibble: 6 × 9\n##   BASIN  YEAR STATION     TT     HW     SF    TMA   TDR sg_rich\n##   &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n## 1 BLK    2007       1 0.0112 0.251  0      0      0       1.38 \n## 2 BLK    2007       2 0.06   0      0      0.03   0.131   1    \n## 3 BLK    2007       3 0.612  0.0112 0.0112 0.0112 0       1.62 \n## 4 BLK    2007       4 0.03   0      0      0.03   0.247   0.333\n## 5 BLK    2007       5 0      0.0075 0      0.015  0       0.25 \n## 6 BLK    2007       6 0.0188 0.075  0      0.178  0       1.12"
  },
  {
    "objectID": "ws3.html#prepare-data",
    "href": "ws3.html#prepare-data",
    "title": "Workshop 3: Ecosystem stability",
    "section": "Prepare data",
    "text": "Prepare data\nBecause hypervolumes can be generated with any continuous data as an axes, many of the times the units are not combatible. Blonder et al. 2014 & 2018 to convert all of the axes into the same units. This can be done by taking the z-score of the values to convert units into standard deviations. Z-scoring data can be done with the formula: \\[ z = \\frac{x_{i}-\\overline{x}}{sd} \\] Where \\(x_{i}\\) is a value, \\(\\overline{x}\\) is the mean, and \\(sd\\) is the standard deviation. By z-scoring each axis, 0 is the mean of that axis, a value of 1 means that the value is 1 standard deviation above the global mean of that axis, and a value of -1 is 1 standard deviation below the global mean of the axis. In R this can be done manually or with the scale() function.\nHypervolumes cannot be made when all values for a single axis are the same (e.g. all values 0 for a species cover in a basin for a year), so we can add a tiny bit of variation in order to make the hypervolume.\nWe then can nest() the data to take advantage of the purr package and map().\n\n# z-score and nest data to make hypervolume\nset.seed(14)\ndf = df |&gt; \n  # z score data across all sites and years\n  mutate(across(c(TT:sg_rich), scale), \n  # add tiny amount so when all values the same can make hv       \n         across(c(TT:sg_rich), \n                ~map_dbl(., ~. + rnorm(1, mean = 0, sd = 0.0001)))) |&gt; \n  # remove station from dataset\n  select(-STATION) |&gt; \n  # nest data by basin and year\n  group_by(BASIN, YEAR) |&gt; \n  nest() \nhead(df)\n## # A tibble: 6 × 3\n## # Groups:   BASIN, YEAR [6]\n##   BASIN  YEAR data             \n##   &lt;chr&gt; &lt;dbl&gt; &lt;list&gt;           \n## 1 BLK    2007 &lt;tibble [29 × 6]&gt;\n## 2 BLK    2008 &lt;tibble [28 × 6]&gt;\n## 3 BLK    2009 &lt;tibble [29 × 6]&gt;\n## 4 BLK    2010 &lt;tibble [29 × 6]&gt;\n## 5 BLK    2011 &lt;tibble [29 × 6]&gt;\n## 6 BLK    2012 &lt;tibble [29 × 6]&gt;"
  },
  {
    "objectID": "ws3.html#generate-hypervolumes",
    "href": "ws3.html#generate-hypervolumes",
    "title": "Workshop 3: Ecosystem stability",
    "section": "Generate hypervolumes",
    "text": "Generate hypervolumes\nHypervolumes are a multidimensional tool that is based on Hutchinson’s n-dimensional niche concept and we can build them with the hypervolume package.\n\nWith a nested dataset of our columns that we want to build hypervolumes for we can use mutate() and map() to generate the hypervolume.\nWe can also use map() and get_centroid() to extract centroid values of each hypervolume.\n\n# generate hypervolumes\ndf = df |&gt; \n  mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = paste(BASIN,YEAR,sep = '_'),\n                                                     samples.per.point = 1000,\n                                                     kde.bandwidth = estimate_bandwidth(data), \n                                                     sd.count = 3, \n                                                     quantile.requested = 0.95, \n                                                     quantile.requested.type = \"probability\", \n                                                     chunk.size = 1000, \n                                                     verbose = F)),\n         centroid = map(hv, \\(hv) get_centroid(hv)),\n         size = map_dbl(hv, \\(hv) get_volume(hv)))\n\n** Do not try to open dataframe with hv column it will hang because it is too big\n\nhead(df)\n## # A tibble: 6 × 6\n## # Groups:   BASIN, YEAR [6]\n##   BASIN  YEAR data              hv         centroid      size\n##   &lt;chr&gt; &lt;dbl&gt; &lt;list&gt;            &lt;list&gt;     &lt;list&gt;       &lt;dbl&gt;\n## 1 BLK    2007 &lt;tibble [29 × 6]&gt; &lt;Hypervlm&gt; &lt;dbl [6]&gt;  62.0   \n## 2 BLK    2008 &lt;tibble [28 × 6]&gt; &lt;Hypervlm&gt; &lt;dbl [6]&gt;   0.0164\n## 3 BLK    2009 &lt;tibble [29 × 6]&gt; &lt;Hypervlm&gt; &lt;dbl [6]&gt;   0.635 \n## 4 BLK    2010 &lt;tibble [29 × 6]&gt; &lt;Hypervlm&gt; &lt;dbl [6]&gt; 492.    \n## 5 BLK    2011 &lt;tibble [29 × 6]&gt; &lt;Hypervlm&gt; &lt;dbl [6]&gt;  14.2   \n## 6 BLK    2012 &lt;tibble [29 × 6]&gt; &lt;Hypervlm&gt; &lt;dbl [6]&gt;   0.0533\n\nIf wanting to save you can save output as .rds\n\nsaveRDS(df, 'data/SAV_hvs.rds')"
  },
  {
    "objectID": "ws3.html#plotting-hypervolumes",
    "href": "ws3.html#plotting-hypervolumes",
    "title": "Workshop 3: Ecosystem stability",
    "section": "plotting hypervolumes",
    "text": "plotting hypervolumes\nWe can plot multiple hypervolumes by joining them together\n\nhvj = hypervolume_join(df$hv[[1]], df$hv[[2]])\n\nplot(hvj, pairplot = T, colors=c('goldenrod','blue'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)"
  },
  {
    "objectID": "ws3.html#comparison-metrics",
    "href": "ws3.html#comparison-metrics",
    "title": "Workshop 3: Ecosystem stability",
    "section": "Comparison metrics",
    "text": "Comparison metrics\nWe can use the overlap to understand the similarity between years, centroid distance to compare mean conditions between years, and the log of the size ratio between years to understand the stability. This can be done by creating a data frame with all of the possible year combinations, and merging dataframes together to easily join. We can then use map() to calculate the metrics between.\n\n# comparison of across each year\ndf_y= tibble(y1 = unique(df$YEAR),\n             y2 = unique(df$YEAR)) |&gt; \n  expand(y1,y2)\n\n# make all unique year comparisons \ndf_y = df_y[!duplicated(t(apply(df_y,1,sort))),] %&gt;% \n  filter(!(y1 == y2))\n\n# make two df to join all unique comparisons  \ndf1 = df |&gt; \n  select(BASIN, y1 = YEAR, hv1 = hv, hv1_size = size, cent1 = centroid)\n\ndf2 = df |&gt; \n  select(BASIN, y2 = YEAR, hv2 = hv, hv2_size = size, cent2 = centroid)\n\n\n# create data frame of all data and make yearly comparisons\ndf_ov = tibble(BASIN = rep(unique(df$BASIN),\n                           each = nrow(df_y)),\n               y1 = rep(df_y$y1, times = length(unique(df$BASIN))),\n               y2 = rep(df_y$y2, times = length(unique(df$BASIN)))) |&gt; \n  inner_join(df1, by = c('BASIN', 'y1')) |&gt; \n  inner_join(df2, by = c('BASIN', 'y2')) |&gt; \n  mutate(ychange = y2-y1,\n  # calculate the differnces in size \n         lsr = log(hv2_size/hv1_size),\n  # join hypervolumees in a set for cverlap\n         set = map2(hv1,hv2, \\(hv1, hv2) hypervolume_set(hv1, hv2, check.memory = F, verbose = F)),\n  # calculate overlap\n         ov = map(set, \\(set) hypervolume_overlap_statistics(set)),\n  # calculate centroid distance \n         dist_cent = map2_dbl(hv1, hv2, \\(hv1,hv2) hypervolume_distance(hv1, hv2, type = 'centroid', check.memory=F))) |&gt; \n  #unnest centroid differences\n  unnest_wider(ov) |&gt; \n  # select only metrics of interest\n  select(BASIN, y1, y2, ychange,lsr,\n         dist_cent, jaccard, sorensen,\n         uniq_y1 = frac_unique_1, uniq_y2 = frac_unique_2)\n\n# save output\nwrite_csv(df_ov, 'data/SAV_centDist.csv')\n\n\n## Rows: 720 Columns: 10\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): BASIN\n## dbl (9): y1, y2, ychange, lsr, dist_cent, jaccard, sorensen, uniq_y1, uniq_y2\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\ndf_ov\n## # A tibble: 720 × 10\n##    BASIN    y1    y2 ychange     lsr dist_cent  jaccard sorensen uniq_y1 uniq_y2\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1 BLK    2007  2008       1 -8.24       0.736 0.000109 0.000219   1.00    0.586\n##  2 BLK    2007  2009       2 -4.58       1.38  0.00239  0.00477    0.998   0.765\n##  3 BLK    2007  2010       3  2.07       3.00  0.0492   0.0937     0.581   0.947\n##  4 BLK    2007  2011       4 -1.48       0.986 0.0799   0.148      0.909   0.602\n##  5 BLK    2007  2012       5 -7.06       0.971 0.000401 0.000802   1.00    0.533\n##  6 BLK    2007  2013       6  0.0145     0.795 0.167    0.286      0.712   0.716\n##  7 BLK    2007  2014       7 -0.715      0.623 0.180    0.306      0.772   0.535\n##  8 BLK    2007  2015       8  0.165      0.861 0.180    0.305      0.668   0.719\n##  9 BLK    2007  2016       9 -1.40       1.37  0.0701   0.131      0.918   0.669\n## 10 BLK    2007  2017      10 -0.480      0.702 0.0928   0.170      0.862   0.778\n## # ℹ 710 more rows\n\nPlot Overlap, centroid distance, and log size ratio of all comparisons.\n\ndf_ov = read_csv('data/SAV_centDist.csv') |&gt; \nmutate(BASIN = factor(BASIN, levels = \n                          c('JON', 'RKB', 'TWN', 'RAN', 'WHP','BLK')))\n## Rows: 720 Columns: 10\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): BASIN\n## dbl (9): y1, y2, ychange, lsr, dist_cent, jaccard, sorensen, uniq_y1, uniq_y2\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#overlap\nggplot(df_ov, aes(BASIN, sorensen, fill = BASIN))+\n  geom_point(aes(color = BASIN), size = 1, \n             position=position_jitterdodge(dodge.width = 0.75, jitter.width = 1))+\n  # geom_errorbar(aes(ymin = lc, ymax = uc), linewidth = 2, width = 0)+\n  geom_boxplot(alpha = 0.6, outliers = F)+\n  labs(x = 'Basin', y = 'Overlap')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n# centroid distance\nggplot(df_ov, aes(BASIN, dist_cent, fill = BASIN))+\n  geom_hline(aes(yintercept = 1), linetype = 'dashed', linewidth = 1)+\n  geom_point(aes(color = BASIN), size = 1, \n             position=position_jitterdodge(dodge.width = 0.75, jitter.width = 1))+\n  # geom_errorbar(aes(ymin = lc, ymax = uc), linewidth = 2, width = 0)+\n  geom_boxplot(alpha = 0.6, outliers = F)+\n  labs(x = 'Basin', y = 'Centroid distance')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\n# log size ratio\nggplot(df_ov, aes(BASIN, lsr, fill = BASIN))+\n  geom_hline(aes(yintercept = 0), linetype = 'dashed', linewidth = 1)+\n  geom_point(aes(color = BASIN), size = 1, \n             position=position_jitterdodge(dodge.width = 0.75, jitter.width = 1))+\n  # geom_errorbar(aes(ymin = lc, ymax = uc), linewidth = 2, width = 0)+\n  geom_boxplot(alpha = 0.6, outliers = F)+\n  labs(x = 'Basin', y = 'log(y2 size/y1 size)')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))"
  },
  {
    "objectID": "ws3.html#year-to-year-comparisons",
    "href": "ws3.html#year-to-year-comparisons",
    "title": "Workshop 3: Ecosystem stability",
    "section": "Year to Year comparisons",
    "text": "Year to Year comparisons\nA common way to look at stability is to plot change over time by plotting the year to year comparisons of each metric.\n\n# filter only single year comparisons\nd = df_ov |&gt; \n  filter(ychange == 1)\n\n# overlap\nggplot(d, aes(y2, sorensen, color = BASIN))+\n  geom_point(size = 2)+\n  geom_line(linewidth = 1)+\n  labs(x = 'Year', y = 'Overlap')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  facet_wrap(~BASIN, nrow = 2)+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n# centroid distance \nggplot(d, aes(y2, dist_cent, color = BASIN))+\n  geom_hline(aes(yintercept = 1), linetype = 'dashed', linewidth = 1)+\n  geom_point(size = 2)+\n  geom_line(linewidth = 1)+\n  labs(x = 'Year', y = 'Centroid distance')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  facet_wrap(~BASIN, nrow = 2)+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n# log size ratio\nggplot(d, aes(y2, lsr, color = BASIN))+\n  geom_hline(aes(yintercept = 0), linetype = 'dashed', linewidth = 1)+\n  geom_point(size = 2)+\n  geom_line(linewidth = 1)+\n  labs(x = 'Year', y = 'log(y2 size/y1 size)')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  facet_wrap(~BASIN, nrow = 2)+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))"
  },
  {
    "objectID": "ws3.html#compare-to-baseline",
    "href": "ws3.html#compare-to-baseline",
    "title": "Workshop 3: Ecosystem stability",
    "section": "Compare to baseline",
    "text": "Compare to baseline\nAnother way to look at stability is to plot change relative to a baseline. Here we plot them relative to the first year of the dataset (2007).\n\n# filter only single year comparisons\nd = df_ov |&gt; \n  filter(y1 == 2007)\n\n# overlap\nggplot(d, aes(y2, sorensen, color = BASIN))+\n  geom_point(size = 2)+\n  geom_line(linewidth = 1)+\n  labs(x = 'Year', y = 'Overlap')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  facet_wrap(~BASIN, nrow = 2)+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n# centroid distance \nggplot(d, aes(y2, dist_cent, color = BASIN))+\n  geom_hline(aes(yintercept = 1), linetype = 'dashed', linewidth = 1)+\n  geom_point(size = 2)+\n  geom_line(linewidth = 1)+\n  labs(x = 'Year', y = 'Centroid distance')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  facet_wrap(~BASIN, nrow = 2)+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\n# log size ratio\nggplot(d, aes(y2, lsr, color = BASIN))+\n  geom_hline(aes(yintercept = 0), linetype = 'dashed', linewidth = 1)+\n  geom_point(size = 2)+\n  geom_line(linewidth = 1)+\n  labs(x = 'Year', y = 'log(y2 size/y1 size)')+\n  scale_fill_viridis_d(option = 'turbo')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  facet_wrap(~BASIN, nrow = 2)+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))"
  },
  {
    "objectID": "ws3.html#trend-in-stability",
    "href": "ws3.html#trend-in-stability",
    "title": "Workshop 3: Ecosystem stability",
    "section": "Trend in stability",
    "text": "Trend in stability\nWe can look across the number of years to understand the trend in stability. By fitting three possible models we can determine the trend of time For each metric. When intercept model is the best, we can determine that the trend is static and not changing with the number of years between comparison. If linear, the centroid distance can indicate a shift in state overtime, and a quadratic with a maximum at middle values can indicate a disturbance with recovery in state.\n\nlibrary(MuMIn)\n# overlap \ndf_o = df_ov |&gt; \n  group_by(BASIN) |&gt;\n  nest() |&gt; \n  # fit intercept, linear, and quadratic model\n  mutate(m_int = map(data, \\(df)lm(sorensen~1, data = df)),\n         m_lin = map(data, \\(df)lm(sorensen~ychange, data = df)),\n         m_quad = map(data, \\(df)lm(sorensen~ychange + I(ychange^2), data = df)),\n         AICc_int = map_dbl(m_int, \\(x) AICc(x)),\n         AICc_lin = map_dbl(m_lin, \\(x) AICc(x)),\n         AICc_quad = map_dbl(m_quad, \\(x) AICc(x)),\n         model = case_when(\n           AICc_int - min(c(AICc_int,AICc_lin,AICc_quad)) &lt;= 4 ~ 'Intercept',\n           AICc_lin - AICc_quad &lt;= 4 ~ 'Linear',\n           T ~ 'Quadratic'))\n\n# unnest data \nd = df_o |&gt; \n  select(BASIN, data, model) |&gt; \n  unnest(cols = c(data)) |&gt;  \n  mutate(BASIN = factor(BASIN, levels = \n                          c('JON', 'RKB', 'TWN', 'RAN', 'WHP','BLK')))\n\nggplot(d, aes(ychange, sorensen, color = BASIN))+\n  geom_point(size = 2.5)+\n  geom_smooth(data = d |&gt; filter(model == 'Intercept'),\n              method = 'lm', formula = y~1, \n              linewidth = 1, color = 'black')+\n  geom_smooth(data = d |&gt; filter(model == 'Linear'),\n              method = 'lm', formula = y~x, \n              linewidth = 1, color = 'black')+\n  geom_smooth(data = d |&gt; filter(model == 'Quadratic'),\n              method = 'lm', formula = y~x+I(x^2), \n              linewidth = 1, color = 'black')+\n  facet_wrap(~BASIN,  nrow = 2)+\n  labs(x = 'Years between comparison', y = 'Overlap')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"),\n        axis.text.x = element_text(size = 12, colour = \"black\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n# centroid distance\ndf_cd = df_ov |&gt; \n  group_by(BASIN) |&gt;\n  nest() |&gt; \n  # fit intercept, linear, and quadratic model\n  mutate(m_int = map(data, \\(df)lm(dist_cent~1, data = df)),\n         m_lin = map(data, \\(df)lm(dist_cent~ychange, data = df)),\n         m_quad = map(data, \\(df)lm(dist_cent~ychange + I(ychange^2), data = df)),\n         AICc_int = map_dbl(m_int, \\(x) AICc(x)),\n         AICc_lin = map_dbl(m_lin, \\(x) AICc(x)),\n         AICc_quad = map_dbl(m_quad, \\(x) AICc(x)),\n         model = case_when(\n           AICc_int - min(c(AICc_int,AICc_lin,AICc_quad)) &lt;= 4 ~ 'Intercept',\n           AICc_lin - AICc_quad &lt;= 4 ~ 'Linear',\n           T ~ 'Quadratic'))\n\n# unnest data \nd = df_cd |&gt; \n  select(BASIN, data, model) |&gt; \n  unnest(cols = c(data)) |&gt;  \n  mutate(BASIN = factor(BASIN, levels = \n                          c('JON', 'RKB', 'TWN', 'RAN', 'WHP','BLK')))\n\nggplot(d, aes(ychange, dist_cent, color = BASIN))+\n  geom_hline(aes(yintercept = 1), linetype = 'dashed')+\n  geom_point(size = 2.5)+\n  geom_smooth(data = d |&gt; filter(model == 'Intercept'),\n              method = 'lm', formula = y~1, \n              linewidth = 1, color = 'black')+\n  geom_smooth(data = d |&gt; filter(model == 'Linear'),\n              method = 'lm', formula = y~x, \n              linewidth = 1, color = 'black')+\n  geom_smooth(data = d |&gt; filter(model == 'Quadratic'),\n              method = 'lm', formula = y~x+I(x^2), \n              linewidth = 1, color = 'black')+\n  facet_wrap(~BASIN,  nrow = 2)+\n  labs(x = 'Years between comparison', y = 'Centroid distance')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"),\n        axis.text.x = element_text(size = 12, colour = \"black\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n# log size ratio\ndf_lsr = df_ov |&gt; \n  group_by(BASIN) |&gt;\n  nest() |&gt; \n  # fit intercept, linear, and quadratic model\n  mutate(m_int = map(data, \\(df)lm(lsr~1, data = df)),\n         m_lin = map(data, \\(df)lm(lsr~ychange, data = df)),\n         m_quad = map(data, \\(df)lm(lsr~ychange + I(ychange^2), data = df)),\n         AICc_int = map_dbl(m_int, \\(x) AICc(x)),\n         AICc_lin = map_dbl(m_lin, \\(x) AICc(x)),\n         AICc_quad = map_dbl(m_quad, \\(x) AICc(x)),\n         model = case_when(\n           AICc_int - min(c(AICc_int,AICc_lin,AICc_quad)) &lt;= 4 ~ 'Intercept',\n           AICc_lin - AICc_quad &lt;= 4 ~ 'Linear',\n           T ~ 'Quadratic'))\n\n# unnest data \nd = df_lsr |&gt; \n  select(BASIN, data, model) |&gt; \n  unnest(cols = c(data)) |&gt;  \n  mutate(BASIN = factor(BASIN, levels = \n                          c('JON', 'RKB', 'TWN', 'RAN', 'WHP','BLK')))\n\nggplot(d, aes(ychange, lsr, color = BASIN))+\n  geom_hline(aes(yintercept = 0), linetype = 'dashed')+\n  geom_point(size = 2.5)+\n  geom_smooth(data = d |&gt; filter(model == 'Intercept'),\n              method = 'lm', formula = y~1, \n              linewidth = 1, color = 'black')+\n  geom_smooth(data = d |&gt; filter(model == 'Linear'),\n              method = 'lm', formula = y~x, \n              linewidth = 1, color = 'black')+\n  geom_smooth(data = d |&gt; filter(model == 'Quadratic'),\n              method = 'lm', formula = y~x+I(x^2), \n              linewidth = 1, color = 'black')+\n  facet_wrap(~BASIN,  nrow = 2)+\n  labs(x = 'Years between comparison', y = 'log(y2 size/y1 size)')+\n  scale_color_viridis_d(option = 'turbo')+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"),\n        axis.text.x = element_text(size = 12, colour = \"black\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))"
  },
  {
    "objectID": "ws3.html#variable-importance",
    "href": "ws3.html#variable-importance",
    "title": "Workshop 3: Ecosystem stability",
    "section": "Variable importance",
    "text": "Variable importance\nBecause hypervolumes are multivariate, each axis has the potential to influence the overall change. We can determine the influence of each axis on the overall change by removing an axis and recalculating each metric. We can then compare the hypervolume without that axis to the metrics with all axes. We can determine the importance of a variable using the following formula: \\[ imp_x = 1 - r_i\\] where \\(imp_x\\) is the importance of axis \\(x\\), and \\(r_i\\) is pearson correlation coefficient between the metric \\(i\\) (overlap, centroid distance, or log size ratio) of the hypervolume with all axes to the hypervolume calculate without axis \\(x\\). High importance values indicate that removing the axis had big changes on the values of that metric.\n\n# get data used for hvs\ndf = readRDS('data/SAV_hvs.rds') |&gt; \n  select(BASIN, YEAR, data) |&gt; \n  unnest(data)\n\n#axes \nax = c(\"TT\", \"HW\", \"SF\", \"TMA\", \"TDR\", \"sg_rich\")\n\nfor (i in 1:length(ax)){\n  d = df |&gt; \n    select(-ax[i])|&gt; \n    group_by(BASIN, YEAR) |&gt;\n    nest() |&gt; \n    mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = paste(BASIN,YEAR,sep = '_'),\n                                                       samples.per.point = 1000,\n                                                       kde.bandwidth = estimate_bandwidth(data), \n                                                       sd.count = 3, \n                                                       quantile.requested = 0.95, \n                                                       quantile.requested.type = \"probability\", \n                                                       chunk.size = 1000, \n                                                       verbose = F)),\n           hv_size = map_dbl(hv, \\(hv) get_volume(hv)),\n           axis = ax[i])\n  \n  if (i == 1){\n    df_tot = d\n  }else{\n    df_tot = bind_rows(df_tot, d)\n  }\n}\n\n# make all comparisons for each axis\n# comparison of across each year\ndf_y= tibble(y1 = unique(df$YEAR),\n             y2 = unique(df$YEAR)) |&gt; \n  expand(y1,y2)\n\n# make all unique year comparisons \ndf_y = df_y[!duplicated(t(apply(df_y,1,sort))),] %&gt;% \n  filter(!(y1 == y2))\n\n# make two df to join all unique comparisons  \ndf1 = df_tot |&gt; \n  select(BASIN, axis, y1 = YEAR, hv1 = hv, hv1_size = hv_size)\n\ndf2 = df_tot |&gt; \n  select(BASIN, axis, y2 = YEAR, hv2 = hv, hv2_size = hv_size)\n\n\n# create data frame of all data and make yearly comparisons\ndf_ax = tibble(BASIN = rep(unique(df$BASIN),\n                           each = nrow(df_y)),\n               y1 = rep(df_y$y1, times = length(unique(df$BASIN))),\n               y2 = rep(df_y$y2, times = length(unique(df$BASIN)))) |&gt;\n  slice(rep(1:n(), each=length(ax)))|&gt; \n  mutate(axis = rep(ax, times=nrow(df_y)*length(unique(df$BASIN)))) |&gt; \n  inner_join(df1, by = c('BASIN', 'y1', 'axis')) |&gt; \n  inner_join(df2, by = c('BASIN', 'y2', 'axis')) |&gt; \n  mutate(ychange = y2-y1,\n  # calculate the differences in size \n         lsr_ax = log(hv2_size/hv1_size),\n  # join hypervolumes in a set for overlap\n         set = map2(hv1,hv2, \\(hv1, hv2) hypervolume_set(hv1, hv2, check.memory = F, verbose = F)),\n  # calculate overlap\n         ov = map(set, \\(set) hypervolume_overlap_statistics(set)),\n  # calculate centroid distance \n         dist_cent_ax = map2_dbl(hv1, hv2, \\(hv1,hv2) hypervolume_distance(hv1, hv2, type = 'centroid', check.memory=F))) |&gt; \n  #unnest overlap\n  unnest_wider(ov) |&gt; \n  # select only metrics of interest\n  select(BASIN, y1, y2, ychange, axis,\n         lsr_ax, dist_cent_ax, sorensen_ax = sorensen)\n\n# save output\nwrite_csv(df_ax, 'data/SAV_metrics_ax.csv')\n\nCalculate correlation and plot\n\n# load and combine with whole data\ndf_ov = read_csv('data/SAV_centDist.csv') |&gt; \n  select(BASIN, y1, y2, lsr, dist_cent, sorensen)\n## Rows: 720 Columns: 10\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): BASIN\n## dbl (9): y1, y2, ychange, lsr, dist_cent, jaccard, sorensen, uniq_y1, uniq_y2\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_all = read_csv('data/SAV_metrics_ax.csv')|&gt; \n  left_join(df_ov) |&gt; \n  group_by(BASIN, axis) |&gt; \n  nest() |&gt; \n  mutate(cor_ov = map(data, \\(df) cor.test(df$sorensen, df$sorensen_ax)),\n         imp_ov = map_dbl(cor_ov, \\(x) 1 - x$estimate),\n         cor_cd = map(data, \\(df) cor.test(df$dist_cent, df$dist_cent_ax)),\n         imp_cd = map_dbl(cor_cd, \\(x) 1 - x$estimate),\n         cor_lsr = map(data, \\(df) cor.test(df$lsr, df$lsr_ax)),\n         imp_lsr = map_dbl(cor_lsr, \\(x) 1 - x$estimate),\n         BASIN = factor(BASIN, levels = \n                          c('JON', 'RKB', 'TWN', 'RAN', 'WHP','BLK')),\n         axis = factor(axis, levels = c('TDR', 'TMA', 'sg_rich',\n                                        'SF', 'HW', 'TT')))\n## Rows: 4320 Columns: 8\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (2): BASIN, axis\n## dbl (6): y1, y2, ychange, lsr_ax, dist_cent_ax, sorensen_ax\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n## Joining with `by = join_by(BASIN, y1, y2)`\n\n# overlap\nggplot(df_all, aes(axis, imp_ov, fill = BASIN))+\n  geom_col()+\n  labs(x = 'Variable', y = 'Overlap variable importance')+\n  coord_flip()+\n  theme_bw()+\n  facet_wrap(~BASIN,  nrow = 2)+\n  scale_fill_viridis_d(option = 'turbo')+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n# centroid distance\nggplot(df_all, aes(axis, imp_cd, fill = BASIN))+\n  geom_col()+\n  labs(x = 'Variable', y = 'Centroid distance variable importance')+\n  coord_flip()+\n  theme_bw()+\n  facet_wrap(~BASIN,  nrow = 2)+\n  scale_fill_viridis_d(option = 'turbo')+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n# log size ratio\nggplot(df_all, aes(axis, imp_lsr, fill = BASIN))+\n  geom_col()+\n  labs(x = 'Variable', y = 'log(y2 size/y1 size) variable importance')+\n  coord_flip()+\n  theme_bw()+\n  facet_wrap(~BASIN,  nrow = 2)+\n  scale_fill_viridis_d(option = 'turbo')+\n  theme(axis.title = element_text(size = 14), \n        axis.text = element_text(size = 14, colour = \"gray0\"), \n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))"
  }
]