[
  {
    "objectID": "ws2.html",
    "href": "ws2.html",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "",
    "text": "This workshop uses hypervolumes to quantify the trophic niche of seagrass consumers. Hypervolumes are generated using mean and standard deviation resource use data from stable isotope mixing models. This process is repeated 50 times. Hypervolume overlap metrics, as well as size and centroid distance, are used to understand how consumers niches change between seasons.\nR script: github\nR script of workshop 2"
  },
  {
    "objectID": "ws2.html#seasonal-comparison-of-trophic-niche-of-seagrass-consumers",
    "href": "ws2.html#seasonal-comparison-of-trophic-niche-of-seagrass-consumers",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "",
    "text": "This workshop uses hypervolumes to quantify the trophic niche of seagrass consumers. Hypervolumes are generated using mean and standard deviation resource use data from stable isotope mixing models. This process is repeated 50 times. Hypervolume overlap metrics, as well as size and centroid distance, are used to understand how consumers niches change between seasons.\nR script: github\nR script of workshop 2"
  },
  {
    "objectID": "ws2.html#data",
    "href": "ws2.html#data",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "data",
    "text": "data\nThe data used for this vignette comes from James et al. 2022. The resource use data consists of mean and standard deviation of four resources for each species in each season based on mixing model outputs. Data comes from species collected across Florida Bay, USA.\n\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nlibrary(hypervolume)\n## Loading required package: Rcpp\nlibrary(truncnorm)\n\n# load all data\nd = read_csv('data/CESImixResults.csv')\n## Rows: 56 Columns: 7\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): species, season, source\n## dbl (4): mean, sd, lowend, highend\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nd\n## # A tibble: 56 × 7\n##    species             season source     mean    sd lowend highend\n##    &lt;chr&gt;               &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1 Bay anchovy         Wet    Algae     0.02  0.016  0       0.061\n##  2 Mojarra             Wet    Algae     0.004 0.004  0       0.013\n##  3 Pigfish             Wet    Algae     0.008 0.008  0       0.028\n##  4 Pinfish             Wet    Algae     0.028 0.018  0       0.062\n##  5 Pink shrimp         Wet    Algae     0.009 0.008  0       0.031\n##  6 Rainwater killifish Wet    Algae     0.016 0.017  0       0.063\n##  7 Silver perch        Wet    Algae     0.02  0.024  0       0.091\n##  8 Bay anchovy         Wet    Epiphytes 0.3   0.067  0.153   0.418\n##  9 Mojarra             Wet    Epiphytes 0.04  0.02   0.009   0.084\n## 10 Pigfish             Wet    Epiphytes 0.076 0.034  0.019   0.149\n## # ℹ 46 more rows"
  },
  {
    "objectID": "ws2.html#prepare-data",
    "href": "ws2.html#prepare-data",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "Prepare data",
    "text": "Prepare data\nHere a custom function HVvalues() is used to generate random points from mean and standard deviation data between end points (lower and upper bounds) using the truncnorm package. Values can then be z-scored.\n*** Note chose either column names or column numbers for ID_rows and names either work but must be the same - df = dataframe or tibble with each row containing - unique entry for making random points - ID_rows = vector of column names or numbers with id information - names = column name or number of name of measure variables - mean = column name or column number of df with mean - sd = column name or column number of df with sd - n = number of points to randomly generate - z_score = T or F, if T z-scores values - end_points = T or F for if random points need to be generated between - a high and low end point (e.g. 5% and 95% interval) - low and high required if end_points = T - low = column name or column number of df with lower bound to sample in - high = column name or column number of df with upper bound to sample in\n\nHVvalues = function(df, ID_rows, names, mean, sd, n, z_score = F,\n                    end_points = F, low = NULL, high = NULL){\n  require(tidyverse)\n  require(truncnorm)\n  \n  # check to see if information is needed to restrict where points are\n  if (end_points){\n    if (is_empty(df[,low]) | is_empty(df[,high])){\n      return(cat('Warning: low and/or high columns not specified \\n\n                  Specific and run again or end_points = F \\n'))\n    }\n  }\n  \n  # check to see if there are more \n  if(T %in% duplicated(df[,c(ID_rows,names)])){\n    return(cat('Warning: some of the rows contain duplicated information \\n\n                make sure data is correct \\n'))\n  }\n  \n  # rename variables to make code work\n  if (is.numeric(mean)){\n    names(df)[mean] = 'Mean'\n  }else {\n    df = df |&gt;  rename(Mean = mean)\n  }\n  \n  if (is.numeric(sd)){\n    names(df)[sd] = 'SD'\n  }else {\n    df = df |&gt;  rename(SD = sd)\n  }\n  \n  if (end_points){\n    if (is.numeric(low)){\n      names(df)[low] = 'lower'\n    }else {\n      df = df |&gt;  rename(lower = low)\n    }\n    \n    if (is.numeric(high)){\n      names(df)[high] = 'upper'\n    }else {\n      df = df |&gt;  rename(upper = high)\n    }\n  }\n  \n  # make sure the names is not numeric \n  if (is.numeric(names)){\n    names = names(df)[names]\n  }\n  \n  labs = unique(as.vector(df[,names])[[1]])\n  # generate random points within bounds\n  if(end_points){\n    \n    df_tot = df |&gt; slice(rep(1:n(), each=n))|&gt; \n      mutate(point = \n               truncnorm::rtruncnorm(1, a = lower, b = upper,\n                                     mean = Mean, sd = SD)) |&gt; \n      ungroup() |&gt; \n      mutate(num = rep(1:n, times=nrow(df))) |&gt;\n      dplyr::select(-Mean, -SD, -lower, -upper)|&gt;\n      pivot_wider(names_from = names, values_from = point)|&gt; \n      dplyr::select(-num)\n  }else {\n    # generate random points outside of bounds\n    df_tot = df |&gt; slice(rep(1:n(), each=n))|&gt;\n      mutate(point = \n               truncnorm::rtruncnorm(1, mean = Mean, sd = SD)) |&gt; \n      ungroup() |&gt; \n      mutate(num = rep(1:n, times=nrow(df))) |&gt;\n      dplyr::select(-Mean, -SD)|&gt;\n      pivot_wider(names_from = names, values_from = point)|&gt; \n      dplyr::select(-num)\n  }\n  if (z_score){\n    df_tot = df_tot  |&gt;  \n      mutate(across(all_of(labs), scale))\n  }\n  \n  return(df_tot)\n  \n}\n\n30 points are generated for each species in each season using map(), and this process is repeated 50 times.\n\n# number or iterations\nreps = 50\n\n# generate points and z-score across iterations\nset.seed(14)\ndf = d |&gt; \n  # duplicate for number of reps\n  slice(rep(1:n(), each=reps))|&gt; \n  mutate(i = rep(1:reps, times=nrow(d))) |&gt; \n  group_by(i) |&gt; \n  nest() |&gt; \n  # apply function to generate random points\n  mutate(points = map(data, \\(data) HVvalues(df = data, ID_rows = c('species', 'season'),\n                                             names = c('source'), \n                                             mean = 'mean', sd = 'sd', n = 30,\n                                             end_points = T, low = 'lowend', high = 'highend',\n                                             z_score = T))) |&gt; \n  select(i, points) |&gt; \n  unnest(points)\n## Warning: There were 3 warnings in `mutate()`.\n## The first warning was:\n## ℹ In argument: `points = map(...)`.\n## ℹ In group 1: `i = 1`.\n## Caused by warning:\n## ! Using an external vector in selections was deprecated in tidyselect 1.1.0.\n## ℹ Please use `all_of()` or `any_of()` instead.\n##   # Was:\n##   data %&gt;% select(low)\n## \n##   # Now:\n##   data %&gt;% select(all_of(low))\n## \n## See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n## ℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\ndf\n## # A tibble: 21,000 × 7\n## # Groups:   i [50]\n##        i species     season Algae[,1] Epiphytes[,1] Mangrove[,1] Seagrass[,1]\n##    &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n##  1     1 Bay anchovy Wet    -0.748           0.260          1.99      -0.330 \n##  2     1 Bay anchovy Wet     0.282           0.332          2.15      -0.466 \n##  3     1 Bay anchovy Wet     0.456          -0.0134         2.10       0.0613\n##  4     1 Bay anchovy Wet     0.186          -0.648          2.39      -0.260 \n##  5     1 Bay anchovy Wet    -0.478          -0.265          1.82      -0.232 \n##  6     1 Bay anchovy Wet     0.0711         -0.369          1.76       0.0775\n##  7     1 Bay anchovy Wet    -0.490          -0.675          1.66      -0.481 \n##  8     1 Bay anchovy Wet     0.000560       -0.465          2.24      -0.322 \n##  9     1 Bay anchovy Wet    -0.625          -0.158          2.34      -0.619 \n## 10     1 Bay anchovy Wet    -0.0106         -0.425          2.07      -0.670 \n## # ℹ 20,990 more rows"
  },
  {
    "objectID": "ws2.html#hypervolumes",
    "href": "ws2.html#hypervolumes",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "Hypervolumes",
    "text": "Hypervolumes\nHypervolumes are generated for each species and season and the size of each hypervolume is calculated.\n\ndf = df |&gt; \n  group_by(species, season, i) |&gt; \n  nest() |&gt; \n  mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = paste(species, season, i,sep = '_'),\n                                                     samples.per.point = 500,\n                                                     kde.bandwidth = estimate_bandwidth(data), \n                                                     sd.count = 3, \n                                                     quantile.requested = 0.95, \n                                                     quantile.requested.type = \"probability\", \n                                                     chunk.size = 1000, \n                                                     verbose = F)),\n         hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n\n** Do not try to view df in rstudio it will freeze your r since it is too big\n\nhead(df)\n## # A tibble: 6 × 7\n## # Groups:   species, season, i [6]\n##       i species             season data              hv         hv_size centroid\n##   &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;       &lt;dbl&gt; &lt;list&gt;  \n## 1     1 Bay anchovy         Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  3.27   &lt;dbl&gt;   \n## 2     1 Mojarra             Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  0.0909 &lt;dbl&gt;   \n## 3     1 Pigfish             Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  1.22   &lt;dbl&gt;   \n## 4     1 Pinfish             Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  3.09   &lt;dbl&gt;   \n## 5     1 Pink shrimp         Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  0.962  &lt;dbl&gt;   \n## 6     1 Rainwater killifish Wet    &lt;tibble [30 × 4]&gt; &lt;Hypervlm&gt;  6.32   &lt;dbl&gt;"
  },
  {
    "objectID": "ws2.html#hypervolume-metrics",
    "href": "ws2.html#hypervolume-metrics",
    "title": "Workshop 2: Trophic niche dynamics",
    "section": "Hypervolume metrics",
    "text": "Hypervolume metrics\nCombine each species and season to calculate the overlap and centroid distance of each species\n\nov_sn = df |&gt; \n  select(species, season, hv, hv_size) |&gt; \n  pivot_wider(names_from = season, values_from = c(hv,hv_size)) |&gt; \n  mutate(size_rat = hv_size_Dry/hv_size_Wet,\n         set = map2(hv_Wet,hv_Dry, \\(hv1, hv2) hypervolume_set(hv1, hv2, check.memory = F, verbose = F)),\n         ov = map(set, \\(set) hypervolume_overlap_statistics(set)),\n         dist_cent = map2_dbl(hv_Wet,hv_Dry, \\(hv1,hv2) hypervolume_distance(hv1, hv2, type = 'centroid', check.memory=F))) |&gt; \n  unnest_wider(ov) |&gt; \n  select(species, i, hv_size_Wet, hv_size_Dry, \n         size_rat, jaccard, sorensen,\n         uniq_Wet = frac_unique_1, uniq_Dry = frac_unique_2, \n         dist_cent)\n\n\n## Rows: 350 Columns: 10\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): species\n## dbl (9): i, hv_size_Wet, hv_size_Dry, size_rat, jaccard, sorensen, uniq_Wet,...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOverlap\n\ncols = c(\"Pinfish\" = 'yellow2',\n         \"Mojarra\" = 'slategray4',\n         \"Silver perch\" = 'snow3',\n         \"Bay anchovy\" = 'deepskyblue1',\n         \"Pigfish\" = 'orange', \n         \"Pink shrimp\" = 'pink',\n         \"Rainwater killifish\" = 'firebrick',\n         'All' = 'black')\n\ndf = ov_sn|&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(sorensen),\n            median = median(sorensen),\n            low = quantile(sorensen, 0.025),\n            up = quantile(sorensen, 0.975))\n\nggplot(df, aes(x = species, y = mean, color = species))+\n  geom_pointrange(aes(ymin = low, ymax = up),\n                  size = 1.5, linewidth = 1.5, fatten = 2, \n                  position=position_dodge(width = 0.5))+\n  labs(x = NULL, y = 'Niche overlap') +\n  scale_x_discrete(labels = c(\"Bay \\nanchovy\",\n                              \"Mojarra\",\n                              \"Pigfish\",\n                              \"Pinfish\",\n                              \"Pink \\nshrimp\",\n                              \"Rainwater \\nkillifish\",\n                              \"Silver \\nperch\" ))+\n  scale_color_manual(values = cols)+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"), \n        axis.text.x = element_text(size = 12, colour = \"black\"),\n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\n\nPercent unique\n\ndf = ov_sn|&gt;  \n  pivot_longer(uniq_Wet:uniq_Dry,names_to = 'season',\n               values_to = 'vol') |&gt; \n  group_by(species, season) |&gt; \n  summarize(mean = mean(vol),\n            median = median(vol),\n            low = quantile(vol, 0.025),\n            up = quantile(vol, 0.975)) |&gt; \n  mutate(season = factor(season, levels = c('uniq_Wet', \n                                            'uniq_Dry')))\n## `summarise()` has grouped output by 'species'. You can override using the\n## `.groups` argument.\n\nggplot(df, aes(x = species, y = mean, color = season))+\n  geom_pointrange(aes(ymin = low, ymax = up),\n                  size = 1.5, linewidth = 1.5, fatten = 2, \n                  position=position_dodge(width = 0.5))+\n  labs(x = 'Species', y = 'Niche volume unique',\n       color = 'Season') +\n  scale_fill_manual(values = c('uniq_Wet' = 'skyblue3', \n                               'uniq_Dry' = 'indianred3'),\n                    labels = c('uniq_Wet' = 'Wet', \n                               'uniq_Dry' = 'Dry')) +\n  scale_color_manual(values = c('uniq_Wet' = 'skyblue3', \n                                'uniq_Dry' = 'indianred3'),\n                     labels = c('uniq_Wet' = 'Wet',\n                                'uniq_Dry' = 'Dry')) +\n  scale_x_discrete(labels = c(\"Bay \\nanchovy\",\n                              \"Mojarra\",\n                              \"Pigfish\",\n                              \"Pinfish\",\n                              \"Pink \\nshrimp\",\n                              \"Rainwater \\nkillifish\",\n                              \"Silver \\nperch\" ))+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"), \n        axis.text.x = element_text(size = 12, colour = \"black\"),\n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'right',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n## Warning: No shared levels found between `names(values)` of the manual scale and the\n## data's fill values.\n\n\n\n\n\n\n\n\n\n\nHypervolume size\n\ndf = ov_sn |&gt; \n  pivot_longer(hv_size_Wet:hv_size_Dry,names_to = 'season',\n               values_to = 'vol') |&gt; \n  group_by(species, season) |&gt; \n  summarize(mean = mean(vol),\n            median = median(vol),\n            low = quantile(vol, 0.025),\n            up = quantile(vol, 0.975)) |&gt; \n  mutate(season = factor(season, levels = c('hv_size_Wet', \n                                            'hv_size_Dry')))\n## `summarise()` has grouped output by 'species'. You can override using the\n## `.groups` argument.\n\nggplot(df, aes(x = species, y = mean, color = season))+\n  geom_pointrange(aes(ymin = low, ymax = up),\n                  size = 1.5, linewidth = 1.5, fatten = 2, \n                  position=position_dodge(width = 0.5))+\n  labs(x = 'Species', y = 'Trophic niche width',\n       color = 'Season') +\n  scale_fill_manual(values = c('hv_size_Wet' = 'skyblue3', \n                               'hv_size_Dry' = 'indianred3'),\n                    labels = c('hv_size_Wet' = 'Wet', \n                               'hv_size_Dry' = 'Dry')) +\n  scale_color_manual(values = c('hv_size_Wet' = 'skyblue3', \n                                'hv_size_Dry' = 'indianred3'),\n                     labels = c('hv_size_Wet' = 'Wet',\n                                'hv_size_Dry' = 'Dry')) +\n  scale_x_discrete(labels = c(\"Bay \\nanchovy\",\n                              \"Mojarra\",\n                              \"Pigfish\",\n                              \"Pinfish\",\n                              \"Pink \\nshrimp\",\n                              \"Rainwater \\nkillifish\",\n                              \"Silver \\nperch\" ))+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"), \n        axis.text.x = element_text(size = 12, colour = \"black\"),\n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'right',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))\n## Warning: No shared levels found between `names(values)` of the manual scale and the\n## data's fill values.\n\n\n\n\n\n\n\n\n\n\nCentroid distance\n\ndf = ov_sn|&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(dist_cent),\n            median = median(dist_cent),\n            low = quantile(dist_cent, 0.025),\n            up = quantile(dist_cent, 0.975))\n\nggplot(df, aes(x = species, y = mean, color = species))+\n  geom_hline(aes(yintercept = 1), linewidth = 1, linetype = 'dashed')+\n  geom_pointrange(aes(ymin = low, ymax = up),\n                  size = 1.5, linewidth = 1.5, fatten = 2, \n                  position=position_dodge(width = 0.5))+\n  labs(x = NULL, y = 'Centroid distance') +\n  scale_x_discrete(labels = c(\"Bay \\nanchovy\",\n                              \"Mojarra\",\n                              \"Pigfish\",\n                              \"Pinfish\",\n                              \"Pink \\nshrimp\",\n                              \"Rainwater \\nkillifish\",\n                              \"Silver \\nperch\" ))+\n  scale_color_manual(values = cols)+\n  scale_y_continuous(limits = c(0,4.1))+\n  theme_bw()+\n  theme(axis.title = element_text(size = 14), \n        axis.text.y = element_text(size = 14, colour = \"black\"), \n        axis.text.x = element_text(size = 12, colour = \"black\"),\n        plot.title = element_text(size = 14, hjust=0.5),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = 'none',\n        legend.title = element_text(size = 14),\n        strip.text.x = element_text(size = 14),\n        legend.text = element_text(size = 12))"
  },
  {
    "objectID": "usefulfx.html",
    "href": "usefulfx.html",
    "title": "Useful functions for hypervolumes",
    "section": "",
    "text": "This script contains information about the functions used in the hypervolume scripts.\nR script of useful functions"
  },
  {
    "objectID": "usefulfx.html#mergejoin",
    "href": "usefulfx.html#mergejoin",
    "title": "Useful functions for hypervolumes",
    "section": "Merge/Join",
    "text": "Merge/Join\nIf two data frames contain different columns of data, then they can be merged together with the family of join functions.\n+left_join() = uses left df as template and joins all matching columns from right df +right_join() = uses right df as template and joins all matching columns from left df +inner_join() = only matches columns contained in both dfs +full_join() = combines all rows in both dfs\n\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nleft = tibble(name = c('a', 'b', 'c'),\n              n = c(1, 6, 7), \n              bio = c(100, 43, 57))\n\nright = tibble(name = c('a', 'b', 'd', 'e'),\n               cals = c(500, 450, 570, 600))\n\nleft_join(left, right, by = 'name')\n## # A tibble: 3 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n## 3 c         7    57    NA\n\nright_join(left, right, by = 'name')\n## # A tibble: 4 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n## 3 d        NA    NA   570\n## 4 e        NA    NA   600\n\ninner_join(left, right, by = 'name')\n## # A tibble: 2 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n\nfull_join(left, right, by = 'name')\n## # A tibble: 5 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n## 3 c         7    57    NA\n## 4 d        NA    NA   570\n## 5 e        NA    NA   600\n\n# multiple matches\nfish = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n              year = rep(c(1999,2005,2020), each = 2),\n              catch = c(50, 60, 40, 50, 60, 100))\n\ncol = tibble(species = c('Salmon', 'Cod'),\n             coast = c('West', 'East'))\n\nleft_join(fish, col, by = 'species')\n## # A tibble: 6 × 4\n##   species  year catch coast\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n## 1 Salmon   1999    50 West \n## 2 Cod      1999    60 East \n## 3 Salmon   2005    40 West \n## 4 Cod      2005    50 East \n## 5 Salmon   2020    60 West \n## 6 Cod      2020   100 East"
  },
  {
    "objectID": "usefulfx.html#scaling-data",
    "href": "usefulfx.html#scaling-data",
    "title": "Useful functions for hypervolumes",
    "section": "scaling data",
    "text": "scaling data\nBecause hypervolumes can be generated with any continuous data as an axes, many of the times the units are not comparable. Blonder et al. 2014 & 2018 to convert all of the axes into the same units. This can be done by taking the z-score of the values to convert units into standard deviations. Z-scoring data can be done with the formula: \\[ z = \\frac{x_{i}-\\overline{x}}{sd} \\] Where \\(x_{i}\\) is a value, \\(\\overline{x}\\) is the mean, and \\(sd\\) is the standard deviation. By z-scoring each axis, 0 is the mean of that axis, a value of 1 means that the value is 1 standard deviation above the global mean of that axis, and a value of -1 is 1 standard deviation below the global mean of the axis. In R this can be done manually or with the scale() function.\n\nfish = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n              year = rep(c(1999,2005,2020), each = 2),\n              catch = c(50, 60, 40, 50, 60, 100))\n\n#\nfish = fish |&gt; \n  mutate(zcatch1 = (catch - mean(catch))/sd(catch), # manual\n         zcatch2 = scale(catch)) # with scale\n\nfish \n## # A tibble: 6 × 5\n##   species  year catch zcatch1 zcatch2[,1]\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Salmon   1999    50  -0.477      -0.477\n## 2 Cod      1999    60   0           0    \n## 3 Salmon   2005    40  -0.953      -0.953\n## 4 Cod      2005    50  -0.477      -0.477\n## 5 Salmon   2020    60   0           0    \n## 6 Cod      2020   100   1.91        1.91\n\n# center = mean, scale = sd\nfish$zcatch2\n##            [,1]\n## [1,] -0.4767313\n## [2,]  0.0000000\n## [3,] -0.9534626\n## [4,] -0.4767313\n## [5,]  0.0000000\n## [6,]  1.9069252\n## attr(,\"scaled:center\")\n## [1] 60\n## attr(,\"scaled:scale\")\n## [1] 20.97618"
  },
  {
    "objectID": "usefulfx.html#nesting-data",
    "href": "usefulfx.html#nesting-data",
    "title": "Useful functions for hypervolumes",
    "section": "nesting data",
    "text": "nesting data\nOne benefit of tibbles is that they can contain list columns. This means that we can make columns of tibbles that are nested within a dataset. Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns. Nesting is a implicitly summarising operation: you get one row for each group defined by the non-nested columns. This is useful in conjunction with other summaries that work with whole datasets, most notably models. This can be done with the nest() and then flattened with unnest()\n\nfish = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n              year = rep(c(1999,2005,2020), each = 2),\n              catch = c(50, 60, 40, 50, 60, 100))\n\n# using group_by\nfish_nest = fish |&gt; \n  group_by(species) |&gt; \n  nest()\n\nfish_nest\n## # A tibble: 2 × 2\n## # Groups:   species [2]\n##   species data            \n##   &lt;chr&gt;   &lt;list&gt;          \n## 1 Salmon  &lt;tibble [3 × 2]&gt;\n## 2 Cod     &lt;tibble [3 × 2]&gt;\nfish_nest$data\n## [[1]]\n## # A tibble: 3 × 2\n##    year catch\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1  1999    50\n## 2  2005    40\n## 3  2020    60\n## \n## [[2]]\n## # A tibble: 3 × 2\n##    year catch\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1  1999    60\n## 2  2005    50\n## 3  2020   100\n\n# using .by in nest\n# column name becomes data unless you change .key\nfish_nest2 = fish |&gt; \n  nest(.by = year, .key = 'df')\n\nfish_nest2\n## # A tibble: 3 × 2\n##    year df              \n##   &lt;dbl&gt; &lt;list&gt;          \n## 1  1999 &lt;tibble [2 × 2]&gt;\n## 2  2005 &lt;tibble [2 × 2]&gt;\n## 3  2020 &lt;tibble [2 × 2]&gt;\nfish_nest2$df\n## [[1]]\n## # A tibble: 2 × 2\n##   species catch\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 Salmon     50\n## 2 Cod        60\n## \n## [[2]]\n## # A tibble: 2 × 2\n##   species catch\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 Salmon     40\n## 2 Cod        50\n## \n## [[3]]\n## # A tibble: 2 × 2\n##   species catch\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 Salmon     60\n## 2 Cod       100"
  },
  {
    "objectID": "usefulfx.html#map",
    "href": "usefulfx.html#map",
    "title": "Useful functions for hypervolumes",
    "section": "map",
    "text": "map\n\npurr\nThe newest and new standard package with tidyverse is purr with its set of map() functions. Some similarity to plyr (and base) and dplyr functions but with more consistent names and arguments. Notice that map function can have some specification for the type of output. + map() makes a list. + map_lgl() makes a logical vector. + map_int() makes an integer vector. + map_dbl() makes a double vector. + map_chr() makes a character vector.\n\ndf = iris  |&gt; \n  select(-Species)\n#summary statistics\nmap_dbl(df, mean)\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\n# using map with mutate and nest\nd = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n           year = rep(c(1999,2005,2020), each = 2),\n           catch = c(50, 60, 40, 50, 60, 100)) |&gt; \n  nest(.by = species) |&gt; \n  mutate(correlation = map(data, \\(data) cor.test(data$year, data$catch)))\n\nd\n## # A tibble: 2 × 3\n##   species data             correlation\n##   &lt;chr&gt;   &lt;list&gt;           &lt;list&gt;     \n## 1 Salmon  &lt;tibble [3 × 2]&gt; &lt;htest&gt;    \n## 2 Cod     &lt;tibble [3 × 2]&gt; &lt;htest&gt;\nd$correlation\n## [[1]]\n## \n##  Pearson's product-moment correlation\n## \n## data:  data$year and data$catch\n## t = 0.96225, df = 1, p-value = 0.5122\n## alternative hypothesis: true correlation is not equal to 0\n## sample estimates:\n##       cor \n## 0.6933752 \n## \n## \n## [[2]]\n## \n##  Pearson's product-moment correlation\n## \n## data:  data$year and data$catch\n## t = 1.963, df = 1, p-value = 0.3\n## alternative hypothesis: true correlation is not equal to 0\n## sample estimates:\n##       cor \n## 0.8910421\n\n# extract information from list\nd = d |&gt; \n  mutate(r = map_dbl(correlation, \\(x) x$estimate),\n         p = map_dbl(correlation, \\(x) x$p.value))\n\nd\n## # A tibble: 2 × 5\n##   species data             correlation     r     p\n##   &lt;chr&gt;   &lt;list&gt;           &lt;list&gt;      &lt;dbl&gt; &lt;dbl&gt;\n## 1 Salmon  &lt;tibble [3 × 2]&gt; &lt;htest&gt;     0.693 0.512\n## 2 Cod     &lt;tibble [3 × 2]&gt; &lt;htest&gt;     0.891 0.300\n\nSometimes, there are multiple arguments required for the function you are mapping. You can provide two lists (or columns) using map2(). If you have more than two lists needed, you can use pmap(). These functions work the same as map() based on the desired output.\n\nlibrary(performance)\n# using map2 and pmap\ndf = iris  |&gt; \n  nest(.by = Species) |&gt; \n  mutate(sw = map(data, \\(data) lm(Sepal.Length ~ Sepal.Width, data = data)),\n         pl = map(data, \\(data) lm(Sepal.Length ~ Petal.Length, data = data)),\n         mod_c = map2(sw,pl, \\(sw,pl) compare_performance(sw,pl)),\n         top = map_chr(mod_c, \\(x) x$Name[which.min(x$AICc)]),\n         sum_top = pmap(list(sw,pl,top), \\(x,y,z) \n                        if (z == 'sw'){\n                          summary(x)\n                        }else{\n                          summary(y)}),\n         p = map_dbl(sum_top, \\(s) s$coefficients[2, 4]))\n\n\ndf\n## # A tibble: 3 × 8\n##   Species    data              sw     pl    mod_c      top   sum_top           p\n##   &lt;fct&gt;      &lt;list&gt;            &lt;list&gt; &lt;lis&gt; &lt;list&gt;     &lt;chr&gt; &lt;list&gt;        &lt;dbl&gt;\n## 1 setosa     &lt;tibble [50 × 4]&gt; &lt;lm&gt;   &lt;lm&gt;  &lt;cmpr_prf&gt; sw    &lt;smmry.lm&gt; 6.71e-10\n## 2 versicolor &lt;tibble [50 × 4]&gt; &lt;lm&gt;   &lt;lm&gt;  &lt;cmpr_prf&gt; pl    &lt;smmry.lm&gt; 2.59e-10\n## 3 virginica  &lt;tibble [50 × 4]&gt; &lt;lm&gt;   &lt;lm&gt;  &lt;cmpr_prf&gt; pl    &lt;smmry.lm&gt; 6.30e-16"
  },
  {
    "objectID": "readings.html#workshop-week-1",
    "href": "readings.html#workshop-week-1",
    "title": "BSC6926-B53B Spring 2025",
    "section": "Workshop week 1",
    "text": "Workshop week 1\nBlonder 2017. Hypervolume concepts in niche- and trait-based ecology. Ecography"
  },
  {
    "objectID": "readings.html#workshop-week-2",
    "href": "readings.html#workshop-week-2",
    "title": "BSC6926-B53B Spring 2025",
    "section": "Workshop week 2",
    "text": "Workshop week 2\nLesser et al. 2020. Trophic niche size and overlap decreases with increasing ecosystem productivity. Oikos"
  },
  {
    "objectID": "comp.html",
    "href": "comp.html",
    "title": "BSC6926-B53B Spring 2025",
    "section": "",
    "text": "R and RStudio\nR and RStudio are separate downloads and installations. R is the underlying statistical computing environment, but using R alone is no fun. RStudio is a graphical integrated development environment (IDE) that makes using R much easier and more interactive. You need to install R before you install RStudio. In the sections below are the instructions for installing R and R Studio on your operating system.\n\nWindows\n\nIf you already have R and RStudio installed\n\nOpen RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio.\nTo check which version of R you are using, start RStudio and the first thing that appears in the console indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it. You can check here for more information on how to remove old versions from your system if you wish to do so.\n\n\n\nIf you don’t have R and RStudio installed\n\nDownload R from the CRAN website.\nRun the .exe file that was just downloaded\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Windows 10/11 (where x, y, and z represent version numbers)\nDouble click the file to install it\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages.\n\n\n\n\nmacOS\n\nIf you already have R and RStudio installed\n\nOpen RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio.\nTo check the version of R you are using, start RStudio and the first thing that appears on the terminal indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it.\n\n\n\nIf you don’t have R and RStudio installed\n\nDownload R from the CRAN website.\nSelect the .pkg file for the latest R version\nDouble click on the downloaded file to install R\nIt is also a good idea to install XQuartz (needed by some packages)\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Mac OS X 10.15+ (64-bit) (where x, y, and z represent version numbers)\nDouble click the file to install RStudio\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages.\n\n\n\n\nLinux\n\nFollow the instructions for your distribution from CRAN, they provide information to get the most recent version of R for common distributions. For most distributions, you could use your package manager (e.g., for Debian/Ubuntu run sudo apt-get install r-base, and for Fedora sudo yum install R), but we don’t recommend this approach as the versions provided by this are usually out of date. In any case, make sure you have at least R 3.3.1.\nGo to the RStudio download page\nUnder Installers select the version that matches your distribution, and install it with your preferred method (e.g., with Debian/Ubuntu sudo dpkg -i   rstudio-x.yy.zzz-amd64.deb at the terminal).\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSC6926-B53B Spring 2025",
    "section": "",
    "text": "BSC6926-B53B: Using a multivariate tool to assess population and community dynamics\nThis is the course website for the R workshop Using a multivariate tool to assess population and community dynamics. This website will have the Quarto markdown lessons for each workshop. Find the course schedule and Syllabus here. This course will be based in R and information about downloading R and Rstudio can be found here.\n\n\nClass Resources\n\nZoom link\nGithub repository\n\n\n\nR Resources\n\nR for Data Science by Hadley Wickham and Garret Grolemund – An introduction to programming with R: https://r4ds.hadley.nz/\n\nQuick-R by datacamp: Quick overview on R programming and statistical approaches. There are more tutorials, but you will be required to register\n\nRStudio Cloud Training Exercises: https://rstudio.cloud/learn/primers\n\nRstudio: learn R https://education.rstudio.com/learn/beginner/"
  },
  {
    "objectID": "sylb.html",
    "href": "sylb.html",
    "title": "Syllabus",
    "section": "",
    "text": "BSC6926-B53B: Using a multivariate tool to assess population and community dynamics \nSpring 2025\nFridays 10:00 am - 1:00 pm\nFormat:\n- In-person + Face-to-Face: MSB 113\n- Remote: Zoom link\n\nInstructors:\nW. Ryan James\nwjames@fiu.edu\nJustin Lesser\njlesser@fiu.edu\nRolando Santos\nrsantosc@fiu.edu\n\n\nCourse description and learning outcome:\nEcosystem structure and function are complex due to ecological and physicochemical processes occurring across multiple spatiotemporal scales. Due to advances in computing and statistical programming, new multivariate tools incorporating multi-scalar ecosystem responses are becoming increasingly popular to characterize ecosystem conditions and stability. One such technique is hypervolume modelling, which is based on Hutchinson’s n-dimensional hypervolume concept that describes the ecological requirements for an individual, species, or population to persist in a location. Due to the geometrical nature of hypervolumes and the flexibility of defining variables as axes, the hypervolume concept has been adapted to other ecological questions and processes above the species level (e.g., communities, habitats). The diverse use of hypervolumes includes quantifying species and community niche space, quantifying the state, stability, and resilience of ecosystems following disturbance and restoration, and how functional diversity, life-history strategies, and habitat filtering of community assemblages vary across environmental and disturbance gradients. During the workshop, we will discuss papers and cases related to these topics, and demonstrate how to conduct these assessments in R. These case studies will be followed by a walkthrough of the data and R code used to conduct the hypervolume analyses. R scripts and markdown/quarto files will be provided to encourage students to conduct hypervolume analysis on their own data and promote comparative cross-system studies.\nHypervolume modelling is a multivariate tool that can be used to characterize ecosystem condition and stability. Multiple case studies using hypervolumes in coastal ecosystems with the data and R code used to conduct the analyses will be presented.\n\n\nWhen and where:\nIn-Person Sessions: Lectures and hands-on programming/modeling exercises will be an integral part of the workshop’s learning experience; thus, most sessions will be based on face-to-face meetings to facilitate learning and assistance during the workshop sessions. In-person sessions will be offered only at BBC (MSB 113).\nCANVAS: We will upload workshop materials on CANVAS and communicate via CANVAS and email. We will announce any changes to the schedule one week or more in advance.\nOffice hours: By appointment - We are constantly in/out of the office; thus, please send an email or talk to us after the workshop lectures to set up an office meeting.\n\n\nTeaching schedule:\nEvery Friday from 03/14/2025-04/18/2025 (3hrs, 10 am to 1:00 pm) at BBC MSB-113. Changes to this schedule and other announcements regarding the course will be posted in CANVAS (sent to FIU email addresses only).\n\n\n\nLearning materials:\nRequired R scripts/files and readings for the workshop exercises will be posted to CANVAS and the workshop website https://seascapeecologylab-workshops.github.io/BSC6926-B53B_Spring2025.\nPlease bring a laptop to class so you can follow the R scripts during class and perform workshop exercises. Please, contact us if you do not have access to a laptop\nR and R studio are free software environment for statistical computing and graphics required for the workshop. Please download it to your laptops:\n   Download R: https://cran.r-project.org/bin/windows/base/\n   Download Rstudio: https://www.rstudio.com/products/rstudio/download/\nMaterials and links on R programming and statistical analyses helpful in learning R and the workshop exercises/homework/project\n\nR for Data Science by Hadley Wickham and Garret Grolemund – An introduction to programming with R: https://r4ds.had.co.nz/\n\nQuick-R by datacamp: Quick overview on R programming and statistical approaches.There are more tutorials, but you will be required to register\n\nRStudio Cloud Training Exercises: https://rstudio.cloud/learn/primers\n\nVirtual Ecology Portal/EcoVirtual R Package: Website that provides various examples of population and community models that will be discussed in class and the workshop. There is also an R package (EcoVirtual) you can use to run various models included on this website: http://ecovirtual.ib.usp.br/doku.php?id=start\n\nModernDive: Introductory book on R and statistical inference: https://moderndive.com/index.html\n\n\n\nTeaching:\nLectures and R programming exercises will be part of each workshop session to introduce several topics on multidimensional and hypervolume-based niche analysis. The grade will be based on class participation (50%), manuscript contributions (25%), and final presentation (25%).\n\n\nGrading:\nClass participation (Total 50 pts) will count for 50% of your final grade. Class attendance, participation in paper/topic discussions, including asking and answering questions, insightful comments and suggestions, and helping peers.\nManuscript Contribution (Total 25 pts) will count for 25% of your grade. As part of the workshop, we will work together to prepare a manuscript on a research topic developed during the workshop. The grade will be based on your contribution to the manuscript, which will entail providing two or three paragraphs.\nFinal Presentation (Total 25 pts) will count for 25% of your grade. Each student will receive a dataset that will contribute to the manuscript that we will prepare as a team. Thus, as part of this grade, you must present the results of a hypervolume analysis of the dataset assigned to you.\n\n\nRubric: \nGrade scale is A (pass): 100-80; C (fail): &lt;79. This is a graduate-level workshop; thus, if you are taking this course, it is because you have a genuine interest in ecological learning and developing your academic career. For this reason, final grades will be pass (A) or no pass (C). Students who don’t commit to the work will be given a C or less. Students will be graded on their performance in the above areas ONLY. Future career plans will have ZERO influence on the grade you receive in this class. Incomplete grades will be considered only under extraordinary circumstances.\n\n\nMake-up policy: \nI will provide make-up opportunities only when students present valid excuses (e.g., medical/family emergencies, COVID-19-related emergencies and precautions, major fieldwork trips, or conferences). Regardless of the reason, please contact me to identify alternative methods for completing course requirements, depending on the type of assignment missed and the severity of missed assignments. Below is a guideline of potential make-up scenarios/methods (Subject to change): - Participation – Due to the limited number of sessions, I will award a participation points for only one missed session with a validated and accepted excuse. - Manuscript/Final Presentation – Make-up alternatives only for medical (including COVID-19) emergencies. The make-up will consist of submitting the homework materials at a later date (e.g., 48-72 hours after the submission deadline), depending on the gravity of the emergency.\n\n\nFIU Discrimination, Harassment and Sexual Misconduct Policy Statement (Title IX):\nFlorida International University (the University) is committed to encouraging and sustaining a learning and living environment that is free from discrimination based on sex, including gender, gender expression, gender identity, and sexual orientation. Discrimination based on sex encompasses Sexual Misconduct, Sexual Harassment, Gender-Based Harassment, Domestic Violence, Dating Violence, and/or Stalking https://dei.fiu.edu/civil-rights-and-accessibility/index.html.\n\nFIU is committed to eliminating sexual harassment. In accordance with the FIU Faculty Senate guidelines, this syllabus includes a warning that any misconduct will be reported. FIU’s sexual harassment policy is available at: https://dei.fiu.edu/civil-rights-and-accessibility/sexual-misconduct/index.html\n\n\nProfessional and academic integrity:\nStudents are encouraged to employ critical thinking and rely on data and verifiable sources to interrogate all assigned readings and subject matter in this course as a way of determining whether they agree with their classmates and/or their instructor. No lesson is intended to espouse, promote, advance, inculcate, or compel a particular feeling, perception, viewpoint, or belief.\nFIU is a community dedicated to generating and imparting knowledge through excellent teaching and research, the rigorous and respectful exchange of ideas, and community service. All students should respect others’ right to have an equitable opportunity to learn and honestly demonstrate the quality of their learning. Therefore, all students are expected to adhere to a standard of academic conduct, which demonstrates respect for themselves, their fellow students, and the University’s educational mission. The University deems all students to understand that if they are found responsible for academic misconduct (e.g., cheating, plagiarism, academic dishonesty), they will be subject to the Academic Misconduct procedures and sanctions, as outlined in the Student Handbook.\n\n\nHonesty Code Statement:\nFIU defines academic misconduct in the Student Conduct and Honor Code (Code) as “any act or omission by a Student, which violates the concept of academic integrity and undermines the academic mission of the University in violation of the Code.” Code violations include, but are not limited to: academic dishonesty, bribery, cheating, commercial use, complicity, falsification, and plagiarism. The Code is available here: https://regulations.fiu.edu/regulation=FIU-2501"
  },
  {
    "objectID": "ws1.html",
    "href": "ws1.html",
    "title": "Workshop 1: Introduction to hypervolumes",
    "section": "",
    "text": "This workshop discusses working with community trait data by introducing the hypervolume package by going over how to prepare the data and how to build hypervolumes in multiple ways.\n\nR script: github\nR script of workshop 1\nabundance data\ntrait data"
  },
  {
    "objectID": "ws1.html#getting-to-know-the-basics",
    "href": "ws1.html#getting-to-know-the-basics",
    "title": "Workshop 1: Introduction to hypervolumes",
    "section": "",
    "text": "This workshop discusses working with community trait data by introducing the hypervolume package by going over how to prepare the data and how to build hypervolumes in multiple ways.\n\nR script: github\nR script of workshop 1\nabundance data\ntrait data"
  },
  {
    "objectID": "ws1.html#hypervolumes",
    "href": "ws1.html#hypervolumes",
    "title": "Workshop 1: Introduction to hypervolumes",
    "section": "Hypervolumes",
    "text": "Hypervolumes\nHypervolumes are a multidimensional tool that is based on Hutchinson’s n-dimensional niche concept and we can build them with the hypervolume package.\n\nPreparing the data\nTypically we have a dataset that has the abundance of the community and another that has trait data. Therefore we need to combine them. Also, we can’t have NAs, so we have to filter any missing data one we combine the datasets.\n\nlibrary(tidyverse)\n# abundance data\nab = read_csv('data/abundHermine.csv')\n## Rows: 68 Columns: 5\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): site, species, hur, period\n## dbl (1): abund\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# trait data\ntr = read_csv('data/fishTraits.csv')\n## Rows: 46 Columns: 7\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): species, project, Class, Order\n## dbl (3): trophic_level, temp_preference, generation_time\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# combine\ndf = left_join(ab, tr, by = 'species') |&gt; \n  drop_na()\n\nNow we can make the hypervolume by selecting the data to be included and z-scoring\n\ndf_before = df |&gt; \n  mutate(across(trophic_level:generation_time, scale)) |&gt; \n  filter(period == 'Before') |&gt; \n  select(trophic_level, temp_preference, generation_time) \n  \n\ndf_before\n## # A tibble: 23 × 3\n##    trophic_level[,1] temp_preference[,1] generation_time[,1]\n##                &lt;dbl&gt;               &lt;dbl&gt;               &lt;dbl&gt;\n##  1            -0.633              -0.597              -1.02 \n##  2            -0.913              -0.147              -1.16 \n##  3            -0.493              -0.957              -0.728\n##  4             0.974               1.20                1.44 \n##  5            -0.144              -0.147               1.44 \n##  6            -0.563               1.92               -0.393\n##  7            -0.563               0.393               1.03 \n##  8             0.345              -0.597               0.524\n##  9            -0.633              -0.597              -1.02 \n## 10            -0.913              -0.147              -1.16 \n## # ℹ 13 more rows\n\n\n\nBuilding hypervolumes\nWith a nested dataset of our columns that we want to build hypervolumes for we can use mutate() and map() to generate the hypervolume.\n\nlibrary(hypervolume)\n## Loading required package: Rcpp\n\nhv_before = hypervolume_gaussian(df_before, name = 'Before',\n                                              samples.per.point = 1000,\n                                              kde.bandwidth = estimate_bandwidth(df_before), \n                                              sd.count = 3, \n                                              quantile.requested = 0.95, \n                                              quantile.requested.type = \"probability\", \n                                              chunk.size = 1000, \n                                              verbose = F)\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n\nhv_before\n## ***** Object of class Hypervolume *****\n## Name: Before\n## Method: Gaussian kernel density estimate\n## Number of data points (after weighting): 23\n## Dimensionality: 3\n## Volume: 78.068034\n## Random point density: 171.606729\n## Number of random points: 13397\n## Random point values:\n##  min: 0.000\n##  mean: 0.001\n##  median: 0.000\n##  max:0.003\n## Parameters:\n##  kde.bandwidth: 0.6189038 0.6189038 0.6189038\n##  kde.method: silverman\n##  samples.per.point: 1000\n##  sd.count: 3\n##  quantile.requested: 0.95\n##  quantile.requested.type: probability\n\nThis can also be done to multiple at the same time with nest().\n\ndf = df |&gt; \n  select(period, trophic_level, temp_preference, generation_time) |&gt; \n  mutate(across(trophic_level:generation_time, scale)) |&gt; \n  group_by(period) |&gt; \n  nest()\n\ndf\n## # A tibble: 2 × 2\n## # Groups:   period [2]\n##   period data             \n##   &lt;chr&gt;  &lt;list&gt;           \n## 1 Before &lt;tibble [23 × 3]&gt;\n## 2 After  &lt;tibble [25 × 3]&gt;\n\ndf = df |&gt; \n  mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = period,\n                                              samples.per.point = 1000,\n                                              kde.bandwidth = estimate_bandwidth(data), \n                                              sd.count = 3, \n                                              quantile.requested = 0.95, \n                                              quantile.requested.type = \"probability\", \n                                              chunk.size = 1000, \n                                              verbose = F)))\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n\nhead(df)\n## # A tibble: 2 × 3\n## # Groups:   period [2]\n##   period data              hv        \n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;    \n## 1 Before &lt;tibble [23 × 3]&gt; &lt;Hypervlm&gt;\n## 2 After  &lt;tibble [25 × 3]&gt; &lt;Hypervlm&gt;\ndf$hv\n## [[1]]\n## ***** Object of class Hypervolume *****\n## Name: Before\n## Method: Gaussian kernel density estimate\n## Number of data points (after weighting): 23\n## Dimensionality: 3\n## Volume: 70.900748\n## Random point density: 188.883762\n## Number of random points: 13392\n## Random point values:\n##  min: 0.000\n##  mean: 0.001\n##  median: 0.000\n##  max:0.004\n## Parameters:\n##  kde.bandwidth: 0.5996196 0.5570341 0.6430513\n##  kde.method: silverman\n##  samples.per.point: 1000\n##  sd.count: 3\n##  quantile.requested: 0.95\n##  quantile.requested.type: probability\n## \n## [[2]]\n## ***** Object of class Hypervolume *****\n## Name: After\n## Method: Gaussian kernel density estimate\n## Number of data points (after weighting): 25\n## Dimensionality: 3\n## Volume: 90.256157\n## Random point density: 160.354712\n## Number of random points: 14473\n## Random point values:\n##  min: 0.000\n##  mean: 0.000\n##  median: 0.000\n##  max:0.002\n## Parameters:\n##  kde.bandwidth: 0.6391039 0.6706801 0.5990887\n##  kde.method: silverman\n##  samples.per.point: 1000\n##  sd.count: 3\n##  quantile.requested: 0.95\n##  quantile.requested.type: probability\n\n\n\nplotting hypervolumes\nWe can plot multiple hypervolumes by joining them together\n\nhvj = hypervolume_join(df$hv[[1]], df$hv[[2]])\n\nplot(hvj, pairplot = T, colors=c('dodgerblue3','cyan2'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)\n\n\n\n\n\n\n\n\n\n\nhypervolume metrics\nThe geometry of hypervolumes are useful when characterizing and comparing hypervolumes. Hypervolume size represents the variation of the data, centroid distance compares the euclidian distance between two hypervolume centroids (mean conditions), and overlap measures the simularity of hypervolumes.\n\n# size \ndf = df |&gt; \n  mutate(hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n\nhead(df)\n## # A tibble: 2 × 4\n## # Groups:   period [2]\n##   period data              hv         hv_size\n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;       &lt;dbl&gt;\n## 1 Before &lt;tibble [23 × 3]&gt; &lt;Hypervlm&gt;    70.9\n## 2 After  &lt;tibble [25 × 3]&gt; &lt;Hypervlm&gt;    90.3\n\n# centroid distance \nhypervolume_distance(df$hv[[1]], df$hv[[2]], type = 'centroid', check.memory=F)\n## [1] 0.2435712\n\n# overlap \nhvset = hypervolume_set(df$hv[[1]], df$hv[[2]], check.memory = F)\n## Choosing num.points.max=53958 (use a larger value for more accuracy.)\n## Using minimum density of 160.354712\n## Retaining 11369 points in hv1 and 14473 points in hv2.\n## Beginning ball queries... \n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## Finished ball queries.\n\nhypervolume_overlap_statistics(hvset)\n##       jaccard      sorensen frac_unique_1 frac_unique_2 \n##    0.75154245    0.85814929    0.02471633    0.23386566\n\n\n\nWeight hypervolume input\nThe above hypervolume is just based on the traits using presence of species, but we can weight the points to shape the hypervolume based on abundance\n\n\n#prep data\ndf_w = left_join(ab, tr, by = 'species') |&gt; \n  drop_na() |&gt; \n  select(period, abund, trophic_level, temp_preference, generation_time) |&gt; \n  mutate(across(trophic_level:generation_time, scale)) |&gt; \n  group_by(period) |&gt; \n  nest(weight = abund, data = trophic_level:generation_time) \ndf_w\n## # A tibble: 2 × 3\n## # Groups:   period [2]\n##   period weight            data             \n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           \n## 1 Before &lt;tibble [23 × 1]&gt; &lt;tibble [23 × 3]&gt;\n## 2 After  &lt;tibble [25 × 1]&gt; &lt;tibble [25 × 3]&gt;\n\n# make hypervolumes\ndf_w = df_w |&gt; \n    mutate(hv = map2(data,weight, \\(data,weight) hypervolume_gaussian(data, name = paste(period,'weighted',sep = '_'),\n                                                      weight = weight$abund,\n                                                      samples.per.point = 1000,\n                                                      kde.bandwidth = estimate_bandwidth(data), \n                                                      sd.count = 3, \n                                                      quantile.requested = 0.95, \n                                                      quantile.requested.type = \"probability\", \n                                                      chunk.size = 1000, \n                                                      verbose = F)),\n           hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Warning: There were 2 warnings in `mutate()`.\n## The first warning was:\n## ℹ In argument: `hv = map2(...)`.\n## ℹ In group 1: `period = \"After\"`.\n## Caused by warning in `hypervolume_gaussian()`:\n## ! The sum of the weights must be equal to 1. Normalizing the weights.\n## ℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\ndf_w\n## # A tibble: 2 × 5\n## # Groups:   period [2]\n##   period weight            data              hv         hv_size\n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;            &lt;list&gt;       &lt;dbl&gt;\n## 1 Before &lt;tibble [23 × 1]&gt; &lt;tibble [23 × 3]&gt; &lt;Hypervlm&gt;    19.0\n## 2 After  &lt;tibble [25 × 1]&gt; &lt;tibble [25 × 3]&gt; &lt;Hypervlm&gt;    24.4\n\n\nhvj_w = hypervolume_join(df_w$hv[[1]], df_w$hv[[2]])\n\nplot(hvj_w, pairplot = T, colors=c('dodgerblue3','cyan2'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)\n\n\n\n\n\n\n\n\n# centroid distance \nhypervolume_distance(df_w$hv[[1]], df_w$hv[[2]], type = 'centroid', check.memory=F)\n## [1] 0.01744452\n\n# overlap \nhvset_w = hypervolume_set(df_w$hv[[1]], df_w$hv[[2]], check.memory = F)\n## Choosing num.points.max=53958 (use a larger value for more accuracy.)\n## Using minimum density of 160.631671\n## Retaining 3047 points in hv1 and 3921 points in hv2.\n## Beginning ball queries... \n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## Finished ball queries.\n\nhypervolume_overlap_statistics(hvset_w)\n##       jaccard      sorensen frac_unique_1 frac_unique_2 \n##    0.74999133    0.85713720    0.02001969    0.23833902\n\n\n\nFrom mean and sd\nSometimes, we do not have enough points to meet assumptions to make a hypervolume. Therefore, we can simulate random points based on mean and sd of our axes. We can then simulate the information needed and make our hypervolumes.\n\n# mean and sd\ndf_m = left_join(ab, tr, by = 'species') |&gt; \n  drop_na() |&gt; \n  pivot_longer(trophic_level:generation_time, names_to = 'trait', values_to = 'value') |&gt; \n  group_by(period,trait) |&gt; \n  summarize(mean = mean(value),\n            sd = sd(value))\n## `summarise()` has grouped output by 'period'. You can override using the\n## `.groups` argument.\n\ndf_m\n## # A tibble: 6 × 4\n## # Groups:   period [2]\n##   period trait            mean    sd\n##   &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n## 1 After  generation_time  3.10 1.79 \n## 2 After  temp_preference 25.1  1.22 \n## 3 After  trophic_level    3.52 0.150\n## 4 Before generation_time  3.35 1.90 \n## 5 Before temp_preference 25.0  1.00 \n## 6 Before trophic_level    3.54 0.139\n\n#generate points \n# number of points \nn = 50 \n\ndf_tot = df_m |&gt; slice(rep(1:n(), each=n))|&gt;\n      mutate(point = map2_dbl(mean,sd, \\(mean,sd) rnorm(1,mean =mean,sd =sd))) |&gt; \n      group_by(period, trait) |&gt; \n      mutate(num = row_number()) |&gt;\n      ungroup() |&gt; \n      select(-mean, -sd)|&gt;\n      pivot_wider(names_from = trait, values_from = point)|&gt; \n      select(-num) |&gt; \n      mutate(across(generation_time:trophic_level,scale)) |&gt; \n      group_by(period) |&gt; \n      nest()\n\n\n# generate hypervolumes\ndf_tot = df_tot |&gt; \n  mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = period,\n                                              samples.per.point = 1000,\n                                              kde.bandwidth = estimate_bandwidth(data), \n                                              sd.count = 3, \n                                              quantile.requested = 0.95, \n                                              quantile.requested.type = \"probability\", \n                                              chunk.size = 1000, \n                                              verbose = F)),\n         hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n\n\n#plot\nhvj_tot = hypervolume_join(df_tot$hv[[1]], df_tot$hv[[2]])\n\nplot(hvj_tot, pairplot = T, colors=c('dodgerblue3','cyan2'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)\n\n\n\n\n\n\n\n\n# centroid distance \nhypervolume_distance(df_tot$hv[[1]], df_tot$hv[[2]], type = 'centroid', check.memory=F)\n## [1] 0.173522\n\n# overlap \nhvset_tot = hypervolume_set(df_tot$hv[[1]], df_tot$hv[[2]], check.memory = F)\n## Choosing num.points.max=53958 (use a larger value for more accuracy.)\n## Using minimum density of 251.407651\n## Retaining 25684 points in hv1 and 24089 points in hv2.\n## Beginning ball queries... \n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## Finished ball queries.\n\nhypervolume_overlap_statistics(hvset_tot)\n##       jaccard      sorensen frac_unique_1 frac_unique_2 \n##     0.5654774     0.7224345     0.2999922     0.2536543"
  }
]