[
  {
    "objectID": "ws1.html",
    "href": "ws1.html",
    "title": "Workshop 1: Introduction to hypervolumes",
    "section": "",
    "text": "This workshop discusses working with community trait data by introducing the hypervolume package by going over how to prepare the data and how to build hypervolumes in multiple ways.\n\nR script: github\nR script of workshop 9\nabundance data\ntrait data"
  },
  {
    "objectID": "ws1.html#getting-to-know-the-basics",
    "href": "ws1.html#getting-to-know-the-basics",
    "title": "Workshop 1: Introduction to hypervolumes",
    "section": "",
    "text": "This workshop discusses working with community trait data by introducing the hypervolume package by going over how to prepare the data and how to build hypervolumes in multiple ways.\n\nR script: github\nR script of workshop 9\nabundance data\ntrait data"
  },
  {
    "objectID": "ws1.html#basic-operations-in-r",
    "href": "ws1.html#basic-operations-in-r",
    "title": "Workshop 1: Introduction to R",
    "section": "Basic operations in R",
    "text": "Basic operations in R\nR is useful for basic operations and follows math rules (i.e. PEMDAS). R will all code on a line unless there is a # to the left.\n\n# addition \n1+1 \n## [1] 2\n\n1+1 # + 2 (won't run anything to right of #)\n## [1] 2\n\n# subtraction\n5-2 \n## [1] 3\n\n# multiplication\n4*5\n## [1] 20\n\n# division\n33/5\n## [1] 6.6\n\n# exponents can be done 2 ways\n2^2\n## [1] 4\n2**2\n## [1] 4\n\n# follows PEMDAS\n1+5*4\n## [1] 21\n# different answer than above\n(1+5)*4\n## [1] 24\n\nNote the [1] appears next to your result. R is just letting you know that this line begins with the first value in your result. Some commands return more than one value, and their results may fill up multiple lines. For example, the command 100:130 returns 31 values; it creates a sequence of integers from 100 to 130. Notice that new bracketed numbers appear at the start of the first and second lines of output. These numbers just mean that the second line begins with that value. You can mostly ignore the numbers that appear in brackets:\n\n100:130\n##  [1] 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n## [20] 119 120 121 122 123 124 125 126 127 128 129 130"
  },
  {
    "objectID": "ws1.html#assigning-objects",
    "href": "ws1.html#assigning-objects",
    "title": "Workshop 1: Introduction to R",
    "section": "Assigning objects",
    "text": "Assigning objects\nWhen working in R it is useful to store data as an object. Assigning objects can be done in multiple ways, but the most common are &lt;- and =. These objects are stored in the R environment and can be called. Objects can be assigned multiple times, but only the last assignment is what is stored. Also it is important to know that R is case sensative and capital and lower case numbers are different.\n\n# assign an object\na = 4 \na\n## [1] 4\n\nb &lt;- 23\n\na+3 \n## [1] 7\n\nb/2\n## [1] 11.5\n\na*b\n## [1] 92\n\nc = 8\nc = 14\nc\n## [1] 14\n\nd = 15 \nD = 1 \nd\n## [1] 15\nD\n## [1] 1"
  },
  {
    "objectID": "ws1.html#types-of-data-structures-in-r",
    "href": "ws1.html#types-of-data-structures-in-r",
    "title": "Workshop 1: Introduction to R",
    "section": "Types of data structures in R",
    "text": "Types of data structures in R\nR has 6 basic data types. (In addition to the five listed below, there is also raw which will not be discussed in this workshop.)\n\ninteger\nnumeric (real or decimal)\ncharacter\nlogical\ncomplex\n\nintegers are whole numbers\nnumeric are numbers with decimals. Integers and numeric are different because of how the underlying data is stored. Other programming languages can use something similar as decimal, float, or double data types, which all slightly differ in how data is stored but are numbers that include decimals.\ncharacters are strings of letters and numbers (e.g. \"abc\" and \"b1x\") and are designated in R by \" \". When using characters, \" \" are required because in R letters without quotations are objects and c = 'd' is different than c = d\nlogical is TRUE or FALSE. One thing to note is that T is the same as TRUE and F is the same as FALSE. Because T and F are special in R they cannot be used to name objects (but t and f are ok because R is case sensative). This is true for other cases as well like NA and NULL.\ncomplex numbers have both real and imaginary parts (1+4i)\nElements of these data types may be combined to form data structures, such as atomic vectors. When we call a vector atomic, we mean that the vector only holds data of a single data type. A vector is the most common and basic data structure in R and is pretty much the workhorse of R. Technically, vectors can be one of two types: + atomic vectors + lists although the term “vector” most commonly refers to the atomic types not to lists. Lists differ because they can take on different data structures and can be more complex.\nThere are different ways to make vectors\n\n\n# make a numeric vector\na = c(1.1,5,3,4)\na\n## [1] 1.1 5.0 3.0 4.0\n\n# make a integer vector\nb = 1:15\nb\n##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n# make a character vector \nc = c('a', 'b', 'c')\nc \n## [1] \"a\" \"b\" \"c\"\n\nBecause characters can be both letters and numbers, numbers in a vector with letters are stored as a character. These cannot be used for math operations, but integers and numeric data types can be used for math.\n\na = 4.4\na / 1 \n## [1] 4.4\n\n\nb = 6L # L can be used to keep a numeric as an integer, R typically defaults to numeric\nb*3\n## [1] 18\n\n# character\nc = '1'\nc*4\n## Error in c * 4: non-numeric argument to binary operator\n\nAnother common way to store data is in a dataframe or tibble (special type of dataframe from the tidyverse package we will see below). This is a collection of atomic vectors with the same length.\n\nb = data.frame(c1 = c(1,2,3), c2 = c('a','b','c'))\nb\n##   c1 c2\n## 1  1  a\n## 2  2  b\n## 3  3  c"
  },
  {
    "objectID": "ws1.html#functions-in-r",
    "href": "ws1.html#functions-in-r",
    "title": "Workshop 1: Introduction to R",
    "section": "Functions in R",
    "text": "Functions in R\nR comes with functions that are used to do tasks. Functions take arguments to complete a task. Functions have the general format function(argument1 = , argument2,...) The types of data used and output of the function is specific to that function. Below are just a few useful examples.\n\n# summary statistics of sequence of numbers\na = c(1.1,5,3,4)\nmean(a) #mean\n## [1] 3.275\nmedian(a) #median\n## [1] 3.5\nsd(a) #standard deviation\n## [1] 1.664081\nquantile(a, 0.5) # quantile at 0.5 (median)\n## 50% \n## 3.5\n\n# make a sequence of numbers\nb = 1:15\nb\n##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\nc = seq(1,15,1) #more flexibility than :\nc\n##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\nseq(4,20,2)\n## [1]  4  6  8 10 12 14 16 18 20\n\n# information about objects\nd = c('a', 'b', 'c')\ntypeof(d) \n## [1] \"character\"\ntypeof(c)\n## [1] \"double\"\nlength(d)\n## [1] 3\n\n# dataframe/tibble specific functions\ne = data.frame(c1 = c(1,2,3), c2 = c('a','b','c'))\nnames(e) # column names\n## [1] \"c1\" \"c2\"\nnrow(e) # number of rows\n## [1] 3\nlength(e) # for dataframe number of columns\n## [1] 2\nstr(e)# structure of data\n## 'data.frame':    3 obs. of  2 variables:\n##  $ c1: num  1 2 3\n##  $ c2: chr  \"a\" \"b\" \"c\""
  },
  {
    "objectID": "ws1.html#using-packages-in-r",
    "href": "ws1.html#using-packages-in-r",
    "title": "Workshop 1: Introduction to R",
    "section": "Using Packages in R",
    "text": "Using Packages in R\nR comes with a lot of base functions that are available for use when you open R, but this does not contain all of the functions useful to your tasks in R. Since R is open source, many R users have created Packages that contain functions that can be downloaded. Which includes the very common tidyverse.\n\nHow to install and load packages\nPackages can be downloaded from CRAN or from Github. To download directly from Github other packages are needed.\n\ninstall.packages('tidyverse') #from cran\n\nOnce downloaded, packages can be loaded into the R environment with library() function. Packages have to be loaded each R session. In addition functions can be called directly from a package with :: in the format of packageName::function().\n\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "ws1.html#tidyverse",
    "href": "ws1.html#tidyverse",
    "title": "Workshop 1: Introduction to R",
    "section": "tidyverse",
    "text": "tidyverse\ntidyverse is a collection of packages that use similar syntax and are used for data science in R. Coding in tidyverse is typically easy to read and understand, and has useful functions that have been adopted into newer versions of base R (e.g. piping). Tibbles are the tidyverse version of a dataframe.\n\nc = tibble(c1 = c(1,2,3), c2 = c('a','b','c'))\nc\n## # A tibble: 3 × 2\n##      c1 c2   \n##   &lt;dbl&gt; &lt;chr&gt;\n## 1     1 a    \n## 2     2 b    \n## 3     3 c"
  },
  {
    "objectID": "ws1.html#working-with-dataframes-and-tibbles",
    "href": "ws1.html#working-with-dataframes-and-tibbles",
    "title": "Workshop 1: Introduction to R",
    "section": "Working with dataframes and tibbles",
    "text": "Working with dataframes and tibbles\nUsing either dataframes or tibbles will likely be the most common data structure for ecological data. Making these data structures is easy with the data.frame() or tibble() functions. Tibbles have more flexibility than dataframes and are part of the tidyverse. Dataframes are base R. When reading in tabular data, read.csv() will create a dataframe, while read_csv() will generate a tibble. read_csv() can be paired with url() to use data directly from the internet from sites like github. Note that if from github the raw file (click on raw tab when looking at github file) is needed for this to work. Similar to reading in data, dataframes and tibbles can be saved as .csv with write.csv() or write_csv().\n\nlibrary(tidyverse)\n# create a dataframe\n\ndf = data.frame(name = c('GOOG', 'AMC', 'GME'),\n                Jan = c(1000, 2, 4),\n                Feb = c(1010, 15, 30),\n                March = c(1005, 25, 180))\n\ndf\n##   name  Jan  Feb March\n## 1 GOOG 1000 1010  1005\n## 2  AMC    2   15    25\n## 3  GME    4   30   180\n\n# create a tibble\ntib = tibble(name = c('GOOG', 'AMC', 'GME'),\n             Jan = c(1000, 2, 4),\n             Feb = c(1010, 15, 30),\n             March = c(1005, 25, 180))\n\ntib\n## # A tibble: 3 × 4\n##   name    Jan   Feb March\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 GOOG   1000  1010  1005\n## 2 AMC       2    15    25\n## 3 GME       4    30   180\n\n#read in data file on computer\n# change file path to path location on computer\n# read.csv('data/LDWFBayAnchovy2007.csv')\n# \n# read_csv('data/LDWFBayAnchovy2007.csv')\n# \n# # read in data file from github\n# # need to use raw file\n# read_csv(url('https://raw.githubusercontent.com/SeascapeEcologyLab-workshops/BSC6926-B52_Fall2024/main/data/LDWFBayAnchovy2007.csv'))\n# \n# # save dataframe or tibble as new csv\n# write.csv(df, 'data/df.csv')\n# \n# write_csv(df, 'data/df.csv')"
  },
  {
    "objectID": "ws1.html#indexing",
    "href": "ws1.html#indexing",
    "title": "Workshop 1: Introduction to R",
    "section": "Indexing",
    "text": "Indexing\nOnce data is stored in an object, being able to retrieve those values is useful. Referred to as indexing, the syntax is specific to how the data is stored. With indexing specific values within your object can be modified.\n\n# vector \nb = 1:15\n# 3rd object \nb[3]\n## [1] 3\n\n# make a character vector \nc = c('a', 'b', 'c')\nc\n## [1] \"a\" \"b\" \"c\"\n# 2nd object\nc[2]\n## [1] \"b\"\n# change \nc[2] = 'new'\nc\n## [1] \"a\"   \"new\" \"c\"\n\n# dataframe and tibbles\nmtcars\n##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n# first column\nmtcars[1]\n##                      mpg\n## Mazda RX4           21.0\n## Mazda RX4 Wag       21.0\n## Datsun 710          22.8\n## Hornet 4 Drive      21.4\n## Hornet Sportabout   18.7\n## Valiant             18.1\n## Duster 360          14.3\n## Merc 240D           24.4\n## Merc 230            22.8\n## Merc 280            19.2\n## Merc 280C           17.8\n## Merc 450SE          16.4\n## Merc 450SL          17.3\n## Merc 450SLC         15.2\n## Cadillac Fleetwood  10.4\n## Lincoln Continental 10.4\n## Chrysler Imperial   14.7\n## Fiat 128            32.4\n## Honda Civic         30.4\n## Toyota Corolla      33.9\n## Toyota Corona       21.5\n## Dodge Challenger    15.5\n## AMC Javelin         15.2\n## Camaro Z28          13.3\n## Pontiac Firebird    19.2\n## Fiat X1-9           27.3\n## Porsche 914-2       26.0\n## Lotus Europa        30.4\n## Ford Pantera L      15.8\n## Ferrari Dino        19.7\n## Maserati Bora       15.0\n## Volvo 142E          21.4\n# first row\nmtcars[1,]\n##           mpg cyl disp  hp drat   wt  qsec vs am gear carb\n## Mazda RX4  21   6  160 110  3.9 2.62 16.46  0  1    4    4\n# 2nd row of first column\nmtcars[2,1]\n## [1] 21\n# can call specific columns (called as a vector)\nmtcars$mpg\n##  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n## [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n## [31] 15.0 21.4\nmtcars$cyl\n##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n#same for tibble\nd = mtcars %&gt;% as_tibble\nd[1]\n## # A tibble: 32 × 1\n##      mpg\n##    &lt;dbl&gt;\n##  1  21  \n##  2  21  \n##  3  22.8\n##  4  21.4\n##  5  18.7\n##  6  18.1\n##  7  14.3\n##  8  24.4\n##  9  22.8\n## 10  19.2\n## # ℹ 22 more rows\nd$mpg\n##  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n## [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n## [31] 15.0 21.4\nd$cyl\n##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n# specific row in specific column\nmtcars$cyl[1]\n## [1] 6\nd$cyl[1]\n## [1] 6"
  },
  {
    "objectID": "ws1.html#exercises",
    "href": "ws1.html#exercises",
    "title": "Workshop 1: Introduction to R",
    "section": "Exercises",
    "text": "Exercises\n\nComplete following exercises and turn in r script on canvas\n\nMake two vectors, object a containing the values 2, 3, 4, and 5 and object bcontaining the values 50, 100, 38, and 42.\nMultiply object a by 3 and assign it to a new object, divide object b by 5 and assign it to a new object, then add the new two objects together.\nCreate a new data.frame/tibble with the four objects created above\nSave the data.frame/tibble created in exercise 3 as a .csv\nLoad in files a.csv and b.csv (found on github and canvas) and assign each as an object.\n\nExercise Solutions"
  },
  {
    "objectID": "sylb.html",
    "href": "sylb.html",
    "title": "Syllabus",
    "section": "",
    "text": "BSC6926-B53B: Using a multivariate tool to assess population and community dynamics \nSpring 2025\nFridays 10:00 am - 1:00 pm\nFormat:\n- In-person + Face-to-Face: MSB 113\n- Remote: Zoom link\n\nInstructors:\nW. Ryan James\nwjames@fiu.edu\nJustin Lesser\njlesser@fiu.edu\nRolando Santos\nrsantosc@fiu.edu\n\n\nCourse description and learning outcome:\nEcosystem structure and function are complex due to ecological and physicochemical processes occurring across multiple spatiotemporal scales. Due to advances in computing and statistical programming, new multivariate tools incorporating multi-scalar ecosystem responses are becoming increasingly popular to characterize ecosystem conditions and stability. One such technique is hypervolume modelling, which is based on Hutchinson’s n-dimensional hypervolume concept that describes the ecological requirements for an individual, species, or population to persist in a location. Due to the geometrical nature of hypervolumes and the flexibility of defining variables as axes, the hypervolume concept has been adapted to other ecological questions and processes above the species level (e.g., communities, habitats). The diverse use of hypervolumes includes quantifying species and community niche space, quantifying the state, stability, and resilience of ecosystems following disturbance and restoration, and how functional diversity, life-history strategies, and habitat filtering of community assemblages vary across environmental and disturbance gradients. During the workshop, we will discuss papers and cases related to these topics, and demonstrate how to conduct these assessments in R. These case studies will be followed by a walkthrough of the data and R code used to conduct the hypervolume analyses. R scripts and markdown/quarto files will be provided to encourage students to conduct hypervolume analysis on their own data and promote comparative cross-system studies.\nHypervolume modelling is a multivariate tool that can be used to characterize ecosystem condition and stability. Multiple case studies using hypervolumes in coastal ecosystems with the data and R code used to conduct the analyses will be presented.\n\n\nWhen and where:\nIn-Person Sessions: Lectures and hands-on programming/modeling exercises will be an integral part of the workshop’s learning experience; thus, most sessions will be based on face-to-face meetings to facilitate learning and assistance during the workshop sessions. In-person sessions will be offered only at BBC (MSB 113).\nCANVAS: We will upload workshop materials on CANVAS and communicate via CANVAS and email. We will announce any changes to the schedule one week or more in advance.\nOffice hours: By appointment - We are constantly in/out of the office; thus, please send an email or talk to us after the workshop lectures to set up an office meeting.\n\n\nTeaching schedule:\nEvery Friday from 03/14/2025-04/18/2025 (3hrs, 10 am to 1:00 pm) at BBC MSB-113. Changes to this schedule and other announcements regarding the course will be posted in CANVAS (sent to FIU email addresses only).\n\n\n\nLearning materials:\nRequired R scripts/files and readings for the workshop exercises will be posted to CANVAS and the workshop website https://seascapeecologylab-workshops.github.io/BSC6926-B53B_Spring2025.\nPlease bring a laptop to class so you can follow the R scripts during class and perform workshop exercises. Please, contact us if you do not have access to a laptop\nR and R studio are free software environment for statistical computing and graphics required for the workshop. Please download it to your laptops:\n   Download R: https://cran.r-project.org/bin/windows/base/\n   Download Rstudio: https://www.rstudio.com/products/rstudio/download/\nMaterials and links on R programming and statistical analyses helpful in learning R and the workshop exercises/homework/project\n\nR for Data Science by Hadley Wickham and Garret Grolemund – An introduction to programming with R: https://r4ds.had.co.nz/\n\nQuick-R by datacamp: Quick overview on R programming and statistical approaches.There are more tutorials, but you will be required to register\n\nRStudio Cloud Training Exercises: https://rstudio.cloud/learn/primers\n\nVirtual Ecology Portal/EcoVirtual R Package: Website that provides various examples of population and community models that will be discussed in class and the workshop. There is also an R package (EcoVirtual) you can use to run various models included on this website: http://ecovirtual.ib.usp.br/doku.php?id=start\n\nModernDive: Introductory book on R and statistical inference: https://moderndive.com/index.html\n\n\n\nTeaching:\nLectures and R programming exercises will be part of each workshop session to introduce several topics on multidimensional and hypervolume-based niche analysis. The grade will be based on class participation (50%), manuscript contributions (25%), and final presentation (25%).\n\n\nGrading:\nThe final grade will be composed of 2 assignments/tasks. These two assignments/tasks are: 1) Class participation (attendance and class exercises) and 2) R exercises homework.\nClass participation (Total 50 pts) will count for 50% of your final grade. Class attendance, participation in paper/topic discussions, including asking and answering questions, insightful comments and suggestions, and helping peers.\nManuscript Contribution (Total 25 pts) will count for 25% of your grade. As part of the workshop, we will work together to prepare a manuscript on a research topic developed during the workshop. The grade will be based on your contribution to the manuscript, which will entail providing two or three paragraphs.\nFinal Presentation (Total 25 pts) will count for 25% of your grade. Each student will receive a dataset that will contribute to the manuscript that we will prepare as a team. Thus, as part of this grade, you must present the results of a hypervolume analysis of the dataset assigned to you.\n\n\nRubric: \nGrade scale is A (pass): 100-80; C (fail): &lt;79. This is a graduate-level workshop; thus, if you are taking this course, it is because you have a genuine interest in ecological learning and developing your academic career. For this reason, final grades will be pass (A) or no pass (C). Students who don’t commit to the work will be given a C or less. Students will be graded on their performance in the above areas ONLY. Future career plans will have ZERO influence on the grade you receive in this class. Incomplete grades will be considered only under extraordinary circumstances.\n\n\nMake-up policy: \nI will provide make-up opportunities only when students present valid excuses (e.g., medical/family emergencies, COVID-19-related emergencies and precautions, major fieldwork trips, or conferences). Regardless of the reason, please contact me to identify alternative methods for completing course requirements, depending on the type of assignment missed and the severity of missed assignments. Below is a guideline of potential make-up scenarios/methods (Subject to change): - Participation – Due to the limited number of sessions, I will award a participation points for only one missed session with a validated and accepted excuse. - Manuscript/Final Presentation – Make-up alternatives only for medical (including COVID-19) emergencies. The make-up will consist of submitting the homework materials at a later date (e.g., 48-72 hours after the submission deadline), depending on the gravity of the emergency.\n\n\nFIU Discrimination, Harassment and Sexual Misconduct Policy Statement (Title IX):\nFlorida International University (the University) is committed to encouraging and sustaining a learning and living environment that is free from discrimination based on sex, including gender, gender expression, gender identity, and sexual orientation. Discrimination based on sex encompasses Sexual Misconduct, Sexual Harassment, Gender-Based Harassment, Domestic Violence, Dating Violence, and/or Stalking https://dei.fiu.edu/civil-rights-and-accessibility/index.html.\n\nFIU is committed to eliminating sexual harassment. In accordance with the FIU Faculty Senate guidelines, this syllabus includes a warning that any misconduct will be reported. FIU’s sexual harassment policy is available at: https://dei.fiu.edu/civil-rights-and-accessibility/sexual-misconduct/index.html\n\n\nProfessional and academic integrity:\nStudents are encouraged to employ critical thinking and rely on data and verifiable sources to interrogate all assigned readings and subject matter in this course as a way of determining whether they agree with their classmates and/or their instructor. No lesson is intended to espouse, promote, advance, inculcate, or compel a particular feeling, perception, viewpoint, or belief.\nFIU is a community dedicated to generating and imparting knowledge through excellent teaching and research, the rigorous and respectful exchange of ideas, and community service. All students should respect others’ right to have an equitable opportunity to learn and honestly demonstrate the quality of their learning. Therefore, all students are expected to adhere to a standard of academic conduct, which demonstrates respect for themselves, their fellow students, and the University’s educational mission. The University deems all students to understand that if they are found responsible for academic misconduct (e.g., cheating, plagiarism, academic dishonesty), they will be subject to the Academic Misconduct procedures and sanctions, as outlined in the Student Handbook.\n\n\nHonesty Code Statement:\nFIU defines academic misconduct in the Student Conduct and Honor Code (Code) as “any act or omission by a Student, which violates the concept of academic integrity and undermines the academic mission of the University in violation of the Code.” Code violations include, but are not limited to: academic dishonesty, bribery, cheating, commercial use, complicity, falsification, and plagiarism. The Code is available here: https://regulations.fiu.edu/regulation=FIU-2501"
  },
  {
    "objectID": "comp.html",
    "href": "comp.html",
    "title": "BSC6926-B53B Spring 2025",
    "section": "",
    "text": "R and RStudio\nR and RStudio are separate downloads and installations. R is the underlying statistical computing environment, but using R alone is no fun. RStudio is a graphical integrated development environment (IDE) that makes using R much easier and more interactive. You need to install R before you install RStudio. In the sections below are the instructions for installing R and R Studio on your operating system.\n\nWindows\n\nIf you already have R and RStudio installed\n\nOpen RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio.\nTo check which version of R you are using, start RStudio and the first thing that appears in the console indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it. You can check here for more information on how to remove old versions from your system if you wish to do so.\n\n\n\nIf you don’t have R and RStudio installed\n\nDownload R from the CRAN website.\nRun the .exe file that was just downloaded\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Windows 10/11 (where x, y, and z represent version numbers)\nDouble click the file to install it\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages.\n\n\n\n\nmacOS\n\nIf you already have R and RStudio installed\n\nOpen RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio.\nTo check the version of R you are using, start RStudio and the first thing that appears on the terminal indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it.\n\n\n\nIf you don’t have R and RStudio installed\n\nDownload R from the CRAN website.\nSelect the .pkg file for the latest R version\nDouble click on the downloaded file to install R\nIt is also a good idea to install XQuartz (needed by some packages)\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Mac OS X 10.15+ (64-bit) (where x, y, and z represent version numbers)\nDouble click the file to install RStudio\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages.\n\n\n\n\nLinux\n\nFollow the instructions for your distribution from CRAN, they provide information to get the most recent version of R for common distributions. For most distributions, you could use your package manager (e.g., for Debian/Ubuntu run sudo apt-get install r-base, and for Fedora sudo yum install R), but we don’t recommend this approach as the versions provided by this are usually out of date. In any case, make sure you have at least R 3.3.1.\nGo to the RStudio download page\nUnder Installers select the version that matches your distribution, and install it with your preferred method (e.g., with Debian/Ubuntu sudo dpkg -i   rstudio-x.yy.zzz-amd64.deb at the terminal).\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSC6926-B53B Spring 2025",
    "section": "",
    "text": "BSC6926-B53B: Using a multivariate tool to assess population and community dynamics\nThis is the course website for the R workshop Using a multivariate tool to assess population and community dynamics. This website will have the Quarto markdown lessons for each workshop. Find the course schedule and Syllabus here. This course will be based in R and information about downloading R and Rstudio can be found here.\n\n\nClass Resources\n\nZoom link\nGithub repository\n\n\n\nR Resources\n\nR for Data Science by Hadley Wickham and Garret Grolemund – An introduction to programming with R: https://r4ds.hadley.nz/\n\nQuick-R by datacamp: Quick overview on R programming and statistical approaches. There are more tutorials, but you will be required to register\n\nRStudio Cloud Training Exercises: https://rstudio.cloud/learn/primers\n\nRstudio: learn R https://education.rstudio.com/learn/beginner/"
  },
  {
    "objectID": "usefulfx.html",
    "href": "usefulfx.html",
    "title": "Useful functions for hypervolumes",
    "section": "",
    "text": "This script contains information about the functions used in the hypervolume scripts."
  },
  {
    "objectID": "usefulfx.html#mergejoin",
    "href": "usefulfx.html#mergejoin",
    "title": "Useful functions for hypervolumes",
    "section": "Merge/Join",
    "text": "Merge/Join\nIf two data frames contain different columns of data, then they can be merged together with the family of join functions.\n+left_join() = uses left df as template and joins all matching columns from right df +right_join() = uses right df as template and joins all matching columns from left df +inner_join() = only matches columns contained in both dfs +full_join() = combines all rows in both dfs\n\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nleft = tibble(name = c('a', 'b', 'c'),\n              n = c(1, 6, 7), \n              bio = c(100, 43, 57))\n\nright = tibble(name = c('a', 'b', 'd', 'e'),\n               cals = c(500, 450, 570, 600))\n\nleft_join(left, right, by = 'name')\n## # A tibble: 3 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n## 3 c         7    57    NA\n\nright_join(left, right, by = 'name')\n## # A tibble: 4 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n## 3 d        NA    NA   570\n## 4 e        NA    NA   600\n\ninner_join(left, right, by = 'name')\n## # A tibble: 2 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n\nfull_join(left, right, by = 'name')\n## # A tibble: 5 × 4\n##   name      n   bio  cals\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 a         1   100   500\n## 2 b         6    43   450\n## 3 c         7    57    NA\n## 4 d        NA    NA   570\n## 5 e        NA    NA   600\n\n# multiple matches\nfish = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n              year = rep(c(1999,2005,2020), each = 2),\n              catch = c(50, 60, 40, 50, 60, 100))\n\ncol = tibble(species = c('Salmon', 'Cod'),\n             coast = c('West', 'East'))\n\nleft_join(fish, col, by = 'species')\n## # A tibble: 6 × 4\n##   species  year catch coast\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n## 1 Salmon   1999    50 West \n## 2 Cod      1999    60 East \n## 3 Salmon   2005    40 West \n## 4 Cod      2005    50 East \n## 5 Salmon   2020    60 West \n## 6 Cod      2020   100 East"
  },
  {
    "objectID": "usefulfx.html#scaling-data",
    "href": "usefulfx.html#scaling-data",
    "title": "Useful functions for hypervolumes",
    "section": "scaling data",
    "text": "scaling data\nBecause hypervolumes can be generated with any continuous data as an axes, many of the times the units are not combatible. Blonder et al. 2014 & 2018 to convert all of the axes into the same units. This can be done by taking the z-score of the values to convert units into standard deviations. Z-scoring data can be done with the formula: \\[ z = \\frac{x_{i}-\\overline{x}}{sd} \\] Where \\(x_{i}\\) is a value, \\(\\overline{x}\\) is the mean, and \\(sd\\) is the standard deviation. By z-scoring each axis, 0 is the mean of that axis, a value of 1 means that the value is 1 standard deviation above the global mean of that axis, and a value of -1 is 1 standard deviation below the global mean of the axis. In R this can be done manually or with the scale() function.\n\nfish = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n              year = rep(c(1999,2005,2020), each = 2),\n              catch = c(50, 60, 40, 50, 60, 100))\n\n#\nfish = fish |&gt; \n  mutate(zcatch1 = (catch - mean(catch))/sd(catch), # manual\n         zcatch2 = scale(catch)) # with scale\n\nfish \n## # A tibble: 6 × 5\n##   species  year catch zcatch1 zcatch2[,1]\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Salmon   1999    50  -0.477      -0.477\n## 2 Cod      1999    60   0           0    \n## 3 Salmon   2005    40  -0.953      -0.953\n## 4 Cod      2005    50  -0.477      -0.477\n## 5 Salmon   2020    60   0           0    \n## 6 Cod      2020   100   1.91        1.91\n\n# center = mean, scale = sd\nfish$zcatch2\n##            [,1]\n## [1,] -0.4767313\n## [2,]  0.0000000\n## [3,] -0.9534626\n## [4,] -0.4767313\n## [5,]  0.0000000\n## [6,]  1.9069252\n## attr(,\"scaled:center\")\n## [1] 60\n## attr(,\"scaled:scale\")\n## [1] 20.97618"
  },
  {
    "objectID": "usefulfx.html#nesting-data",
    "href": "usefulfx.html#nesting-data",
    "title": "Useful functions for hypervolumes",
    "section": "nesting data",
    "text": "nesting data\nOne benefit of tibbles is that they can contain list columns. This means that we can make columns of tibbles that are nested within a dataset. Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns. Nesting is a implicitly summarising operation: you get one row for each group defined by the non-nested columns. This is useful in conjunction with other summaries that work with whole datasets, most notably models. This can be done with the nest() and then flattened with unnest()\n\nfish = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n              year = rep(c(1999,2005,2020), each = 2),\n              catch = c(50, 60, 40, 50, 60, 100))\n\n# using group_by\nfish_nest = fish |&gt; \n  group_by(species) |&gt; \n  nest()\n\nfish_nest\n## # A tibble: 2 × 2\n## # Groups:   species [2]\n##   species data            \n##   &lt;chr&gt;   &lt;list&gt;          \n## 1 Salmon  &lt;tibble [3 × 2]&gt;\n## 2 Cod     &lt;tibble [3 × 2]&gt;\nfish_nest$data\n## [[1]]\n## # A tibble: 3 × 2\n##    year catch\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1  1999    50\n## 2  2005    40\n## 3  2020    60\n## \n## [[2]]\n## # A tibble: 3 × 2\n##    year catch\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1  1999    60\n## 2  2005    50\n## 3  2020   100\n\n# using .by in nest\n# column name becomes data unless you change .key\nfish_nest2 = fish |&gt; \n  nest(.by = year, .key = 'df')\n\nfish_nest2\n## # A tibble: 3 × 2\n##    year df              \n##   &lt;dbl&gt; &lt;list&gt;          \n## 1  1999 &lt;tibble [2 × 2]&gt;\n## 2  2005 &lt;tibble [2 × 2]&gt;\n## 3  2020 &lt;tibble [2 × 2]&gt;\nfish_nest2$df\n## [[1]]\n## # A tibble: 2 × 2\n##   species catch\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 Salmon     50\n## 2 Cod        60\n## \n## [[2]]\n## # A tibble: 2 × 2\n##   species catch\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 Salmon     40\n## 2 Cod        50\n## \n## [[3]]\n## # A tibble: 2 × 2\n##   species catch\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 Salmon     60\n## 2 Cod       100"
  },
  {
    "objectID": "usefulfx.html#map",
    "href": "usefulfx.html#map",
    "title": "Useful functions for hypervolumes",
    "section": "map",
    "text": "map\n\npurr\nThe newest and new standard package with tidyverse is purr with its set of map() functions. Some similarity to plyr (and base) and dplyr functions but with more consistent names and arguments. Notice that map function can have some specification for the type of output. + map() makes a list. + map_lgl() makes a logical vector. + map_int() makes an integer vector. + map_dbl() makes a double vector. + map_chr() makes a character vector.\n\ndf = iris  |&gt; \n  select(-Species)\n#summary statistics\nmap_dbl(df, mean)\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\n# using map with mutate and nest\nd = tibble(species = rep(c('Salmon', 'Cod'),times = 3),\n           year = rep(c(1999,2005,2020), each = 2),\n           catch = c(50, 60, 40, 50, 60, 100)) |&gt; \n  nest(.by = species) |&gt; \n  mutate(correlation = map(data, \\(data) cor.test(data$year, data$catch)))\n\nd\n## # A tibble: 2 × 3\n##   species data             correlation\n##   &lt;chr&gt;   &lt;list&gt;           &lt;list&gt;     \n## 1 Salmon  &lt;tibble [3 × 2]&gt; &lt;htest&gt;    \n## 2 Cod     &lt;tibble [3 × 2]&gt; &lt;htest&gt;\nd$correlation\n## [[1]]\n## \n##  Pearson's product-moment correlation\n## \n## data:  data$year and data$catch\n## t = 0.96225, df = 1, p-value = 0.5122\n## alternative hypothesis: true correlation is not equal to 0\n## sample estimates:\n##       cor \n## 0.6933752 \n## \n## \n## [[2]]\n## \n##  Pearson's product-moment correlation\n## \n## data:  data$year and data$catch\n## t = 1.963, df = 1, p-value = 0.3\n## alternative hypothesis: true correlation is not equal to 0\n## sample estimates:\n##       cor \n## 0.8910421\n\n# extract information from list\nd = d |&gt; \n  mutate(r = map_dbl(correlation, \\(x) x$estimate),\n         p = map_dbl(correlation, \\(x) x$p.value))\n\nSometimes, there are multiple arguments required for the function you are mapping. You can provide two lists (or columns) using map2(). If you have more than two lists needed, you can use pmap(). These functions work the same as map() based on the desired output.\n\nlibrary(performance)\n# using map2 and pmap\ndf = iris  |&gt; \n  nest(.by = Species) |&gt; \n  mutate(sw = map(data, \\(data) lm(Sepal.Length ~ Sepal.Width, data = data)),\n         pl = map(data, \\(data) lm(Sepal.Length ~ Petal.Length, data = data)),\n         mod_c = map2(sw,pl, \\(sw,pl) compare_performance(sw,pl)),\n         top = map_chr(mod_c, \\(x) x$Name[which.min(x$AICc)]),\n         sum_top = pmap(list(sw,pl,top), \\(x,y,z) \n                        if (z == 'sw'){\n                          summary(x)\n                        }else{\n                          summary(y)}),\n         p = map_dbl(sum_top, \\(s) s$coefficients[2, 4]))\n\n\ndf\n## # A tibble: 3 × 8\n##   Species    data              sw     pl    mod_c      top   sum_top           p\n##   &lt;fct&gt;      &lt;list&gt;            &lt;list&gt; &lt;lis&gt; &lt;list&gt;     &lt;chr&gt; &lt;list&gt;        &lt;dbl&gt;\n## 1 setosa     &lt;tibble [50 × 4]&gt; &lt;lm&gt;   &lt;lm&gt;  &lt;cmpr_prf&gt; sw    &lt;smmry.lm&gt; 6.71e-10\n## 2 versicolor &lt;tibble [50 × 4]&gt; &lt;lm&gt;   &lt;lm&gt;  &lt;cmpr_prf&gt; pl    &lt;smmry.lm&gt; 2.59e-10\n## 3 virginica  &lt;tibble [50 × 4]&gt; &lt;lm&gt;   &lt;lm&gt;  &lt;cmpr_prf&gt; pl    &lt;smmry.lm&gt; 6.30e-16"
  },
  {
    "objectID": "ws1.html#hypervolumes",
    "href": "ws1.html#hypervolumes",
    "title": "Workshop 1: Introduction to hypervolumes",
    "section": "Hypervolumes",
    "text": "Hypervolumes\nHypervolumes are a multidimensional tool that is based on Hutchinson’s n-dimensional niche concept and we can build them with the hypervolume package.\n\nPreparing the data\nTypically we have a dataset that has the abundance of the community and another that has trait data. Therefore we need to combine them. Also, we can’t have NAs, so we have to filter any missing data one we combine the datasets.\n\nlibrary(tidyverse)\n# abundance data\nab = read_csv('data/abundHermine.csv')\n## Rows: 68 Columns: 5\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): site, species, hur, period\n## dbl (1): abund\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# trait data\ntr = read_csv('data/fishTraits.csv')\n## Rows: 46 Columns: 7\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): species, project, Class, Order\n## dbl (3): trophic_level, temp_preference, generation_time\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# combine\ndf = left_join(ab, tr, by = 'species') |&gt; \n  drop_na()\n\nNow we can make the hypervolume by selecting the data to be included and z-scoring\n\ndf_before = df |&gt; \n  filter(period == 'Before') |&gt; \n  select(trophic_level, temp_preference, generation_time)|&gt; \n  mutate(across(trophic_level:generation_time, scale))\n\n\n\nBuilding hypervolumes\nWith a nested dataset of our columns that we want to build hypervolumes for we can use mutate() and map() to generate the hypervolume.\n\nlibrary(hypervolume)\n## Loading required package: Rcpp\n\nhv_before = hypervolume_gaussian(df_before, name = 'Before',\n                                              samples.per.point = 1000,\n                                              kde.bandwidth = estimate_bandwidth(df_before), \n                                              sd.count = 3, \n                                              quantile.requested = 0.95, \n                                              quantile.requested.type = \"probability\", \n                                              chunk.size = 1000, \n                                              verbose = F)\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n\nhv_before\n## ***** Object of class Hypervolume *****\n## Name: Before\n## Method: Gaussian kernel density estimate\n## Number of data points (after weighting): 23\n## Dimensionality: 3\n## Volume: 78.068034\n## Random point density: 171.606729\n## Number of random points: 13397\n## Random point values:\n##  min: 0.000\n##  mean: 0.001\n##  median: 0.000\n##  max:0.003\n## Parameters:\n##  kde.bandwidth: 0.6189038 0.6189038 0.6189038\n##  kde.method: silverman\n##  samples.per.point: 1000\n##  sd.count: 3\n##  quantile.requested: 0.95\n##  quantile.requested.type: probability\n\nThis can also be done to multiple at the same time with nest().\n\ndf = df |&gt; \n  select(period, trophic_level, temp_preference, generation_time) |&gt; \n  mutate(across(trophic_level:generation_time, scale)) |&gt; \n  group_by(period) |&gt; \n  nest()\n\ndf\n## # A tibble: 2 × 2\n## # Groups:   period [2]\n##   period data             \n##   &lt;chr&gt;  &lt;list&gt;           \n## 1 Before &lt;tibble [23 × 3]&gt;\n## 2 After  &lt;tibble [25 × 3]&gt;\n\ndf = df |&gt; \n  mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = period,\n                                              samples.per.point = 1000,\n                                              kde.bandwidth = estimate_bandwidth(data), \n                                              sd.count = 3, \n                                              quantile.requested = 0.95, \n                                              quantile.requested.type = \"probability\", \n                                              chunk.size = 1000, \n                                              verbose = F)))\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n\nhead(df)\n## # A tibble: 2 × 3\n## # Groups:   period [2]\n##   period data              hv        \n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;    \n## 1 Before &lt;tibble [23 × 3]&gt; &lt;Hypervlm&gt;\n## 2 After  &lt;tibble [25 × 3]&gt; &lt;Hypervlm&gt;\ndf$hv\n## [[1]]\n## ***** Object of class Hypervolume *****\n## Name: Before\n## Method: Gaussian kernel density estimate\n## Number of data points (after weighting): 23\n## Dimensionality: 3\n## Volume: 70.900748\n## Random point density: 188.883762\n## Number of random points: 13392\n## Random point values:\n##  min: 0.000\n##  mean: 0.001\n##  median: 0.000\n##  max:0.004\n## Parameters:\n##  kde.bandwidth: 0.5996196 0.5570341 0.6430513\n##  kde.method: silverman\n##  samples.per.point: 1000\n##  sd.count: 3\n##  quantile.requested: 0.95\n##  quantile.requested.type: probability\n## \n## [[2]]\n## ***** Object of class Hypervolume *****\n## Name: After\n## Method: Gaussian kernel density estimate\n## Number of data points (after weighting): 25\n## Dimensionality: 3\n## Volume: 90.256157\n## Random point density: 160.354712\n## Number of random points: 14473\n## Random point values:\n##  min: 0.000\n##  mean: 0.000\n##  median: 0.000\n##  max:0.002\n## Parameters:\n##  kde.bandwidth: 0.6391039 0.6706801 0.5990887\n##  kde.method: silverman\n##  samples.per.point: 1000\n##  sd.count: 3\n##  quantile.requested: 0.95\n##  quantile.requested.type: probability\n\n\n\nplotting hypervolumes\nWe can plot multiple hypervolumes by joining them together\n\nhvj = hypervolume_join(df$hv[[1]], df$hv[[2]])\n\nplot(hvj, pairplot = T, colors=c('dodgerblue3','cyan2'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)\n\n\n\n\n\n\n\n\n\n\nhypervolume metrics\nThe geometry of hypervolumes are useful when characterizing and comparing hypervolumes. Hypervolume size represents the variation of the data, centroid distance compares the euclidian distance between two hypervolume centroids (mean conditions), and overlap measures the simularity of hypervolumes.\n\n# size \ndf = df |&gt; \n  mutate(hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n\nhead(df)\n## # A tibble: 2 × 4\n## # Groups:   period [2]\n##   period data              hv         hv_size\n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;       &lt;dbl&gt;\n## 1 Before &lt;tibble [23 × 3]&gt; &lt;Hypervlm&gt;    70.9\n## 2 After  &lt;tibble [25 × 3]&gt; &lt;Hypervlm&gt;    90.3\n\n# centroid distance \nhypervolume_distance(df$hv[[1]], df$hv[[2]], type = 'centroid', check.memory=F)\n## [1] 0.2435712\n\n# overlap \nhvset = hypervolume_set(df$hv[[1]], df$hv[[2]], check.memory = F)\n## Choosing num.points.max=53958 (use a larger value for more accuracy.)\n## Using minimum density of 160.354712\n## Retaining 11369 points in hv1 and 14473 points in hv2.\n## Beginning ball queries... \n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## Finished ball queries.\n\nhypervolume_overlap_statistics(hvset)\n##       jaccard      sorensen frac_unique_1 frac_unique_2 \n##    0.75154245    0.85814929    0.02471633    0.23386566\n\n\n\nWeight hypervolume input\nThe above hypervolume is just based on the traits using presence of species, but we can weight the points to shape the hypervolume based on abundance\n\n\n#prep data\ndf_w = left_join(ab, tr, by = 'species') |&gt; \n  drop_na() |&gt; \n  select(period, abund, trophic_level, temp_preference, generation_time) |&gt; \n  mutate(across(trophic_level:generation_time, scale)) |&gt; \n  group_by(period) |&gt; \n  nest(weight = abund, data = trophic_level:generation_time) \ndf_w\n## # A tibble: 2 × 3\n## # Groups:   period [2]\n##   period weight            data             \n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           \n## 1 Before &lt;tibble [23 × 1]&gt; &lt;tibble [23 × 3]&gt;\n## 2 After  &lt;tibble [25 × 1]&gt; &lt;tibble [25 × 3]&gt;\n\n# make hypervolumes\ndf_w = df_w |&gt; \n    mutate(hv = map2(data,weight, \\(data,weight) hypervolume_gaussian(data, name = paste(period,'weighted',sep = '_'),\n                                                      weight = weight$abund,\n                                                      samples.per.point = 1000,\n                                                      kde.bandwidth = estimate_bandwidth(data), \n                                                      sd.count = 3, \n                                                      quantile.requested = 0.95, \n                                                      quantile.requested.type = \"probability\", \n                                                      chunk.size = 1000, \n                                                      verbose = F)),\n           hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Warning: There were 2 warnings in `mutate()`.\n## The first warning was:\n## ℹ In argument: `hv = map2(...)`.\n## ℹ In group 1: `period = \"After\"`.\n## Caused by warning in `hypervolume_gaussian()`:\n## ! The sum of the weights must be equal to 1. Normalizing the weights.\n## ℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\ndf_w\n## # A tibble: 2 × 5\n## # Groups:   period [2]\n##   period weight            data              hv         hv_size\n##   &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;            &lt;list&gt;       &lt;dbl&gt;\n## 1 Before &lt;tibble [23 × 1]&gt; &lt;tibble [23 × 3]&gt; &lt;Hypervlm&gt;    19.0\n## 2 After  &lt;tibble [25 × 1]&gt; &lt;tibble [25 × 3]&gt; &lt;Hypervlm&gt;    24.4\n\n\nhvj_w = hypervolume_join(df_w$hv[[1]], df_w$hv[[2]])\n\nplot(hvj_w, pairplot = T, colors=c('dodgerblue3','cyan2'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)\n\n\n\n\n\n\n\n\n# centroid distance \nhypervolume_distance(df_w$hv[[1]], df_w$hv[[2]], type = 'centroid', check.memory=F)\n## [1] 0.01744452\n\n# overlap \nhvset_w = hypervolume_set(df_w$hv[[1]], df_w$hv[[2]], check.memory = F)\n## Choosing num.points.max=53958 (use a larger value for more accuracy.)\n## Using minimum density of 160.631671\n## Retaining 3047 points in hv1 and 3921 points in hv2.\n## Beginning ball queries... \n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## Finished ball queries.\n\nhypervolume_overlap_statistics(hvset_w)\n##       jaccard      sorensen frac_unique_1 frac_unique_2 \n##    0.74999133    0.85713720    0.02001969    0.23833902\n\n\n\nFrom mean and sd\nSometimes, we do not have enough points to meet assumptions to make a hypervolume. Therefore, we can simulate random points based on mean and sd of our axes. We can then simulate the information needed and make our hypervolumes.\n\n# mean and sd\ndf_m = left_join(ab, tr, by = 'species') |&gt; \n  drop_na() |&gt; \n  pivot_longer(trophic_level:generation_time, names_to = 'trait', values_to = 'value') |&gt; \n  group_by(period,trait) |&gt; \n  summarize(mean = mean(value),\n            sd = sd(value))\n## `summarise()` has grouped output by 'period'. You can override using the\n## `.groups` argument.\n\ndf_m\n## # A tibble: 6 × 4\n## # Groups:   period [2]\n##   period trait            mean    sd\n##   &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n## 1 After  generation_time  3.10 1.79 \n## 2 After  temp_preference 25.1  1.22 \n## 3 After  trophic_level    3.52 0.150\n## 4 Before generation_time  3.35 1.90 \n## 5 Before temp_preference 25.0  1.00 \n## 6 Before trophic_level    3.54 0.139\n\n#generate points \n# number of points \nn = 50 \n\ndf_tot = df_m |&gt; slice(rep(1:n(), each=n))|&gt;\n      mutate(point = map2_dbl(mean,sd, \\(mean,sd) rnorm(1,mean =mean,sd =sd))) |&gt; \n      group_by(period, trait) |&gt; \n      mutate(num = row_number()) |&gt;\n      select(-mean, -sd)|&gt;\n      pivot_wider(names_from = trait, values_from = point)|&gt; \n      select(-num) |&gt; \n      mutate(across(generation_time:trophic_level,scale)) |&gt; \n      group_by(period) |&gt; \n      nest()\n\n\n# generate hypervolumes\ndf_tot = df_tot |&gt; \n  mutate(hv = map(data, \\(data) hypervolume_gaussian(data, name = period,\n                                              samples.per.point = 1000,\n                                              kde.bandwidth = estimate_bandwidth(data), \n                                              sd.count = 3, \n                                              quantile.requested = 0.95, \n                                              quantile.requested.type = \"probability\", \n                                              chunk.size = 1000, \n                                              verbose = F)),\n         hv_size = map_dbl(hv, \\(hv) get_volume(hv)))\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n## Note that the formula used for the Silverman estimator differs in version 3 compared to prior versions of this package.\n## Use method='silverman-1d' to replicate prior behavior.\n\n\n#plot\nhvj_tot = hypervolume_join(df_tot$hv[[1]], df_tot$hv[[2]])\n\nplot(hvj_tot, pairplot = T, colors=c('dodgerblue3','cyan2'),\n     show.3d=FALSE,plot.3d.axes.id=NULL,\n     show.axes=TRUE, show.frame=TRUE,\n     show.random=T, show.density=TRUE,show.data=F,\n     show.legend=T, limits=c(-5,5), \n     show.contour=F, contour.lwd= 2, \n     contour.type='alphahull', \n     contour.alphahull.alpha=0.25,\n     contour.ball.radius.factor=1, \n     contour.kde.level=0.01,\n     contour.raster.resolution=100,\n     show.centroid=TRUE, cex.centroid=2,\n     point.alpha.min=0.2, point.dark.factor=0.5,\n     cex.random=0.5,cex.data=1,cex.axis=1.5,cex.names=2,cex.legend=2,\n     num.points.max.data = 100000, num.points.max.random = 200000, reshuffle=TRUE,\n     plot.function.additional=NULL,\n     verbose=FALSE\n)\n\n\n\n\n\n\n\n\n# centroid distance \nhypervolume_distance(df_tot$hv[[1]], df_tot$hv[[2]], type = 'centroid', check.memory=F)\n## [1] 0.09437035\n\n# overlap \nhvset_tot = hypervolume_set(df_tot$hv[[1]], df_tot$hv[[2]], check.memory = F)\n## Choosing num.points.max=53958 (use a larger value for more accuracy.)\n## Using minimum density of 250.203549\n## Retaining 25684 points in hv1 and 24689 points in hv2.\n## Beginning ball queries... \n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## \n## Building tree... \n## done.\n## Ball query... \n## \n## done.\n## Finished ball queries.\n\nhypervolume_overlap_statistics(hvset_tot)\n##       jaccard      sorensen frac_unique_1 frac_unique_2 \n##     0.5857395     0.7387588     0.2755412     0.2463653"
  }
]